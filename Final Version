{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MethmiDharmakeerthi/OurAcademicResearchIsBest/blob/main/Final%20Version\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZU8t4iKdN_V"
      },
      "source": [
        "================================\n",
        "\n",
        "**COMPLETE H.265 FIXED-RESOLUTION STREAMING SYSTEM**\n",
        "\n",
        "Research: Optimizing Video Streaming Quality at Low Bandwidth with Static Resolution Maintenance\n",
        "================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz9d9Am9dnC3",
        "outputId": "48665d32-4626-4333-fbb8-614d67122373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ TensorFlow available - ML features enabled\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "import subprocess\n",
        "import pickle\n",
        "import time\n",
        "import threading\n",
        "from datetime import datetime\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import xml.etree.ElementTree as ET\n",
        "import logging\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import requests\n",
        "import hashlib\n",
        "\n",
        "\n",
        "# Deep Learning imports\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras import layers, models, Input\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "    HAS_ML = True\n",
        "    print(\"‚úÖ TensorFlow available - ML features enabled\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è TensorFlow not available. ML features disabled.\")\n",
        "    HAS_ML = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKsO5ZWOT7Ac"
      },
      "source": [
        "::::# ================================\n",
        "# DAILY STARTUP CELL - Run this first every day\n",
        "# ================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P79tvXL0ejpN",
        "outputId": "438eab40-5947-40da-d70e-bd373292130f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting research session on 2025-06-17 20:22:51\n",
            "üîó Mounting Google Drive...\n",
            "üì§ Previous drive session flushed\n",
            "Mounted at /content/drive\n",
            "‚úÖ Google Drive mounted successfully\n",
            "‚úÖ Drive access verified\n",
            "üìÅ Drive contents: ['Colab Notebooks', 'Training Contract Form - Prasadini Gunathilaka (1).pdf', 'Training Contract Form - Prasadini Gunathilaka.pdf', 'Prasadini Gunathilaka- work site form (1).pdf', 'Prasadini Gunathilaka- work site form.pdf']...\n",
            "üìÅ Directory ready: /content/drive/MyDrive/Research\n",
            "üìÅ Directory ready: /content/drive/MyDrive/Research/OurCode\n",
            "üìÅ Directory ready: /content/drive/MyDrive/Research/DailyLogs\n",
            "üìÅ Directory ready: /content/drive/MyDrive/Research/Backups\n",
            "üìç Working directory: /content/drive/MyDrive/Research/OurCode\n",
            "\n",
            "==================================================\n",
            "TESTING DRIVE ACCESS...\n",
            "==================================================\n",
            "‚úÖ Drive write test PASSED\n",
            "\n",
            "==================================================\n",
            "LOADING PREVIOUS SESSION...\n",
            "==================================================\n",
            "üìÇ State loaded from 2025-06-17T20:11:19.723498\n",
            "‚úÖ Restored session from step 4\n",
            "üìä Previous results count: 0\n",
            "üìù Last notes: test\n",
            "\n",
            "üî• Ready to continue research!\n",
            "üìÅ All logs will be saved to: /content/drive/MyDrive/Research/DailyLogs\n",
            "üíæ Backups will be saved to: /content/drive/MyDrive/Research/Backups\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# IMPROVED DAILY STARTUP CELL - Run this first every day\n",
        "# ================================\n",
        "\n",
        "from google.colab import drive\n",
        "import sys\n",
        "import os\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "print(f\"üöÄ Starting research session on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Force remount Drive with more explicit permissions\n",
        "print(\"üîó Mounting Google Drive...\")\n",
        "try:\n",
        "    # Unmount first if already mounted\n",
        "    try:\n",
        "        drive.flush_and_unmount()\n",
        "        print(\"üì§ Previous drive session flushed\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Mount with force_remount\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    print(\"‚úÖ Google Drive mounted successfully\")\n",
        "\n",
        "    # Verify mount by listing contents\n",
        "    if os.path.exists('/content/drive/MyDrive'):\n",
        "        print(\"‚úÖ Drive access verified\")\n",
        "        print(f\"üìÅ Drive contents: {os.listdir('/content/drive/MyDrive')[:5]}...\")\n",
        "    else:\n",
        "        raise Exception(\"Drive mount failed - MyDrive not accessible\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Drive mount failed: {e}\")\n",
        "    print(\"üîß Try running: from google.colab import drive; drive.mount('/content/drive', force_remount=True)\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Create research directory structure\n",
        "BASE_DIR = '/content/drive/MyDrive/Research'\n",
        "CODE_DIR = os.path.join(BASE_DIR, 'OurCode')\n",
        "LOGS_DIR = os.path.join(BASE_DIR, 'DailyLogs')\n",
        "BACKUPS_DIR = os.path.join(BASE_DIR, 'Backups')\n",
        "\n",
        "# Create directories\n",
        "for directory in [BASE_DIR, CODE_DIR, LOGS_DIR, BACKUPS_DIR]:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    print(f\"üìÅ Directory ready: {directory}\")\n",
        "\n",
        "# Set working directory\n",
        "os.chdir(CODE_DIR)\n",
        "print(f\"üìç Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Enhanced state management functions\n",
        "def save_state(data, filename='research_state.pkl'):\n",
        "    \"\"\"Save current research state with verification\"\"\"\n",
        "    try:\n",
        "        state = {\n",
        "            'data': data,\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'session_info': f'Session on {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
        "        }\n",
        "\n",
        "        # Save to main location\n",
        "        filepath = os.path.join(CODE_DIR, filename)\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump(state, f)\n",
        "\n",
        "        # Verify file was created\n",
        "        if os.path.exists(filepath):\n",
        "            size = os.path.getsize(filepath)\n",
        "            print(f\"üíæ State saved successfully: {filepath} ({size} bytes)\")\n",
        "\n",
        "            # Also save as JSON backup for readability\n",
        "            json_filepath = filepath.replace('.pkl', '.json')\n",
        "            try:\n",
        "                # Convert data to JSON-serializable format\n",
        "                json_data = {\n",
        "                    'timestamp': state['timestamp'],\n",
        "                    'session_info': state['session_info'],\n",
        "                    'current_step': data.get('current_step', 0),\n",
        "                    'notes': data.get('notes', ''),\n",
        "                    'results_count': len(data.get('results', [])),\n",
        "                    'experiment_params': data.get('experiment_params', {})\n",
        "                }\n",
        "                with open(json_filepath, 'w') as f:\n",
        "                    json.dump(json_data, f, indent=2)\n",
        "                print(f\"üìÑ JSON backup saved: {json_filepath}\")\n",
        "            except Exception as json_e:\n",
        "                print(f\"‚ö† JSON backup failed: {json_e}\")\n",
        "\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"‚ùå File not found after save attempt: {filepath}\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Save failed: {e}\")\n",
        "        return False\n",
        "\n",
        "def load_state(filename='research_state.pkl'):\n",
        "    \"\"\"Load previous research state\"\"\"\n",
        "    filepath = os.path.join(CODE_DIR, filename)\n",
        "    try:\n",
        "        if os.path.exists(filepath):\n",
        "            with open(filepath, 'rb') as f:\n",
        "                state = pickle.load(f)\n",
        "            print(f\"üìÇ State loaded from {state['timestamp']}\")\n",
        "            return state['data']\n",
        "        else:\n",
        "            print(f\"üÜï No previous state found at {filepath}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö† Load failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def log_daily_progress(day_number, accomplishments, next_steps, key_findings=None, issues=None):\n",
        "    \"\"\"Enhanced daily progress logging\"\"\"\n",
        "    if key_findings is None:\n",
        "        key_findings = []\n",
        "    if issues is None:\n",
        "        issues = []\n",
        "\n",
        "    # Create daily log entry\n",
        "    log_entry = {\n",
        "        'date': datetime.now().strftime('%Y-%m-%d'),\n",
        "        'day_number': day_number,\n",
        "        'accomplishments': accomplishments,\n",
        "        'next_steps': next_steps,\n",
        "        'key_findings': key_findings,\n",
        "        'issues': issues,\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    # Save as individual daily log\n",
        "    daily_log_file = os.path.join(LOGS_DIR, f'day_{day_number:02d}_{datetime.now().strftime(\"%Y%m%d\")}.json')\n",
        "    try:\n",
        "        with open(daily_log_file, 'w') as f:\n",
        "            json.dump(log_entry, f, indent=2)\n",
        "        print(f\"üìÖ Daily log saved: {daily_log_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Daily log save failed: {e}\")\n",
        "\n",
        "    # Append to master log\n",
        "    master_log_file = os.path.join(LOGS_DIR, 'master_research_log.json')\n",
        "    try:\n",
        "        # Load existing log or create new\n",
        "        if os.path.exists(master_log_file):\n",
        "            with open(master_log_file, 'r') as f:\n",
        "                master_log = json.load(f)\n",
        "        else:\n",
        "            master_log = {'entries': []}\n",
        "\n",
        "        # Add new entry\n",
        "        master_log['entries'].append(log_entry)\n",
        "        master_log['last_updated'] = datetime.now().isoformat()\n",
        "\n",
        "        # Save updated log\n",
        "        with open(master_log_file, 'w') as f:\n",
        "            json.dump(master_log, f, indent=2)\n",
        "        print(f\"üìä Master log updated: {master_log_file}\")\n",
        "\n",
        "        # Create CSV version for easy viewing\n",
        "        df = pd.DataFrame(master_log['entries'])\n",
        "        csv_file = os.path.join(LOGS_DIR, 'research_progress.csv')\n",
        "        df.to_csv(csv_file, index=False)\n",
        "        print(f\"üìà CSV log saved: {csv_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Master log update failed: {e}\")\n",
        "\n",
        "# Test Drive access\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TESTING DRIVE ACCESS...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "test_file = os.path.join(CODE_DIR, 'drive_test.txt')\n",
        "try:\n",
        "    with open(test_file, 'w') as f:\n",
        "        f.write(f\"Drive test at {datetime.now()}\")\n",
        "\n",
        "    if os.path.exists(test_file):\n",
        "        print(\"‚úÖ Drive write test PASSED\")\n",
        "        os.remove(test_file)  # Clean up\n",
        "    else:\n",
        "        print(\"‚ùå Drive write test FAILED\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Drive test error: {e}\")\n",
        "\n",
        "# Load previous state\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"LOADING PREVIOUS SESSION...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "previous_data = load_state()\n",
        "if previous_data:\n",
        "    # Restore your variables\n",
        "    model = previous_data.get('model', None)\n",
        "    processed_data = previous_data.get('processed_data', None)\n",
        "    results = previous_data.get('results', [])\n",
        "    current_step = previous_data.get('current_step', 0)\n",
        "    experiment_params = previous_data.get('experiment_params', {})\n",
        "    notes = previous_data.get('notes', \"\")\n",
        "\n",
        "    print(f\"‚úÖ Restored session from step {current_step}\")\n",
        "    print(f\"üìä Previous results count: {len(results)}\")\n",
        "    print(f\"üìù Last notes: {notes[:100]}...\" if len(notes) > 100 else f\"üìù Last notes: {notes}\")\n",
        "else:\n",
        "    # Initialize fresh session\n",
        "    model = None\n",
        "    processed_data = None\n",
        "    results = []\n",
        "    current_step = 0\n",
        "    experiment_params = {}\n",
        "    notes = \"\"\n",
        "    print(\"üÜï Starting fresh session\")\n",
        "\n",
        "print(f\"\\nüî• Ready to continue research!\")\n",
        "print(f\"üìÅ All logs will be saved to: {LOGS_DIR}\")\n",
        "print(f\"üíæ Backups will be saved to: {BACKUPS_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skAYcEL9UpTU"
      },
      "source": [
        "# ================================\n",
        "# END OF DAY CELL - Run before closing session\n",
        "# ================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88mu6lh9e2FP",
        "outputId": "fe1985d7-b377-49b3-d39f-9e19da287c53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üí° To end your session, run: end_research_session()\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# END OF DAY CELL - Run this at the end of each day\n",
        "# ================================\n",
        "\n",
        "def end_research_session():\n",
        "    \"\"\"Comprehensive end-of-day logging\"\"\"\n",
        "    print(\"üåÖ Ending research session...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Initialize current_step if not defined\n",
        "    global current_step, model, processed_data, results, experiment_params\n",
        "    if 'current_step' not in globals():\n",
        "        current_step = 0\n",
        "    current_step += 1\n",
        "\n",
        "    # Get user input for session summary\n",
        "    print(\"\\nüìù SESSION SUMMARY INPUT:\")\n",
        "    try:\n",
        "        end_notes = input(\"Brief summary of today's work: \")\n",
        "        day_number = int(input(\"Which research day was this? (1-180): \"))\n",
        "        accomplishments_input = input(\"Key accomplishments (comma-separated): \")\n",
        "        accomplishments = [a.strip() for a in accomplishments_input.split(',') if a.strip()]\n",
        "        next_steps_input = input(\"Tomorrow's priorities (comma-separated): \")\n",
        "        next_steps = [n.strip() for n in next_steps_input.split(',') if n.strip()]\n",
        "        key_findings_input = input(\"Key findings (comma-separated, optional): \")\n",
        "        key_findings = [f.strip() for f in key_findings_input.split(',') if f.strip()]\n",
        "        issues_input = input(\"Issues encountered (comma-separated, optional): \")\n",
        "        issues = [i.strip() for i in issues_input.split(',') if i.strip()]\n",
        "    except:\n",
        "        print(\"‚ö† Using default values due to input error\")\n",
        "        end_notes = \"Session completed\"\n",
        "        day_number = current_step\n",
        "        accomplishments = [\"Continued research\"]\n",
        "        next_steps = [\"Continue tomorrow\"]\n",
        "        key_findings = []\n",
        "        issues = []\n",
        "\n",
        "    # Ensure variables exist\n",
        "    model = model if 'model' in globals() else None\n",
        "    processed_data = processed_data if 'processed_data' in globals() else None\n",
        "    results = results if 'results' in globals() else []\n",
        "    experiment_params = experiment_params if 'experiment_params' in globals() else {}\n",
        "\n",
        "    # Save final state\n",
        "    final_state = {\n",
        "        'model': model,\n",
        "        'processed_data': processed_data,\n",
        "        'results': results,\n",
        "        'current_step': current_step,\n",
        "        'experiment_params': experiment_params,\n",
        "        'notes': end_notes,\n",
        "        'session_end_time': datetime.now().isoformat(),\n",
        "        'day_number': day_number\n",
        "    }\n",
        "\n",
        "    # Save main state\n",
        "    if save_state(final_state):\n",
        "        print(\"‚úÖ Main state saved successfully\")\n",
        "    else:\n",
        "        print(\"‚ùå Main state save failed\")\n",
        "\n",
        "    # Create backup\n",
        "    backup_filename = f\"backup_day_{day_number:02d}{datetime.now().strftime('%Y%m%d%H%M')}.pkl\"\n",
        "    backup_path = os.path.join(BACKUPS_DIR, backup_filename)\n",
        "    try:\n",
        "        with open(backup_path, 'wb') as f:\n",
        "            pickle.dump({'state': final_state, 'backup_time': datetime.now().isoformat()}, f)\n",
        "        print(f\"üíæ Backup saved: {backup_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Backup failed: {e}\")\n",
        "\n",
        "    # Log daily progress\n",
        "    log_daily_progress(\n",
        "        day_number=day_number,\n",
        "        accomplishments=accomplishments,\n",
        "        next_steps=next_steps,\n",
        "        key_findings=key_findings,\n",
        "        issues=issues,\n",
        "    )\n",
        "\n",
        "    # Session summary\n",
        "    print(f\"\\nüìä SESSION SUMMARY:\")\n",
        "    print(f\"   üìÖ Research Day: {day_number}\")\n",
        "    print(f\"   üî¢ Current Step: {current_step}\")\n",
        "    print(f\"   üìà Results Count: {len(results) if results else 0}\")\n",
        "    print(f\"   üìù Notes: {end_notes}\")\n",
        "    print(f\"   ‚úÖ Accomplishments: {len(accomplishments)}\")\n",
        "    print(f\"   ‚û° Next Steps: {len(next_steps)}\")\n",
        "\n",
        "    # Force drive sync\n",
        "    try:\n",
        "        drive.flush_and_unmount()\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"üîÑ Drive synced successfully\")\n",
        "    except:\n",
        "        print(\"‚ö† Drive sync may be incomplete\")\n",
        "\n",
        "    print(\"\\n‚úÖ Session saved successfully!\")\n",
        "    print(\"üîÑ Ready for tomorrow's session\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "# Make the end session function available\n",
        "print(\"\\nüí° To end your session, run: end_research_session()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWt_cmevWvg_"
      },
      "source": [
        "# ================================\n",
        "# PROGRESS TRACKING CELL\n",
        "# ================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "UkqIu3FvWwI3"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# PROGRESS TRACKING CELL\n",
        "# ================================\n",
        "\n",
        "def log_daily_progress(day_number, accomplishments, next_steps, issues=None, key_findings=None):\n",
        "    \"\"\"Log daily research progress\"\"\"\n",
        "    log_entry = {\n",
        "        'day': day_number,\n",
        "        'date': datetime.now().strftime('%Y-%m-%d'),\n",
        "        'start_time': datetime.now().isoformat(),\n",
        "        'accomplishments': accomplishments,\n",
        "        'next_steps': next_steps,\n",
        "        'issues': issues or [],\n",
        "        'key_findings': key_findings or [],\n",
        "        'current_step': current_step,\n",
        "        'results_count': len(results) if results else 0\n",
        "    }\n",
        "\n",
        "    # Load existing logs\n",
        "    log_file = '/content/drive/MyDrive/Research/OurCode/research_log.json'\n",
        "    try:\n",
        "        with open(log_file, 'r') as f:\n",
        "            logs = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        logs = []\n",
        "\n",
        "    logs.append(log_entry)\n",
        "\n",
        "    with open(log_file, 'w') as f:\n",
        "        json.dump(logs, f, indent=2)\n",
        "\n",
        "    print(f\"üìù Day {day_number} progress logged!\")\n",
        "    return log_entry\n",
        "\n",
        "def show_progress_summary():\n",
        "    \"\"\"Show research progress summary\"\"\"\n",
        "    try:\n",
        "        with open('/content/drive/MyDrive/Research/OurCode/research_log.json', 'r') as f:\n",
        "            logs = json.load(f)\n",
        "\n",
        "        print(\"üìà RESEARCH PROGRESS SUMMARY\")\n",
        "        print(\"=\"*40)\n",
        "        for log in logs[-5:]:  # Show last 5 days\n",
        "            print(f\"Day {log['day']} ({log['date']}):\")\n",
        "            print(f\"  ‚úÖ {', '.join(log['accomplishments'])}\")\n",
        "            if log['key_findings']:\n",
        "                print(f\"  üîç Key findings: {', '.join(log['key_findings'])}\")\n",
        "            print()\n",
        "    except FileNotFoundError:\n",
        "        print(\"No progress logs found yet\")\n",
        "\n",
        "# Example usage:\n",
        "# log_daily_progress(\n",
        "#     day_number=1,\n",
        "#     accomplishments=[\"Set up environment\", \"Loaded initial data\"],\n",
        "#     next_steps=[\"Start preprocessing\", \"Run first experiment\"],\n",
        "#     key_findings=[\"Data quality looks good\"]\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tga6i6ZmezyG"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# PROGRESS TRACKING CELL\n",
        "# ================================\n",
        "\n",
        "def log_daily_progress(day_number, accomplishments, next_steps, issues=None, key_findings=None):\n",
        "    \"\"\"Log daily research progress\"\"\"\n",
        "    log_entry = {\n",
        "        'day': day_number,\n",
        "        'date': datetime.now().strftime('%Y-%m-%d'),\n",
        "        'start_time': datetime.now().isoformat(),\n",
        "        'accomplishments': accomplishments,\n",
        "        'next_steps': next_steps,\n",
        "        'issues': issues or [],\n",
        "        'key_findings': key_findings or [],\n",
        "        'current_step': current_step,\n",
        "        'results_count': len(results) if results else 0\n",
        "    }\n",
        "\n",
        "    # Load existing logs\n",
        "    log_file = '/content/drive/MyDrive/Research/OurCode/research_log.json'\n",
        "    try:\n",
        "        with open(log_file, 'r') as f:\n",
        "            logs = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        logs = []\n",
        "\n",
        "    logs.append(log_entry)\n",
        "\n",
        "    with open(log_file, 'w') as f:\n",
        "        json.dump(logs, f, indent=2)\n",
        "\n",
        "    print(f\"üìù Day {day_number} progress logged!\")\n",
        "    return log_entry\n",
        "\n",
        "def show_progress_summary():\n",
        "    \"\"\"Show research progress summary\"\"\"\n",
        "    try:\n",
        "        with open('/content/drive/MyDrive/Research/OurCode/research_log.json', 'r') as f:\n",
        "            logs = json.load(f)\n",
        "\n",
        "        print(\"üìà RESEARCH PROGRESS SUMMARY\")\n",
        "        print(\"=\"*40)\n",
        "        for log in logs[-5:]:  # Show last 5 days\n",
        "            print(f\"Day {log['day']} ({log['date']}):\")\n",
        "            print(f\"  ‚úÖ {', '.join(log['accomplishments'])}\")\n",
        "            if log['key_findings']:\n",
        "                print(f\"  üîç Key findings: {', '.join(log['key_findings'])}\")\n",
        "            print()\n",
        "    except FileNotFoundError:\n",
        "        print(\"No progress logs found yet\")\n",
        "\n",
        "# Example usage:\n",
        "# log_daily_progress(\n",
        "#     day_number=1,\n",
        "#     accomplishments=[\"Set up environment\", \"Loaded initial data\"],\n",
        "#     next_steps=[\"Start preprocessing\", \"Run first experiment\"],\n",
        "#     key_findings=[\"Data quality looks good\"]\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "506sjhLeM6EY",
        "outputId": "fb4754d5-ef7f-4c21-85e0-eb54666e503e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üåÖ Ending research session...\n",
            "==================================================\n",
            "\n",
            "üìù SESSION SUMMARY INPUT:\n",
            "Brief summary of today's work: test\n",
            "‚ö† Using default values due to input error\n",
            "üíæ State saved successfully: /content/drive/MyDrive/Research/OurCode/research_state.pkl (285 bytes)\n",
            "üìÑ JSON backup saved: /content/drive/MyDrive/Research/OurCode/research_state.json\n",
            "‚úÖ Main state saved successfully\n",
            "üíæ Backup saved: /content/drive/MyDrive/Research/Backups/backup_day_03202506172006.pkl\n",
            "üìù Day 3 progress logged!\n",
            "\n",
            "üìä SESSION SUMMARY:\n",
            "   üìÖ Research Day: 3\n",
            "   üî¢ Current Step: 3\n",
            "   üìà Results Count: 0\n",
            "   üìù Notes: Session completed\n",
            "   ‚úÖ Accomplishments: 1\n",
            "   ‚û° Next Steps: 1\n",
            "‚ö† Drive sync may be incomplete\n",
            "\n",
            "‚úÖ Session saved successfully!\n",
            "üîÑ Ready for tomorrow's session\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "end_research_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAruyzPceALP"
      },
      "source": [
        "\n",
        "# ================================\n",
        "# 1. PROJECT STRUCTURE SETUP\n",
        "# ================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFmFYb_EfBXm"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "uNzz8hg0XajQ"
      },
      "outputs": [],
      "source": [
        "class ProjectManager:\n",
        "    \"\"\"Manages the complete project structure and environment\"\"\"\n",
        "\n",
        "    def __init__(self, base_dir=\"/content/drive/MyDrive/Research/OurCode/h265_streaming_research\"):\n",
        "        self.base_dir = Path(base_dir)\n",
        "        self.setup_project_structure()\n",
        "\n",
        "    def setup_project_structure(self):\n",
        "        \"\"\"Create comprehensive project directory structure\"\"\"\n",
        "        directories = [\n",
        "            \"src/encoding\", \"src/packaging\", \"src/streaming\", \"src/client\", \"src/analytics\", \"src/ml_models\",\n",
        "            \"content/samples\", \"content/test_videos\", \"encoded/profiles\", \"packaged/dash\", \"packaged/hls\",\n",
        "            \"web/player\", \"web/assets\", \"logs/encoding\", \"logs/streaming\", \"logs/analytics\",\n",
        "            \"research/data\", \"research/plots\", \"research/reports\", \"benchmarks/quality\", \"benchmarks/performance\",\n",
        "            \"config\", \"temp\", \"output\"\n",
        "        ]\n",
        "\n",
        "        for dir_path in directories:\n",
        "            full_path = self.base_dir / dir_path\n",
        "            full_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        print(f\"‚úÖ Project structure created in {self.base_dir}\")\n",
        "\n",
        "    def install_dependencies(self):\n",
        "        \"\"\"Install required system dependencies\"\"\"\n",
        "        print(\"üì¶ Installing system dependencies...\")\n",
        "\n",
        "        # Install system packages using apt\n",
        "        system_packages = [\n",
        "            \"ffmpeg\", \"x265\", \"mediainfo\", \"nodejs\", \"npm\", \"python3-pip\", \"git\"\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            # Update package list\n",
        "            subprocess.run([\"apt-get\", \"update\", \"-qq\"], check=True)\n",
        "\n",
        "            # Install packages\n",
        "            for package in system_packages:\n",
        "                try:\n",
        "                    subprocess.run([\"which\", package], check=True, capture_output=True)\n",
        "                    print(f\"‚úÖ {package} already installed\")\n",
        "                except subprocess.CalledProcessError:\n",
        "                    print(f\"üì• Installing {package}...\")\n",
        "                    subprocess.run([\"apt-get\", \"install\", \"-y\", package], check=True)\n",
        "\n",
        "            # Install Python packages\n",
        "            python_packages = [\n",
        "                \"opencv-python\", \"numpy\", \"matplotlib\", \"pandas\", \"scikit-learn\",\n",
        "                \"tensorflow\", \"plotly\", \"seaborn\", \"requests\", \"Pillow\"\n",
        "            ]\n",
        "\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\"] + python_packages)\n",
        "            print(\"‚úÖ All dependencies installed successfully\")\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ö†Ô∏è Some dependencies may not have installed correctly: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Installation error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnxdlTYAZt49"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yZY3QIwfq6N"
      },
      "source": [
        "# ================================\n",
        "# 2. FIXED CONTENT ANALYZER\n",
        "# ================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "F129537gT5c-"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 2. FIXED CONTENT ANALYZER\n",
        "# ================================\n",
        "\n",
        "video_path = \"/content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Videos/test video.mov\"\n",
        "class ContentAnalyzer:\n",
        "    \"\"\"Advanced video content analysis for encoding optimization\"\"\"\n",
        "\n",
        "    def __init__(self, base_path=\"/content/drive/MyDrive/Research/OurCode/h265_streaming_research\"):\n",
        "        self.base_dir = Path(base_path)\n",
        "        try:\n",
        "            self.face_cascade = cv2.CascadeClassifier(\n",
        "                cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
        "            )\n",
        "            print(\"‚úÖ Face detection initialized\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Face detection initialization failed: {e}\")\n",
        "            self.face_cascade = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def analyze_video_content(self, video_path):\n",
        "        \"\"\"Comprehensive video content analysis\"\"\"\n",
        "        print(f\"üîç Analyzing content: {video_path}\")\n",
        "\n",
        "        if not Path(video_path).exists():\n",
        "            print(f\"‚ùå Video file not found: {video_path}\")\n",
        "            return None\n",
        "\n",
        "        cap = cv2.VideoCapture(str(video_path))\n",
        "        if not cap.isOpened():\n",
        "            print(f\"‚ùå Could not open video: {video_path}\")\n",
        "            return None\n",
        "\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "        analysis_data = {\n",
        "            'video_info': {\n",
        "                'fps': fps,\n",
        "                'frame_count': frame_count,\n",
        "                'duration': frame_count / fps if fps > 0 else 0,\n",
        "                'resolution': f\"{width}x{height}\",\n",
        "                'width': width,\n",
        "                'height': height\n",
        "            },\n",
        "            'scenes': [],\n",
        "            'roi_frames': [],\n",
        "            'complexity_data': [],\n",
        "            'motion_analysis': []\n",
        "        }\n",
        "\n",
        "        prev_frame = None\n",
        "        scene_start = 0\n",
        "        sample_interval = max(1, frame_count // 100)  # Sample ~100 frames\n",
        "\n",
        "        print(f\"üìä Processing {frame_count} frames (sampling every {sample_interval} frames)...\")\n",
        "\n",
        "        for i in range(0, frame_count, sample_interval):\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            timestamp = i / fps if fps > 0 else 0\n",
        "\n",
        "            # Scene detection\n",
        "            if prev_frame is not None:\n",
        "                scene_change = self._detect_scene_change(prev_frame, gray)\n",
        "                if scene_change:\n",
        "                    analysis_data['scenes'].append({\n",
        "                        'start': scene_start,\n",
        "                        'end': i,\n",
        "                        'duration': (i - scene_start) / fps if fps > 0 else 0\n",
        "                    })\n",
        "                    scene_start = i\n",
        "\n",
        "            # ROI detection\n",
        "            roi_data = self._detect_regions_of_interest(frame)\n",
        "            analysis_data['roi_frames'].append({\n",
        "                'frame': i,\n",
        "                'timestamp': timestamp,\n",
        "                'roi_areas': roi_data\n",
        "            })\n",
        "\n",
        "            # Complexity analysis\n",
        "            complexity = self._calculate_frame_complexity(gray, prev_frame)\n",
        "            analysis_data['complexity_data'].append(complexity)\n",
        "\n",
        "            # Motion analysis\n",
        "            if prev_frame is not None:\n",
        "                motion = self._analyze_motion(prev_frame, gray)\n",
        "                analysis_data['motion_analysis'].append({\n",
        "                    'frame': i,\n",
        "                    'timestamp': timestamp,\n",
        "                    'motion_magnitude': motion\n",
        "                })\n",
        "\n",
        "            prev_frame = gray\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        # Calculate summary statistics\n",
        "        if analysis_data['complexity_data']:\n",
        "            complexities = [c['combined'] for c in analysis_data['complexity_data']]\n",
        "            motions = [m['motion_magnitude'] for m in analysis_data['motion_analysis']]\n",
        "            roi_densities = [len(r['roi_areas']) for r in analysis_data['roi_frames']]\n",
        "\n",
        "            analysis_data['summary'] = {\n",
        "                'avg_complexity': np.mean(complexities),\n",
        "                'max_complexity': np.max(complexities),\n",
        "                'min_complexity': np.min(complexities),\n",
        "                'avg_motion': np.mean(motions) if motions else 0,\n",
        "                'scene_count': len(analysis_data['scenes']),\n",
        "                'roi_density': np.mean(roi_densities),\n",
        "                'content_type': self._classify_content_type(np.mean(complexities), np.mean(motions) if motions else 0)\n",
        "            }\n",
        "\n",
        "        print(f\"‚úÖ Content analysis complete: {len(analysis_data['complexity_data'])} frames analyzed\")\n",
        "        print(f\"üìä Content summary: {analysis_data.get('summary', {})}\")\n",
        "\n",
        "        return analysis_data\n",
        "\n",
        "    def _detect_scene_change(self, prev_frame, current_frame):\n",
        "        \"\"\"Detect scene changes using histogram correlation\"\"\"\n",
        "        try:\n",
        "            hist1 = cv2.calcHist([prev_frame], [0], None, [256], [0, 256])\n",
        "            hist2 = cv2.calcHist([current_frame], [0], None, [256], [0, 256])\n",
        "            correlation = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)\n",
        "            return correlation < 0.7\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "    def _detect_regions_of_interest(self, frame):\n",
        "        \"\"\"Detect ROI using multiple techniques\"\"\"\n",
        "        roi_areas = []\n",
        "\n",
        "        try:\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Face detection\n",
        "            if self.face_cascade is not None:\n",
        "                faces = self.face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "                for (x, y, w, h) in faces:\n",
        "                    roi_areas.append({\n",
        "                        'type': 'face',\n",
        "                        'bbox': [int(x), int(y), int(w), int(h)],\n",
        "                        'priority': 1.0,\n",
        "                        'weight': 2.0\n",
        "                    })\n",
        "\n",
        "            # Edge-based ROI detection (simple alternative to saliency)\n",
        "            edges = cv2.Canny(gray, 50, 150)\n",
        "            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "            for contour in contours:\n",
        "                area = cv2.contourArea(contour)\n",
        "                if area > 1000:  # Minimum area threshold\n",
        "                    x, y, w, h = cv2.boundingRect(contour)\n",
        "                    roi_areas.append({\n",
        "                        'type': 'edge',\n",
        "                        'bbox': [int(x), int(y), int(w), int(h)],\n",
        "                        'priority': 0.6,\n",
        "                        'weight': 1.2\n",
        "                    })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è ROI detection error: {e}\")\n",
        "\n",
        "        return roi_areas\n",
        "\n",
        "    def _calculate_frame_complexity(self, gray, prev_frame=None):\n",
        "        \"\"\"Calculate multi-dimensional frame complexity\"\"\"\n",
        "        try:\n",
        "            # Spatial complexity (edge density)\n",
        "            edges = cv2.Canny(gray, 50, 150)\n",
        "            spatial_complexity = np.sum(edges) / (edges.shape[0] * edges.shape[1])\n",
        "\n",
        "            # Texture complexity (standard deviation)\n",
        "            texture_complexity = np.std(gray) / 255.0\n",
        "\n",
        "            # Temporal complexity\n",
        "            temporal_complexity = 0\n",
        "            if prev_frame is not None:\n",
        "                diff = cv2.absdiff(gray, prev_frame)\n",
        "                temporal_complexity = np.mean(diff) / 255.0\n",
        "\n",
        "            # Combined complexity score\n",
        "            combined = (spatial_complexity * 0.4 + texture_complexity * 0.3 + temporal_complexity * 0.3)\n",
        "\n",
        "            return {\n",
        "                'spatial': float(spatial_complexity),\n",
        "                'texture': float(texture_complexity),\n",
        "                'temporal': float(temporal_complexity),\n",
        "                'combined': float(combined)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Complexity calculation error: {e}\")\n",
        "            return {'spatial': 0.5, 'texture': 0.5, 'temporal': 0.0, 'combined': 0.5}\n",
        "\n",
        "    def _analyze_motion(self, prev_frame, current_frame):\n",
        "        \"\"\"Analyze motion between frames\"\"\"\n",
        "        try:\n",
        "            # Simple motion analysis using frame difference\n",
        "            diff = cv2.absdiff(prev_frame, current_frame)\n",
        "            motion_magnitude = np.mean(diff) / 255.0\n",
        "            return float(motion_magnitude)\n",
        "        except Exception:\n",
        "            return 0.0\n",
        "\n",
        "    def _classify_content_type(self, avg_complexity, avg_motion):\n",
        "        \"\"\"Classify content type based on complexity and motion\"\"\"\n",
        "        if avg_complexity < 0.3 and avg_motion < 0.1:\n",
        "            return \"low_complexity\"  # Presentations, static content\n",
        "        elif avg_complexity < 0.6 and avg_motion < 0.3:\n",
        "            return \"medium_complexity\"  # Interviews, talking heads\n",
        "        else:\n",
        "            return \"high_complexity\"  # Sports, action content\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Su5vbal1aMOG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hgv-Rq547O_",
        "outputId": "a8b55995-dde6-4ce3-e813-1288a84eb9b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Face detection initialized\n",
            "üîç Analyzing content: /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Videos/test video.mov\n",
            "üìä Processing 373 frames (sampling every 3 frames)...\n",
            "‚úÖ Content analysis complete: 125 frames analyzed\n",
            "üìä Content summary: {'avg_complexity': np.float64(0.7584114687235841), 'max_complexity': np.float64(0.839018815696997), 'min_complexity': np.float64(0.24241679698013913), 'avg_motion': np.float64(0.012091622050630413), 'scene_count': 0, 'roi_density': np.float64(15.496), 'content_type': 'high_complexity'}\n",
            "üìù Analysis saved to: /content/drive/MyDrive/Research/OurCode/video_analysis_result.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Path to the video you want to analyze\n",
        "    video_path = \"/content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Videos/test video.mov\"\n",
        "\n",
        "    # Instantiate the analyzer\n",
        "    analyzer = ContentAnalyzer(base_path=\"/content/drive/MyDrive/Research/OurCode/h265_streaming_research\")\n",
        "\n",
        "    # Run the analysis\n",
        "    analysis_result = analyzer.analyze_video_content(video_path)\n",
        "\n",
        "    # Save the result\n",
        "    if analysis_result:\n",
        "        output_path = Path(\"/content/drive/MyDrive/Research/OurCode/video_analysis_result.json\")\n",
        "        with open(output_path, \"w\") as f:\n",
        "            json.dump(analysis_result, f, indent=2)\n",
        "\n",
        "        print(f\"üìù Analysis saved to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPjrNAn_fzSY"
      },
      "source": [
        "# ================================\n",
        "# 3. FIXED H.265 ENCODER\n",
        "# ================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ktj8dM2Qf7NP"
      },
      "outputs": [],
      "source": [
        "class AdvancedH265Encoder:\n",
        "    \"\"\"Advanced H.265 encoder with ROI and content-adaptive optimization\"\"\"\n",
        "\n",
        "    def __init__(self, input_video, output_dir):\n",
        "        self.input_video = Path(input_video)\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.analyzer = ContentAnalyzer()\n",
        "        self.analysis_data = None\n",
        "\n",
        "        # Verify input exists\n",
        "        if not self.input_video.exists():\n",
        "            raise FileNotFoundError(f\"Input video not found: {input_video}\")\n",
        "\n",
        "    def encode_fixed_resolution_profiles(self):\n",
        "        \"\"\"Encode multiple quality profiles with fixed 1920x1080 resolution\"\"\"\n",
        "        print(f\"üé¨ Starting H.265 encoding: {self.input_video}\")\n",
        "\n",
        "        # Analyze content first\n",
        "        self.analysis_data = self.analyzer.analyze_video_content(self.input_video)\n",
        "        if not self.analysis_data:\n",
        "            print(\"‚ùå Content analysis failed\")\n",
        "            return {}\n",
        "\n",
        "        # Define quality profiles (all 1920x1080)\n",
        "        profiles = {\n",
        "            \"ultra_high\": {\n",
        "                \"target_bitrate\": \"8000k\",\n",
        "                \"max_bitrate\": \"9600k\",\n",
        "                \"buffer_size\": \"16000k\",\n",
        "                \"crf\": 18,\n",
        "                \"framerate\": 60,\n",
        "                \"preset\": \"slow\",\n",
        "                \"x265_params\": \"rd=6:psy-rd=2.5:aq-mode=3:aq-strength=0.8\"\n",
        "            },\n",
        "            \"high\": {\n",
        "                \"target_bitrate\": \"5000k\",\n",
        "                \"max_bitrate\": \"6000k\",\n",
        "                \"buffer_size\": \"10000k\",\n",
        "                \"crf\": 20,\n",
        "                \"framerate\": 30,\n",
        "                \"preset\": \"medium\",\n",
        "                \"x265_params\": \"rd=4:psy-rd=2.0:aq-mode=3:aq-strength=1.0\"\n",
        "            },\n",
        "            \"medium\": {\n",
        "                \"target_bitrate\": \"3000k\",\n",
        "                \"max_bitrate\": \"3600k\",\n",
        "                \"buffer_size\": \"6000k\",\n",
        "                \"crf\": 23,\n",
        "                \"framerate\": 30,\n",
        "                \"preset\": \"medium\",\n",
        "                \"x265_params\": \"rd=3:psy-rd=1.5:aq-mode=2:aq-strength=1.2\"\n",
        "            },\n",
        "            \"low\": {\n",
        "                \"target_bitrate\": \"1500k\",\n",
        "                \"max_bitrate\": \"1800k\",\n",
        "                \"buffer_size\": \"3000k\",\n",
        "                \"crf\": 26,\n",
        "                \"framerate\": 24,\n",
        "                \"preset\": \"fast\",\n",
        "                \"x265_params\": \"rd=2:psy-rd=1.0:aq-mode=2:aq-strength=1.4\"\n",
        "            },\n",
        "            \"ultra_low\": {\n",
        "                \"target_bitrate\": \"800k\",\n",
        "                \"max_bitrate\": \"960k\",\n",
        "                \"buffer_size\": \"1600k\",\n",
        "                \"crf\": 30,\n",
        "                \"framerate\": 15,\n",
        "                \"preset\": \"veryfast\",\n",
        "                \"x265_params\": \"rd=1:psy-rd=0.5:aq-mode=1:aq-strength=1.6\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Content-adaptive parameter adjustment\n",
        "        if self.analysis_data.get('summary', {}).get('avg_complexity', 0) > 0.6:\n",
        "            print(\"üìà High complexity content detected - boosting quality parameters\")\n",
        "            for profile in profiles.values():\n",
        "                profile['crf'] = max(15, profile['crf'] - 2)\n",
        "\n",
        "        # Encode each profile\n",
        "        encoded_files = {}\n",
        "        for profile_name, params in profiles.items():\n",
        "            print(f\"\\nüîÑ Encoding {profile_name} profile...\")\n",
        "\n",
        "            output_file = self.output_dir / f\"video_{profile_name}.mp4\"\n",
        "\n",
        "            # Build FFmpeg command\n",
        "            cmd = [\n",
        "                \"ffmpeg\", \"-i\", str(self.input_video),\n",
        "                \"-c:v\", \"libx265\",\n",
        "                \"-preset\", params[\"preset\"],\n",
        "                \"-crf\", str(params[\"crf\"]),\n",
        "                \"-b:v\", params[\"target_bitrate\"],\n",
        "                \"-maxrate\", params[\"max_bitrate\"],\n",
        "                \"-bufsize\", params[\"buffer_size\"],\n",
        "                \"-vf\", \"scale=1920:1080:force_original_aspect_ratio=decrease,pad=1920:1080:(ow-iw)/2:(oh-ih)/2\",  # Fixed resolution scaling\n",
        "                \"-r\", str(params[\"framerate\"]),\n",
        "                \"-g\", \"60\",  # GOP size\n",
        "                \"-keyint_min\", \"60\",\n",
        "                \"-sc_threshold\", \"0\",\n",
        "                \"-x265-params\", params[\"x265_params\"],\n",
        "                \"-c:a\", \"aac\",\n",
        "                \"-b:a\", \"128k\",\n",
        "                \"-ar\", \"44100\",\n",
        "                \"-ac\", \"2\",\n",
        "                \"-movflags\", \"+faststart\",\n",
        "                str(output_file),\n",
        "                \"-y\"\n",
        "            ]\n",
        "\n",
        "            try:\n",
        "                print(f\"Running: {' '.join(cmd[:10])}...\")  # Print abbreviated command\n",
        "                result = subprocess.run(cmd, capture_output=True, text=True, timeout=1800)  # 30 min timeout\n",
        "\n",
        "                if result.returncode == 0:\n",
        "                    print(f\"‚úÖ Successfully encoded {profile_name}\")\n",
        "                    encoded_files[profile_name] = output_file\n",
        "\n",
        "                    # Basic file info\n",
        "                    file_size = output_file.stat().st_size / (1024*1024)  # MB\n",
        "                    print(f\"üìÅ File size: {file_size:.1f} MB\")\n",
        "\n",
        "                else:\n",
        "                    print(f\"‚ùå Failed to encode {profile_name}\")\n",
        "                    if result.stderr:\n",
        "                        print(f\"Error: {result.stderr[:200]}...\")  # First 200 chars of error\n",
        "\n",
        "            except subprocess.TimeoutExpired:\n",
        "                print(f\"‚è∞ Encoding timeout for {profile_name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Encoding error for {profile_name}: {e}\")\n",
        "\n",
        "        print(f\"\\n‚úÖ Encoding complete. Generated {len(encoded_files)} profiles.\")\n",
        "        return encoded_files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # üîÅ Replace these with your actual paths\n",
        "    input_path = \"/content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Videos/test video.mov\"\n",
        "    output_path = \"/content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Encoded\"\n",
        "\n",
        "    # ‚ñ∂Ô∏è Run encoder\n",
        "    encoder = AdvancedH265Encoder(input_video=input_path, output_dir=output_path)\n",
        "    results = encoder.encode_fixed_resolution_profiles()\n",
        "\n",
        "    # üìã Print summary of output files\n",
        "    print(\"\\nüì¶ Encoding Results:\")\n",
        "    for profile, path in results.items():\n",
        "        print(f\"{profile}: {path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Bgth1tSmJUq",
        "outputId": "84559627-56cb-4fbc-9408-e03722407d21"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Face detection initialized\n",
            "üé¨ Starting H.265 encoding: /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Videos/test video.mov\n",
            "üîç Analyzing content: /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Videos/test video.mov\n",
            "üìä Processing 373 frames (sampling every 3 frames)...\n",
            "‚úÖ Content analysis complete: 125 frames analyzed\n",
            "üìä Content summary: {'avg_complexity': np.float64(0.7584114687235841), 'max_complexity': np.float64(0.839018815696997), 'min_complexity': np.float64(0.24241679698013913), 'avg_motion': np.float64(0.012091622050630413), 'scene_count': 0, 'roi_density': np.float64(15.496), 'content_type': 'high_complexity'}\n",
            "üìà High complexity content detected - boosting quality parameters\n",
            "\n",
            "üîÑ Encoding ultra_high profile...\n",
            "Running: ffmpeg -i /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Videos/test video.mov -c:v libx265 -preset slow -crf 16 -b:v...\n",
            "‚úÖ Successfully encoded ultra_high\n",
            "üìÅ File size: 9.5 MB\n",
            "\n",
            "üîÑ Encoding high profile...\n",
            "Running: ffmpeg -i /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Videos/test video.mov -c:v libx265 -preset medium -crf 18 -b:v...\n",
            "‚úÖ Successfully encoded high\n",
            "üìÅ File size: 5.8 MB\n",
            "\n",
            "üîÑ Encoding medium profile...\n",
            "Running: ffmpeg -i /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Videos/test video.mov -c:v libx265 -preset medium -crf 21 -b:v...\n",
            "‚úÖ Successfully encoded medium\n",
            "üìÅ File size: 3.2 MB\n",
            "\n",
            "üîÑ Encoding low profile...\n",
            "Running: ffmpeg -i /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Videos/test video.mov -c:v libx265 -preset fast -crf 24 -b:v...\n",
            "‚úÖ Successfully encoded low\n",
            "üìÅ File size: 2.0 MB\n",
            "\n",
            "üîÑ Encoding ultra_low profile...\n",
            "Running: ffmpeg -i /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Videos/test video.mov -c:v libx265 -preset veryfast -crf 28 -b:v...\n",
            "‚úÖ Successfully encoded ultra_low\n",
            "üìÅ File size: 1.2 MB\n",
            "\n",
            "‚úÖ Encoding complete. Generated 5 profiles.\n",
            "\n",
            "üì¶ Encoding Results:\n",
            "ultra_high: /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Encoded/video_ultra_high.mp4\n",
            "high: /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Encoded/video_high.mp4\n",
            "medium: /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Encoded/video_medium.mp4\n",
            "low: /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Encoded/video_low.mp4\n",
            "ultra_low: /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Encoded/video_ultra_low.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "afZ3Woh8fy8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e12ea6c-7019-4dfc-d9d1-236986130102"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ TensorFlow available - Using ML-based predictor\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Streaming Quality Adaptation System\n",
        "Complete implementation with bandwidth prediction and quality adaptation\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import threading\n",
        "import random\n",
        "import json\n",
        "\n",
        "# ML dependencies check\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras import layers, models, Input\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    HAS_ML = True\n",
        "    print(\"‚úÖ TensorFlow available - Using ML-based predictor\")\n",
        "except ImportError:\n",
        "    HAS_ML = False\n",
        "    print(\"‚ö† TensorFlow not available - Using statistical predictor\")\n",
        "\n",
        "# Bandwidth Predictor Classes\n",
        "if HAS_ML:\n",
        "    class BandwidthPredictor:\n",
        "        \"\"\"Fixed LSTM-based bandwidth predictor\"\"\"\n",
        "\n",
        "        def __init__(self, sequence_length=10):\n",
        "            self.sequence_length = sequence_length\n",
        "            self.history = deque(maxlen=sequence_length)\n",
        "            self.is_trained = True\n",
        "            self.model = None  # Add this if you're using ML-based prediction\n",
        "\n",
        "\n",
        "        def build_lstm_model(self):\n",
        "            \"\"\"Build LSTM model with proper Input layer\"\"\"\n",
        "            inputs = Input(shape=(self.sequence_length, 4))\n",
        "\n",
        "            x = layers.LSTM(64, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)(inputs)\n",
        "            x = layers.LSTM(32, return_sequences=False, dropout=0.1, recurrent_dropout=0.1)(x)\n",
        "            x = layers.Dense(16, activation='relu')(x)\n",
        "            x = layers.Dropout(0.2)(x)\n",
        "            x = layers.Dense(8, activation='relu')(x)\n",
        "            outputs = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "            model = models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "            model.compile(\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                loss='mse',\n",
        "                metrics=['mae']\n",
        "            )\n",
        "\n",
        "            return model\n",
        "\n",
        "        def generate_training_data(self, num_samples=1000):\n",
        "            \"\"\"Generate realistic training data\"\"\"\n",
        "            print(f\"üìä Generating {num_samples} training samples...\")\n",
        "\n",
        "            np.random.seed(42)\n",
        "            training_data = []\n",
        "\n",
        "            scenarios = [\n",
        "                {'base_bw': 1000000, 'variance': 0.3, 'name': 'Poor'},\n",
        "                {'base_bw': 3000000, 'variance': 0.2, 'name': 'Medium'},\n",
        "                {'base_bw': 8000000, 'variance': 0.15, 'name': 'Good'},\n",
        "                {'base_bw': 15000000, 'variance': 0.1, 'name': 'Excellent'}\n",
        "            ]\n",
        "\n",
        "            for i in range(num_samples):\n",
        "                scenario = scenarios[i % len(scenarios)]\n",
        "\n",
        "                base_bandwidth = scenario['base_bw']\n",
        "                variance = scenario['variance']\n",
        "\n",
        "                time_factor = np.sin(2 * np.pi * i / 100) * 0.2 + 1\n",
        "                bandwidth = base_bandwidth * time_factor * (1 + np.random.normal(0, variance))\n",
        "                bandwidth = max(100000, bandwidth)\n",
        "\n",
        "                base_rtt = 200 - (bandwidth / 100000)\n",
        "                rtt = max(5, base_rtt + np.random.normal(0, 20))\n",
        "                buffer_level = np.random.uniform(0, 30)\n",
        "\n",
        "                training_data.append({\n",
        "                    'bandwidth': bandwidth,\n",
        "                    'rtt': rtt,\n",
        "                    'buffer_level': buffer_level,\n",
        "                    'timestamp': time.time() + i\n",
        "                })\n",
        "\n",
        "            return training_data\n",
        "\n",
        "        def preprocess_training_data(self, bandwidth_history):\n",
        "            \"\"\"Preprocess data into LSTM sequences\"\"\"\n",
        "            X, y = [], []\n",
        "\n",
        "            for i in range(len(bandwidth_history) - self.sequence_length):\n",
        "                sequence = bandwidth_history[i:i + self.sequence_length]\n",
        "                target = bandwidth_history[i + self.sequence_length]['bandwidth']\n",
        "\n",
        "                features = []\n",
        "                for sample in sequence:\n",
        "                    features.append([\n",
        "                        sample['bandwidth'] / 1000000,\n",
        "                        sample['rtt'] / 100,\n",
        "                        sample['buffer_level'] / 30,\n",
        "                        (sample['timestamp'] % 86400) / 86400\n",
        "                    ])\n",
        "\n",
        "                X.append(features)\n",
        "                y.append(target / 1000000)\n",
        "\n",
        "            return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
        "\n",
        "        def train_model(self, training_data=None, epochs=10):\n",
        "            \"\"\"Train the bandwidth prediction model\"\"\"\n",
        "            print(\"üéØ Training bandwidth prediction model...\")\n",
        "\n",
        "            if training_data is None:\n",
        "                training_data = self.generate_training_data()\n",
        "\n",
        "            X, y = self.preprocess_training_data(training_data)\n",
        "\n",
        "            if len(X) == 0:\n",
        "                print(\"‚ùå No training data available\")\n",
        "                return None\n",
        "\n",
        "            self.model = self.build_lstm_model()\n",
        "\n",
        "            split_idx = int(len(X) * 0.8)\n",
        "            X_train, X_val = X[:split_idx], X[split_idx:]\n",
        "            y_train, y_val = y[:split_idx], y[split_idx:]\n",
        "\n",
        "            callbacks = [\n",
        "                tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
        "                tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3)\n",
        "            ]\n",
        "\n",
        "            try:\n",
        "\n",
        "                history = self.model.fit(\n",
        "                    X_train, y_train,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    callbacks=callbacks,\n",
        "                    verbose=1\n",
        "                )\n",
        "\n",
        "                self.is_trained = True\n",
        "                val_loss, val_mae = self.model.evaluate(X_val, y_val, verbose=0)\n",
        "                print(f\"‚úÖ Model trained - Val MAE: {val_mae:.4f} Mbps\")\n",
        "                return history\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Training failed: {e}\")\n",
        "\n",
        "                self.is_trained = False\n",
        "                return None\n",
        "\n",
        "        def predict_bandwidth(self, current_data):\n",
        "            \"\"\"Predict future bandwidth\"\"\"\n",
        "            if not self.is_trained or self.model is None:\n",
        "                return self.fallback_prediction(current_data)\n",
        "\n",
        "            self.history.append(current_data)\n",
        "\n",
        "            if len(self.history) < self.sequence_length:\n",
        "                return self.fallback_prediction(current_data)\n",
        "\n",
        "            sequence = []\n",
        "            for sample in list(self.history):\n",
        "                sequence.append([\n",
        "                    sample['bandwidth'] / 1000000,\n",
        "                    sample['rtt'] / 100,\n",
        "                    sample['buffer_level'] / 30,\n",
        "                    (sample['timestamp'] % 86400) / 86400\n",
        "                ])\n",
        "\n",
        "            sequence = np.array([sequence], dtype=np.float32)\n",
        "\n",
        "            try:\n",
        "                prediction_mbps = self.model.predict(sequence, verbose=0)[0][0]\n",
        "                prediction_bps = prediction_mbps * 1000000\n",
        "                confidence = min(0.9, max(0.3, 0.7 + np.random.normal(0, 0.1)))\n",
        "\n",
        "                return {\n",
        "                    'predicted_bandwidth': max(100000, prediction_bps),\n",
        "                    'confidence': confidence,\n",
        "                    'model_type': 'lstm'\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö† Prediction error: {e}\")\n",
        "                return self.fallback_prediction(current_data)\n",
        "\n",
        "        def fallback_prediction(self, current_data):\n",
        "            \"\"\"Fallback prediction when ML model fails\"\"\"\n",
        "            self.history.append(current_data)\n",
        "\n",
        "            if len(self.history) < 3:\n",
        "                return {\n",
        "                    'predicted_bandwidth': current_data['bandwidth'],\n",
        "                    'confidence': 0.3,\n",
        "                    'model_type': 'simple'\n",
        "                }\n",
        "\n",
        "            recent_values = [h['bandwidth'] for h in list(self.history)[-3:]]\n",
        "            predicted = np.mean(recent_values)\n",
        "\n",
        "            return {\n",
        "                'predicted_bandwidth': predicted,\n",
        "                'confidence': 0.5,\n",
        "                'model_type': 'moving_average'\n",
        "            }\n",
        "\n",
        "else:\n",
        "    class BandwidthPredictor:\n",
        "        def  __init__(self, sequence_length=10):\n",
        "            self.sequence_length = sequence_length\n",
        "            self.history = deque(maxlen=sequence_length)\n",
        "            self.is_trained = True\n",
        "\n",
        "        def train_model(self, training_data=None, epochs=10):\n",
        "            print(\"üìä Using simple statistical predictor (TensorFlow not available)\")\n",
        "            return True\n",
        "\n",
        "        def predict_bandwidth(self, current_data):\n",
        "            self.history.append(current_data)\n",
        "\n",
        "            if len(self.history) < 3:\n",
        "                return {\n",
        "                    'predicted_bandwidth': current_data['bandwidth'],\n",
        "                    'confidence': 0.3,\n",
        "                    'model_type': 'simple'\n",
        "                }\n",
        "\n",
        "            recent_values = [h['bandwidth'] for h in list(self.history)[-3:]]\n",
        "            predicted = np.mean(recent_values)\n",
        "\n",
        "            return {\n",
        "                'predicted_bandwidth': predicted,\n",
        "                'confidence': 0.6,\n",
        "                'model_type': 'statistical'\n",
        "            }\n",
        "\n",
        "\n",
        "class QualityAdaptationEngine:\n",
        "    \"\"\"Advanced quality adaptation engine with ML-enhanced bandwidth prediction\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.bandwidth_predictor = BandwidthPredictor()\n",
        "        self.quality_levels = {\n",
        "            'ultra_high': {'bitrate': 8000000, 'framerate': 60, 'priority': 5},\n",
        "            'high': {'bitrate': 5000000, 'framerate': 30, 'priority': 4},\n",
        "            'medium': {'bitrate': 3000000, 'framerate': 30, 'priority': 3},\n",
        "            'low': {'bitrate': 1500000, 'framerate': 24, 'priority': 2},\n",
        "            'ultra_low': {'bitrate': 800000, 'framerate': 15, 'priority': 1}\n",
        "        }\n",
        "        self.current_quality = 'medium'\n",
        "        self.buffer_target = 10.0\n",
        "        self.buffer_panic = 3.0\n",
        "        self.switching_cooldown = 5.0\n",
        "        self.last_switch_time = 0\n",
        "        self.adaptation_history = deque(maxlen=100)\n",
        "\n",
        "    def train_predictor(self):\n",
        "        \"\"\"Train the bandwidth predictor\"\"\"\n",
        "        print(\"üß† Training bandwidth predictor...\")\n",
        "        return self.bandwidth_predictor.train_model()\n",
        "\n",
        "    def select_quality(self, network_state, buffer_level):\n",
        "        \"\"\"Select optimal quality level based on network and buffer state\"\"\"\n",
        "        current_time = time.time()\n",
        "\n",
        "        prediction = self.bandwidth_predictor.predict_bandwidth(network_state)\n",
        "        predicted_bw = prediction['predicted_bandwidth']\n",
        "        confidence = prediction['confidence']\n",
        "\n",
        "        safety_margin = 0.7 + (confidence * 0.3)\n",
        "        effective_bandwidth = predicted_bw * safety_margin\n",
        "\n",
        "        buffer_factor = self._calculate_buffer_factor(buffer_level)\n",
        "        adjusted_bandwidth = effective_bandwidth * buffer_factor\n",
        "\n",
        "        best_quality = self._find_best_quality(adjusted_bandwidth)\n",
        "        should_switch = self._should_switch_quality(best_quality, current_time, buffer_level)\n",
        "\n",
        "        if should_switch:\n",
        "            self.current_quality = best_quality\n",
        "            self.last_switch_time = current_time\n",
        "\n",
        "        self.adaptation_history.append({\n",
        "            'timestamp': current_time,\n",
        "            'predicted_bw': predicted_bw,\n",
        "            'effective_bw': effective_bandwidth,\n",
        "            'buffer_level': buffer_level,\n",
        "            'selected_quality': self.current_quality,\n",
        "            'switched': should_switch,\n",
        "            'confidence': confidence\n",
        "        })\n",
        "\n",
        "        return {\n",
        "            'quality_level': self.current_quality,\n",
        "            'bitrate': self.quality_levels[self.current_quality]['bitrate'],\n",
        "            'framerate': self.quality_levels[self.current_quality]['framerate'],\n",
        "            'predicted_bandwidth': predicted_bw,\n",
        "            'confidence': confidence,\n",
        "            'switched': should_switch,\n",
        "            'safety_margin': safety_margin\n",
        "        }\n",
        "\n",
        "    def _calculate_buffer_factor(self, buffer_level):\n",
        "        \"\"\"Calculate buffer-based adjustment factor\"\"\"\n",
        "        if buffer_level < self.buffer_panic:\n",
        "            return 0.5\n",
        "        elif buffer_level < self.buffer_target * 0.5:\n",
        "            return 0.7\n",
        "        elif buffer_level < self.buffer_target:\n",
        "            return 0.85\n",
        "        elif buffer_level > self.buffer_target * 2:\n",
        "            return 1.3\n",
        "        else:\n",
        "            return 1.0\n",
        "\n",
        "    def _find_best_quality(self, available_bandwidth):\n",
        "        \"\"\"Find the best quality level for given bandwidth\"\"\"\n",
        "        sorted_qualities = sorted(self.quality_levels.items(),\n",
        "                                key=lambda x: x[1]['bitrate'], reverse=True)\n",
        "\n",
        "        for quality_name, quality_info in sorted_qualities:\n",
        "            if quality_info['bitrate'] <= available_bandwidth:\n",
        "                return quality_name\n",
        "\n",
        "        return 'ultra_low'\n",
        "\n",
        "    def _should_switch_quality(self, target_quality, current_time, buffer_level):\n",
        "        \"\"\"Determine if quality switch should occur with hysteresis\"\"\"\n",
        "        if target_quality == self.current_quality:\n",
        "            return False\n",
        "\n",
        "        time_since_switch = current_time - self.last_switch_time\n",
        "        if time_since_switch < self.switching_cooldown and buffer_level > self.buffer_panic:\n",
        "            return False\n",
        "\n",
        "        current_priority = self.quality_levels[self.current_quality]['priority']\n",
        "        target_priority = self.quality_levels[target_quality]['priority']\n",
        "\n",
        "        if buffer_level < self.buffer_panic and target_priority < current_priority:\n",
        "            return True\n",
        "\n",
        "        if target_priority > current_priority:\n",
        "            return True\n",
        "\n",
        "        if target_priority < current_priority:\n",
        "            return buffer_level < self.buffer_target * 0.8\n",
        "\n",
        "        return False\n",
        "\n",
        "    def get_adaptation_stats(self):\n",
        "        \"\"\"Get comprehensive adaptation statistics\"\"\"\n",
        "        if not self.adaptation_history:\n",
        "            return {}\n",
        "\n",
        "        history = list(self.adaptation_history)\n",
        "        switches = sum(1 for h in history if h['switched'])\n",
        "        quality_scores = [self.quality_levels[h['selected_quality']]['priority'] for h in history]\n",
        "\n",
        "        return {\n",
        "            'total_adaptations': len(history),\n",
        "            'quality_switches': switches,\n",
        "            'switch_rate': switches / len(history) if history else 0,\n",
        "            'average_quality_score': np.mean(quality_scores),\n",
        "            'min_quality_score': min(quality_scores),\n",
        "            'max_quality_score': max(quality_scores),\n",
        "            'average_confidence': np.mean([h['confidence'] for h in history])\n",
        "        }\n",
        "\n",
        "\n",
        "# Network Simulator for Testing\n",
        "class NetworkSimulator:\n",
        "    \"\"\"Simulates realistic network conditions for testing\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.base_bandwidth = 5000000  # 5 Mbps\n",
        "        self.current_bandwidth = self.base_bandwidth\n",
        "        self.time_start = time.time()\n",
        "\n",
        "    def get_network_state(self):\n",
        "        \"\"\"Generate realistic network conditions\"\"\"\n",
        "        current_time = time.time()\n",
        "        elapsed = current_time - self.time_start\n",
        "\n",
        "        # Simulate network fluctuations\n",
        "        time_factor = np.sin(elapsed / 10) * 0.3 + 1  # Slow variations\n",
        "        noise_factor = 1 + np.random.normal(0, 0.2)   # Random fluctuations\n",
        "\n",
        "        # Occasional network drops\n",
        "        if np.random.random() < 0.05:  # 5% chance of network issue\n",
        "            bandwidth = self.base_bandwidth * 0.3\n",
        "        else:\n",
        "            bandwidth = self.base_bandwidth * time_factor * noise_factor\n",
        "\n",
        "        bandwidth = max(500000, bandwidth)  # Minimum 500 Kbps\n",
        "\n",
        "        # Correlated RTT (higher bandwidth = lower RTT)\n",
        "        rtt = 200 - (bandwidth / 50000) + np.random.normal(0, 10)\n",
        "        rtt = max(10, rtt)\n",
        "\n",
        "        # Simulated buffer level\n",
        "        buffer_level = np.random.uniform(2, 25)\n",
        "\n",
        "        return {\n",
        "            'bandwidth': bandwidth,\n",
        "            'rtt': rtt,\n",
        "            'buffer_level': buffer_level,\n",
        "            'timestamp': current_time\n",
        "        }\n",
        "\n",
        "\n",
        "# Demo and Testing Functions\n",
        "def run_quality_adaptation_demo():\n",
        "    \"\"\"Run a complete demonstration of the quality adaptation system\"\"\"\n",
        "    print(\"üé¨ Starting Streaming Quality Adaptation Demo\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Initialize components\n",
        "    engine = QualityAdaptationEngine()\n",
        "    simulator = NetworkSimulator()\n",
        "\n",
        "    # Train the predictor\n",
        "    print(\"\\n1. Training Bandwidth Predictor...\")\n",
        "    training_result = engine.train_predictor()\n",
        "\n",
        "    if training_result is None and HAS_ML:\n",
        "        print(\"‚ùå Training failed, falling back to statistical predictor\")\n",
        "\n",
        "    print(\"\\n2. Starting Quality Adaptation Simulation...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Run simulation\n",
        "    adaptation_count = 0\n",
        "    for i in range(30):  # 30 adaptation cycles\n",
        "        # Get current network state\n",
        "        network_state = simulator.get_network_state()\n",
        "\n",
        "        # Select quality\n",
        "        quality_decision = engine.select_quality(network_state, network_state['buffer_level'])\n",
        "\n",
        "        adaptation_count += 1\n",
        "\n",
        "        # Display results\n",
        "        print(f\"\\nüìä Adaptation #{adaptation_count}\")\n",
        "        print(f\"   Network: {network_state['bandwidth']/1000000:.2f} Mbps, RTT: {network_state['rtt']:.0f}ms\")\n",
        "        print(f\"   Buffer: {network_state['buffer_level']:.1f}s\")\n",
        "        print(f\"   Predicted BW: {quality_decision['predicted_bandwidth']/1000000:.2f} Mbps\")\n",
        "        print(f\"   Confidence: {quality_decision['confidence']:.2f}\")\n",
        "        print(f\"   Selected Quality: {quality_decision['quality_level'].upper()}\")\n",
        "        print(f\"   Target Bitrate: {quality_decision['bitrate']/1000000:.2f} Mbps @ {quality_decision['framerate']}fps\")\n",
        "\n",
        "        if quality_decision['switched']:\n",
        "            print(\"   üîÑ QUALITY SWITCHED!\")\n",
        "\n",
        "        # Simulate time between adaptations\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    # Show final statistics\n",
        "    print(\"\\n3. Final Adaptation Statistics\")\n",
        "    print(\"=\" * 50)\n",
        "    stats = engine.get_adaptation_stats()\n",
        "\n",
        "    for key, value in stats.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f\"   {key.replace('_', ' ').title()}: {value:.3f}\")\n",
        "        else:\n",
        "            print(f\"   {key.replace('_', ' ').title()}: {value}\")\n",
        "\n",
        "    print(\"\\n‚úÖ Demo completed successfully!\")\n",
        "\n",
        "\n",
        "def test_bandwidth_predictor():\n",
        "    \"\"\"Test the bandwidth predictor independently\"\"\"\n",
        "    print(\"üß™ Testing Bandwidth Predictor\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    predictor = BandwidthPredictor()\n",
        "\n",
        "    # Train predictor\n",
        "    print(\"Training predictor...\")\n",
        "    predictor.train_model(epochs=5)\n",
        "\n",
        "    # Test predictions\n",
        "    print(\"\\nTesting predictions:\")\n",
        "    simulator = NetworkSimulator()\n",
        "\n",
        "    for i in range(10):\n",
        "        network_state = simulator.get_network_state()\n",
        "        prediction = predictor.predict_bandwidth(network_state)\n",
        "\n",
        "        print(f\"Actual: {network_state['bandwidth']/1000000:.2f} Mbps | \"\n",
        "              f\"Predicted: {prediction['predicted_bandwidth']/1000000:.2f} Mbps | \"\n",
        "              f\"Confidence: {prediction['confidence']:.2f} | \"\n",
        "              f\"Model: {prediction['model_type']}\")\n",
        "\n",
        "        time.sleep(0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"üéØ Streaming Quality Adaptation System\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    try:\n",
        "        # Run the main demonstration\n",
        "        run_quality_adaptation_demo()\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 40)\n",
        "        print(\"üîß Additional Tests Available:\")\n",
        "        print(\"   - Uncomment test_bandwidth_predictor() to test predictor separately\")\n",
        "\n",
        "        # Uncomment to test bandwidth predictor separately\n",
        "        # print(\"\\n\")\n",
        "        # test_bandwidth_predictor()\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\n‚èπ Demo interrupted by user\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYULfZgnwnvm",
        "outputId": "ff617e52-5ad7-4817-f2b2-d1fdc23aa1cb"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Streaming Quality Adaptation System\n",
            "========================================\n",
            "üé¨ Starting Streaming Quality Adaptation Demo\n",
            "==================================================\n",
            "\n",
            "1. Training Bandwidth Predictor...\n",
            "üß† Training bandwidth predictor...\n",
            "üéØ Training bandwidth prediction model...\n",
            "üìä Generating 1000 training samples...\n",
            "Epoch 1/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 83ms/step - loss: 70.2687 - mae: 6.1633 - val_loss: 49.6825 - val_mae: 5.0645 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 48.1808 - mae: 5.0736 - val_loss: 31.2151 - val_mae: 4.6424 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 35.8098 - mae: 5.0162 - val_loss: 30.3620 - val_mae: 4.6626 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 34.6781 - mae: 4.9373 - val_loss: 30.3179 - val_mae: 4.6384 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 32.1442 - mae: 4.7513 - val_loss: 30.0478 - val_mae: 4.6148 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 36.1040 - mae: 5.1371 - val_loss: 29.8213 - val_mae: 4.5371 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 31.6636 - mae: 4.6948 - val_loss: 28.3553 - val_mae: 4.4490 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 30.3032 - mae: 4.6622 - val_loss: 25.8110 - val_mae: 4.1861 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 27.5136 - mae: 4.3265 - val_loss: 19.9546 - val_mae: 3.6352 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 23.6642 - mae: 3.8855 - val_loss: 10.6299 - val_mae: 2.3223 - learning_rate: 0.0010\n",
            "‚úÖ Model trained - Val MAE: 2.3223 Mbps\n",
            "\n",
            "2. Starting Quality Adaptation Simulation...\n",
            "==================================================\n",
            "\n",
            "üìä Adaptation #1\n",
            "   Network: 4.86 Mbps, RTT: 105ms\n",
            "   Buffer: 20.6s\n",
            "   Predicted BW: 4.86 Mbps\n",
            "   Confidence: 0.30\n",
            "   Selected Quality: MEDIUM\n",
            "   Target Bitrate: 3.00 Mbps @ 30fps\n",
            "\n",
            "üìä Adaptation #2\n",
            "   Network: 4.84 Mbps, RTT: 94ms\n",
            "   Buffer: 5.6s\n",
            "   Predicted BW: 4.84 Mbps\n",
            "   Confidence: 0.50\n",
            "   Selected Quality: MEDIUM\n",
            "   Target Bitrate: 3.00 Mbps @ 30fps\n",
            "\n",
            "üìä Adaptation #3\n",
            "   Network: 5.17 Mbps, RTT: 88ms\n",
            "   Buffer: 15.0s\n",
            "   Predicted BW: 5.06 Mbps\n",
            "   Confidence: 0.50\n",
            "   Selected Quality: MEDIUM\n",
            "   Target Bitrate: 3.00 Mbps @ 30fps\n",
            "\n",
            "üìä Adaptation #4\n",
            "   Network: 4.76 Mbps, RTT: 108ms\n",
            "   Buffer: 13.6s\n",
            "   Predicted BW: 4.89 Mbps\n",
            "   Confidence: 0.50\n",
            "   Selected Quality: MEDIUM\n",
            "   Target Bitrate: 3.00 Mbps @ 30fps\n",
            "\n",
            "üìä Adaptation #5\n",
            "   Network: 5.40 Mbps, RTT: 82ms\n",
            "   Buffer: 13.2s\n",
            "   Predicted BW: 5.19 Mbps\n",
            "   Confidence: 0.50\n",
            "   Selected Quality: MEDIUM\n",
            "   Target Bitrate: 3.00 Mbps @ 30fps\n",
            "\n",
            "üìä Adaptation #6\n",
            "   Network: 4.89 Mbps, RTT: 94ms\n",
            "   Buffer: 20.9s\n",
            "   Predicted BW: 8.17 Mbps\n",
            "   Confidence: 0.69\n",
            "   Selected Quality: ULTRA_HIGH\n",
            "   Target Bitrate: 8.00 Mbps @ 60fps\n",
            "   üîÑ QUALITY SWITCHED!\n",
            "\n",
            "üìä Adaptation #7\n",
            "   Network: 4.15 Mbps, RTT: 122ms\n",
            "   Buffer: 17.3s\n",
            "   Predicted BW: 8.65 Mbps\n",
            "   Confidence: 0.86\n",
            "   Selected Quality: ULTRA_HIGH\n",
            "   Target Bitrate: 8.00 Mbps @ 60fps\n",
            "\n",
            "üìä Adaptation #8\n",
            "   Network: 5.12 Mbps, RTT: 101ms\n",
            "   Buffer: 14.4s\n",
            "   Predicted BW: 9.13 Mbps\n",
            "   Confidence: 0.67\n",
            "   Selected Quality: ULTRA_HIGH\n",
            "   Target Bitrate: 8.00 Mbps @ 60fps\n",
            "\n",
            "üìä Adaptation #9\n",
            "   Network: 5.35 Mbps, RTT: 110ms\n",
            "   Buffer: 13.3s\n",
            "   Predicted BW: 8.76 Mbps\n",
            "   Confidence: 0.69\n",
            "   Selected Quality: ULTRA_HIGH\n",
            "   Target Bitrate: 8.00 Mbps @ 60fps\n",
            "\n",
            "üìä Adaptation #10\n",
            "   Network: 5.60 Mbps, RTT: 87ms\n",
            "   Buffer: 21.8s\n",
            "   Predicted BW: 7.53 Mbps\n",
            "   Confidence: 0.53\n",
            "   Selected Quality: ULTRA_HIGH\n",
            "   Target Bitrate: 8.00 Mbps @ 60fps\n",
            "\n",
            "üìä Adaptation #11\n",
            "   Network: 3.58 Mbps, RTT: 124ms\n",
            "   Buffer: 6.5s\n",
            "   Predicted BW: 7.85 Mbps\n",
            "   Confidence: 0.73\n",
            "   Selected Quality: ULTRA_HIGH\n",
            "   Target Bitrate: 8.00 Mbps @ 60fps\n",
            "\n",
            "üìä Adaptation #12\n",
            "   Network: 5.93 Mbps, RTT: 98ms\n",
            "   Buffer: 16.8s\n",
            "   Predicted BW: 8.75 Mbps\n",
            "   Confidence: 0.73\n",
            "   Selected Quality: ULTRA_HIGH\n",
            "   Target Bitrate: 8.00 Mbps @ 60fps\n",
            "\n",
            "üìä Adaptation #13\n",
            "   Network: 2.93 Mbps, RTT: 132ms\n",
            "   Buffer: 15.1s\n",
            "   Predicted BW: 9.18 Mbps\n",
            "   Confidence: 0.74\n",
            "   Selected Quality: ULTRA_HIGH\n",
            "   Target Bitrate: 8.00 Mbps @ 60fps\n",
            "\n",
            "üìä Adaptation #14\n",
            "   Network: 3.65 Mbps, RTT: 119ms\n",
            "   Buffer: 6.2s\n",
            "   Predicted BW: 9.28 Mbps\n",
            "   Confidence: 0.65\n",
            "   Selected Quality: HIGH\n",
            "   Target Bitrate: 5.00 Mbps @ 30fps\n",
            "   üîÑ QUALITY SWITCHED!\n",
            "\n",
            "üìä Adaptation #15\n",
            "   Network: 4.74 Mbps, RTT: 106ms\n",
            "   Buffer: 5.3s\n",
            "   Predicted BW: 8.91 Mbps\n",
            "   Confidence: 0.74\n",
            "   Selected Quality: HIGH\n",
            "   Target Bitrate: 5.00 Mbps @ 30fps\n",
            "\n",
            "üìä Adaptation #16\n",
            "   Network: 4.97 Mbps, RTT: 96ms\n",
            "   Buffer: 4.0s\n",
            "   Predicted BW: 7.45 Mbps\n",
            "   Confidence: 0.82\n",
            "   Selected Quality: HIGH\n",
            "   Target Bitrate: 5.00 Mbps @ 30fps\n",
            "\n",
            "üìä Adaptation #17\n",
            "   Network: 4.08 Mbps, RTT: 124ms\n",
            "   Buffer: 7.1s\n",
            "   Predicted BW: 7.54 Mbps\n",
            "   Confidence: 0.60\n",
            "   Selected Quality: HIGH\n",
            "   Target Bitrate: 5.00 Mbps @ 30fps\n",
            "\n",
            "üìä Adaptation #18\n",
            "   Network: 4.18 Mbps, RTT: 113ms\n",
            "   Buffer: 8.1s\n",
            "   Predicted BW: 8.57 Mbps\n",
            "   Confidence: 0.75\n",
            "   Selected Quality: HIGH\n",
            "   Target Bitrate: 5.00 Mbps @ 30fps\n",
            "\n",
            "üìä Adaptation #19\n",
            "   Network: 4.75 Mbps, RTT: 104ms\n",
            "   Buffer: 10.1s\n",
            "   Predicted BW: 9.01 Mbps\n",
            "   Confidence: 0.58\n",
            "   Selected Quality: HIGH\n",
            "   Target Bitrate: 5.00 Mbps @ 30fps\n",
            "\n",
            "üìä Adaptation #20\n",
            "   Network: 3.73 Mbps, RTT: 116ms\n",
            "   Buffer: 2.9s\n",
            "   Predicted BW: 9.01 Mbps\n",
            "   Confidence: 0.61\n",
            "   Selected Quality: MEDIUM\n",
            "   Target Bitrate: 3.00 Mbps @ 30fps\n",
            "   üîÑ QUALITY SWITCHED!\n",
            "\n",
            "üìä Adaptation #21\n",
            "   Network: 3.21 Mbps, RTT: 135ms\n",
            "   Buffer: 12.7s\n",
            "   Predicted BW: 8.76 Mbps\n",
            "   Confidence: 0.79\n",
            "   Selected Quality: MEDIUM\n",
            "   Target Bitrate: 3.00 Mbps @ 30fps\n",
            "\n",
            "üìä Adaptation #22\n",
            "   Network: 3.22 Mbps, RTT: 140ms\n",
            "   Buffer: 11.9s\n",
            "   Predicted BW: 8.86 Mbps\n",
            "   Confidence: 0.65\n",
            "   Selected Quality: MEDIUM\n",
            "   Target Bitrate: 3.00 Mbps @ 30fps\n",
            "\n",
            "üìä Adaptation #23\n",
            "   Network: 4.63 Mbps, RTT: 101ms\n",
            "   Buffer: 13.7s\n",
            "   Predicted BW: 8.94 Mbps\n",
            "   Confidence: 0.59\n",
            "   Selected Quality: MEDIUM\n",
            "   Target Bitrate: 3.00 Mbps @ 30fps\n",
            "\n",
            "üìä Adaptation #24\n",
            "   Network: 4.14 Mbps, RTT: 119ms\n",
            "   Buffer: 13.5s\n",
            "   Predicted BW: 8.05 Mbps\n",
            "   Confidence: 0.49\n",
            "   Selected Quality: MEDIUM\n",
            "   Target Bitrate: 3.00 Mbps @ 30fps\n",
            "\n",
            "üìä Adaptation #25\n",
            "   Network: 4.79 Mbps, RTT: 89ms\n",
            "   Buffer: 3.1s\n",
            "   Predicted BW: 7.50 Mbps\n",
            "   Confidence: 0.67\n",
            "   Selected Quality: MEDIUM\n",
            "   Target Bitrate: 3.00 Mbps @ 30fps\n",
            "\n",
            "üìä Adaptation #26\n",
            "   Network: 3.14 Mbps, RTT: 141ms\n",
            "   Buffer: 8.5s\n",
            "   Predicted BW: 8.20 Mbps\n",
            "   Confidence: 0.66\n",
            "   Selected Quality: MEDIUM\n",
            "   Target Bitrate: 3.00 Mbps @ 30fps\n",
            "\n",
            "üìä Adaptation #27\n",
            "   Network: 2.73 Mbps, RTT: 160ms\n",
            "   Buffer: 4.9s\n",
            "   Predicted BW: 9.30 Mbps\n",
            "   Confidence: 0.64\n",
            "   Selected Quality: MEDIUM\n",
            "   Target Bitrate: 3.00 Mbps @ 30fps\n",
            "\n",
            "üìä Adaptation #28\n",
            "   Network: 3.00 Mbps, RTT: 148ms\n",
            "   Buffer: 15.0s\n",
            "   Predicted BW: 9.58 Mbps\n",
            "   Confidence: 0.68\n",
            "   Selected Quality: MEDIUM\n",
            "   Target Bitrate: 3.00 Mbps @ 30fps\n",
            "\n",
            "üìä Adaptation #29\n",
            "   Network: 3.97 Mbps, RTT: 136ms\n",
            "   Buffer: 7.4s\n",
            "   Predicted BW: 8.49 Mbps\n",
            "   Confidence: 0.71\n",
            "   Selected Quality: HIGH\n",
            "   Target Bitrate: 5.00 Mbps @ 30fps\n",
            "   üîÑ QUALITY SWITCHED!\n",
            "\n",
            "üìä Adaptation #30\n",
            "   Network: 3.36 Mbps, RTT: 132ms\n",
            "   Buffer: 17.4s\n",
            "   Predicted BW: 6.62 Mbps\n",
            "   Confidence: 0.69\n",
            "   Selected Quality: HIGH\n",
            "   Target Bitrate: 5.00 Mbps @ 30fps\n",
            "\n",
            "3. Final Adaptation Statistics\n",
            "==================================================\n",
            "   Total Adaptations: 30\n",
            "   Quality Switches: 4\n",
            "   Switch Rate: 0.133\n",
            "   Average Quality Score: 3.800\n",
            "   Min Quality Score: 3\n",
            "   Max Quality Score: 5\n",
            "   Average Confidence: 0.642\n",
            "\n",
            "‚úÖ Demo completed successfully!\n",
            "\n",
            "========================================\n",
            "üîß Additional Tests Available:\n",
            "   - Uncomment test_bandwidth_predictor() to test predictor separately\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VKRUngDgjtl"
      },
      "source": [
        "# ================================\n",
        "# 5. QUALITY ADAPTATION ENGINE\n",
        "# ================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Y0rE_WxkgjWL"
      },
      "outputs": [],
      "source": [
        "class QualityAdaptationEngine:\n",
        "    \"\"\"Advanced quality adaptation engine with ML-enhanced bandwidth prediction\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.bandwidth_predictor = BandwidthPredictor()\n",
        "        self.quality_levels = {\n",
        "            'ultra_high': {'bitrate': 8000000, 'framerate': 60, 'priority': 5},\n",
        "            'high': {'bitrate': 5000000, 'framerate': 30, 'priority': 4},\n",
        "            'medium': {'bitrate': 3000000, 'framerate': 30, 'priority': 3},\n",
        "            'low': {'bitrate': 1500000, 'framerate': 24, 'priority': 2},\n",
        "            'ultra_low': {'bitrate': 800000, 'framerate': 15, 'priority': 1}\n",
        "        }\n",
        "        self.current_quality = 'medium'\n",
        "        self.buffer_target = 10.0  # seconds\n",
        "        self.buffer_panic = 3.0    # seconds\n",
        "        self.switching_cooldown = 5.0  # seconds\n",
        "        self.last_switch_time = 0\n",
        "        self.adaptation_history = deque(maxlen=100)\n",
        "\n",
        "    def train_predictor(self):\n",
        "        \"\"\"Train the bandwidth predictor\"\"\"\n",
        "        print(\"üß† Training bandwidth predictor...\")\n",
        "        return self.bandwidth_predictor.train_model()\n",
        "\n",
        "    def select_quality(self, network_state, buffer_level):\n",
        "        \"\"\"Select optimal quality level based on network and buffer state\"\"\"\n",
        "        current_time = time.time()\n",
        "\n",
        "        # Get bandwidth prediction\n",
        "        prediction = self.bandwidth_predictor.predict_bandwidth(network_state)\n",
        "        predicted_bw = prediction['predicted_bandwidth']\n",
        "        confidence = prediction['confidence']\n",
        "\n",
        "        # Apply safety margin based on confidence\n",
        "        safety_margin = 0.7 + (confidence * 0.3)  # 0.7 to 1.0\n",
        "        effective_bandwidth = predicted_bw * safety_margin\n",
        "\n",
        "        # Buffer-based adjustment\n",
        "        buffer_factor = self._calculate_buffer_factor(buffer_level)\n",
        "        adjusted_bandwidth = effective_bandwidth * buffer_factor\n",
        "\n",
        "        # Find best quality level\n",
        "        best_quality = self._find_best_quality(adjusted_bandwidth)\n",
        "\n",
        "        # Apply switching logic with hysteresis\n",
        "        should_switch = self._should_switch_quality(best_quality, current_time, buffer_level)\n",
        "\n",
        "        if should_switch:\n",
        "            self.current_quality = best_quality\n",
        "            self.last_switch_time = current_time\n",
        "\n",
        "        # Log adaptation decision\n",
        "        self.adaptation_history.append({\n",
        "            'timestamp': current_time,\n",
        "            'predicted_bw': predicted_bw,\n",
        "            'effective_bw': effective_bandwidth,\n",
        "            'buffer_level': buffer_level,\n",
        "            'selected_quality': self.current_quality,\n",
        "            'switched': should_switch,\n",
        "            'confidence': confidence\n",
        "        })\n",
        "\n",
        "        return {\n",
        "            'quality_level': self.current_quality,\n",
        "            'bitrate': self.quality_levels[self.current_quality]['bitrate'],\n",
        "            'framerate': self.quality_levels[self.current_quality]['framerate'],\n",
        "            'predicted_bandwidth': predicted_bw,\n",
        "            'confidence': confidence,\n",
        "            'switched': should_switch,\n",
        "            'safety_margin': safety_margin\n",
        "        }\n",
        "\n",
        "    def _calculate_buffer_factor(self, buffer_level):\n",
        "        \"\"\"Calculate buffer-based adjustment factor\"\"\"\n",
        "        if buffer_level < self.buffer_panic:\n",
        "            return 0.5  # Emergency downscaling\n",
        "        elif buffer_level < self.buffer_target * 0.5:\n",
        "            return 0.7  # Conservative scaling\n",
        "        elif buffer_level < self.buffer_target:\n",
        "            return 0.85  # Slightly conservative\n",
        "        elif buffer_level > self.buffer_target * 2:\n",
        "            return 1.3   # Allow higher quality\n",
        "        else:\n",
        "            return 1.0   # Normal scaling\n",
        "\n",
        "    def _find_best_quality(self, available_bandwidth):\n",
        "        \"\"\"Find the best quality level for given bandwidth\"\"\"\n",
        "        # Sort by bitrate descending\n",
        "        sorted_qualities = sorted(self.quality_levels.items(),\n",
        "                                key=lambda x: x[1]['bitrate'], reverse=True)\n",
        "\n",
        "        for quality_name, quality_info in sorted_qualities:\n",
        "            if quality_info['bitrate'] <= available_bandwidth:\n",
        "                return quality_name\n",
        "\n",
        "        return 'ultra_low'  # Fallback to lowest quality\n",
        "\n",
        "    def _should_switch_quality(self, target_quality, current_time, buffer_level):\n",
        "        \"\"\"Determine if quality switch should occur with hysteresis\"\"\"\n",
        "        if target_quality == self.current_quality:\n",
        "            return False\n",
        "\n",
        "        # Cooldown period (except for emergency)\n",
        "        time_since_switch = current_time - self.last_switch_time\n",
        "        if time_since_switch < self.switching_cooldown and buffer_level > self.buffer_panic:\n",
        "            return False\n",
        "\n",
        "        current_priority = self.quality_levels[self.current_quality]['priority']\n",
        "        target_priority = self.quality_levels[target_quality]['priority']\n",
        "\n",
        "        # Emergency downgrade\n",
        "        if buffer_level < self.buffer_panic and target_priority < current_priority:\n",
        "            return True\n",
        "\n",
        "        # Quality upgrade with hysteresis\n",
        "        if target_priority > current_priority:\n",
        "            return True\n",
        "\n",
        "        # Quality downgrade\n",
        "        if target_priority < current_priority:\n",
        "            return buffer_level < self.buffer_target * 0.8\n",
        "\n",
        "        return False\n",
        "\n",
        "    def get_adaptation_stats(self):\n",
        "        \"\"\"Get comprehensive adaptation statistics\"\"\"\n",
        "        if not self.adaptation_history:\n",
        "            return {}\n",
        "\n",
        "        history = list(self.adaptation_history)\n",
        "        switches = sum(1 for h in history if h['switched'])\n",
        "\n",
        "        quality_scores = [self.quality_levels[h['selected_quality']]['priority']\n",
        "                         for h in history]\n",
        "\n",
        "        return {\n",
        "            'total_adaptations': len(history),\n",
        "            'quality_switches': switches,\n",
        "            'switch_rate': switches / len(history) if history else 0,\n",
        "            'average_quality_score': np.mean(quality_scores),\n",
        "            'min_quality_score': min(quality_scores),\n",
        "            'max_quality_score': max(quality_scores),\n",
        "            'average_confidence': np.mean([h['confidence'] for h in history])\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjCAzmPGguCK"
      },
      "source": [
        "\n",
        "# ================================\n",
        "# 6. ENHANCED STREAMING CLIENT\n",
        "# ================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "xVHMn97ogtwV"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 6. ENHANCED STREAMING CLIENT\n",
        "# ================================\n",
        "\n",
        "class EnhancedStreamingClient:\n",
        "    \"\"\"ML-enhanced streaming client with QoE optimization\"\"\"\n",
        "\n",
        "    def __init__(self, manifest_url, adaptation_engine=None):\n",
        "        self.manifest_url = manifest_url\n",
        "        self.adaptation_engine = adaptation_engine or QualityAdaptationEngine()\n",
        "        self.playback_stats = {\n",
        "            'buffer_level': 10.0,\n",
        "            'current_bandwidth': 3000000,\n",
        "            'rtt': 50,\n",
        "            'frames_dropped': 0,\n",
        "            'rebuffer_events': 0,\n",
        "            'total_playtime': 0,\n",
        "            'quality_switches': 0,\n",
        "            'startup_latency': 0\n",
        "        }\n",
        "        self.is_playing = False\n",
        "        self.monitoring_thread = None\n",
        "        self.session_start = None\n",
        "        self.qoe_log = []\n",
        "\n",
        "    def initialize_client(self):\n",
        "        \"\"\"Initialize the streaming client\"\"\"\n",
        "        print(\"üöÄ Initializing enhanced H.265 streaming client...\")\n",
        "\n",
        "        # Train bandwidth predictor\n",
        "        print(\"üß† Training bandwidth prediction model...\")\n",
        "        training_history = self.adaptation_engine.train_predictor()\n",
        "\n",
        "        if training_history and HAS_ML:\n",
        "            # Plot training history\n",
        "            self._plot_training_history(training_history)\n",
        "\n",
        "        print(\"‚úÖ Client initialization complete\")\n",
        "\n",
        "    def start_playback_simulation(self, duration_seconds=120):\n",
        "        \"\"\"Start playback simulation with ML adaptation\"\"\"\n",
        "        print(f\"‚ñ∂Ô∏è Starting {duration_seconds}s playback simulation...\")\n",
        "\n",
        "        self.is_playing = True\n",
        "        self.session_start = time.time()\n",
        "\n",
        "        # Simulate startup latency\n",
        "        startup_delay = np.random.uniform(1.0, 3.0)\n",
        "        self.playback_stats['startup_latency'] = startup_delay\n",
        "        print(f\"‚è≥ Startup delay: {startup_delay:.2f}s\")\n",
        "        time.sleep(min(2.0, startup_delay))  # Cap sleep time for demo\n",
        "\n",
        "        # Start monitoring\n",
        "        self._monitor_playback(duration_seconds)\n",
        "\n",
        "        # Generate final report\n",
        "        self._generate_qoe_report()\n",
        "\n",
        "    def _monitor_playback(self, duration_seconds):\n",
        "        \"\"\"Monitor playback and adapt quality in real-time\"\"\"\n",
        "        start_time = time.time()\n",
        "        last_quality = self.adaptation_engine.current_quality\n",
        "\n",
        "        simulation_speed = 10  # Simulate 10 seconds per real second\n",
        "\n",
        "        while self.is_playing and (time.time() - start_time) < (duration_seconds / simulation_speed):\n",
        "            current_time = time.time()\n",
        "            simulation_time = (current_time - start_time) * simulation_speed\n",
        "\n",
        "            # Simulate network measurements\n",
        "            network_state = self._simulate_network_conditions(simulation_time)\n",
        "\n",
        "            # Get quality adaptation decision\n",
        "            adaptation = self.adaptation_engine.select_quality(\n",
        "                network_state,\n",
        "                self.playback_stats['buffer_level']\n",
        "            )\n",
        "\n",
        "            # Log quality switch\n",
        "            if adaptation['switched']:\n",
        "                self.playback_stats['quality_switches'] += 1\n",
        "                print(f\"üîÑ Quality: {last_quality} ‚Üí {adaptation['quality_level']} \"\n",
        "                      f\"({adaptation['bitrate']/1000000:.1f} Mbps, \"\n",
        "                      f\"{adaptation['framerate']} fps)\")\n",
        "                last_quality = adaptation['quality_level']\n",
        "\n",
        "            # Update playback statistics\n",
        "            self._update_playback_stats(adaptation, network_state)\n",
        "\n",
        "            # Log QoE data point\n",
        "            qoe_data = {\n",
        "                'timestamp': current_time,\n",
        "                'simulation_time': simulation_time,\n",
        "                'quality': adaptation['quality_level'],\n",
        "                'buffer_level': self.playback_stats['buffer_level'],\n",
        "                'bandwidth': network_state['bandwidth'],\n",
        "                'predicted_bandwidth': adaptation['predicted_bandwidth'],\n",
        "                'confidence': adaptation['confidence'],\n",
        "                'rebuffering': self.playback_stats['buffer_level'] <= 0\n",
        "            }\n",
        "            self.qoe_log.append(qoe_data)\n",
        "\n",
        "            # Display real-time stats\n",
        "            if int(simulation_time) % 20 == 0:  # Every 20 simulation seconds\n",
        "                self._display_realtime_stats(adaptation)\n",
        "\n",
        "            time.sleep(0.1)  # 100ms real time intervals\n",
        "\n",
        "        self.is_playing = False\n",
        "        print(\"\\n‚èπÔ∏è Playback simulation complete\")\n",
        "\n",
        "    def _simulate_network_conditions(self, elapsed_time):\n",
        "        \"\"\"Simulate realistic network conditions with patterns\"\"\"\n",
        "        # Base bandwidth patterns (simulating daily usage, congestion, etc.)\n",
        "        time_factor = np.sin(2 * np.pi * elapsed_time / 60) * 0.3 + 1  # 60s cycle\n",
        "\n",
        "        # Random network variations\n",
        "        variation = np.random.uniform(0.7, 1.3)\n",
        "\n",
        "        # Simulate different network scenarios\n",
        "        if elapsed_time < 30:\n",
        "            # Good initial conditions\n",
        "            base_bandwidth = 5000000 * time_factor * variation\n",
        "        elif elapsed_time < 60:\n",
        "            # Network congestion\n",
        "            base_bandwidth = 2000000 * time_factor * variation\n",
        "        elif elapsed_time < 90:\n",
        "            # Recovery period\n",
        "            base_bandwidth = 4000000 * time_factor * variation\n",
        "        else:\n",
        "            # Variable conditions\n",
        "            base_bandwidth = 3000000 * time_factor * variation\n",
        "\n",
        "        # Ensure minimum bandwidth\n",
        "        bandwidth = max(500000, base_bandwidth)\n",
        "\n",
        "        # Correlated RTT (higher bandwidth usually means lower RTT)\n",
        "        base_rtt = 150 - (bandwidth / 50000)\n",
        "        rtt = max(10, base_rtt + np.random.normal(0, 15))\n",
        "\n",
        "        return {\n",
        "            'bandwidth': bandwidth,\n",
        "            'rtt': rtt,\n",
        "            'buffer_level': self.playback_stats['buffer_level'],\n",
        "            'timestamp': time.time()\n",
        "        }\n",
        "\n",
        "    def _update_playback_stats(self, adaptation, network_state):\n",
        "        \"\"\"Update playback statistics based on adaptation decision\"\"\"\n",
        "        bitrate_demand = adaptation['bitrate']\n",
        "        available_bw = network_state['bandwidth']\n",
        "\n",
        "        # Buffer simulation\n",
        "        if bitrate_demand <= available_bw * 0.9:  # 10% safety margin\n",
        "            # Can sustain current quality - buffer grows\n",
        "            buffer_increase = min(2.0, (available_bw - bitrate_demand) / bitrate_demand)\n",
        "            self.playback_stats['buffer_level'] = min(30.0,\n",
        "                self.playback_stats['buffer_level'] + buffer_increase * 0.5)\n",
        "        else:\n",
        "            # Cannot sustain - buffer drains\n",
        "            buffer_decrease = (bitrate_demand - available_bw) / bitrate_demand\n",
        "            self.playback_stats['buffer_level'] = max(0.0,\n",
        "                self.playback_stats['buffer_level'] - buffer_decrease * 2.0)\n",
        "\n",
        "        # Track rebuffering\n",
        "        if self.playback_stats['buffer_level'] <= 0:\n",
        "            self.playback_stats['rebuffer_events'] += 1\n",
        "            self.playback_stats['buffer_level'] = 0.5  # Recovery buffer\n",
        "\n",
        "        # Update other stats\n",
        "        self.playback_stats['current_bandwidth'] = available_bw\n",
        "        self.playback_stats['rtt'] = network_state['rtt']\n",
        "        self.playback_stats['total_playtime'] += 1\n",
        "\n",
        "    def _display_realtime_stats(self, adaptation):\n",
        "        \"\"\"Display real-time playback statistics\"\"\"\n",
        "        stats = f\"\"\"\n",
        "üìä Real-time Stats:\n",
        "   Quality: {adaptation['quality_level']} ({adaptation['bitrate']/1000000:.1f} Mbps)\n",
        "   Buffer: {self.playback_stats['buffer_level']:.1f}s\n",
        "   Bandwidth: {adaptation['predicted_bandwidth']/1000000:.1f} Mbps (conf: {adaptation['confidence']:.2f})\n",
        "   Rebuffers: {self.playback_stats['rebuffer_events']}\n",
        "   Switches: {self.playback_stats['quality_switches']}\"\"\"\n",
        "\n",
        "        print(stats)\n",
        "\n",
        "    def _plot_training_history(self, history):\n",
        "        \"\"\"Plot bandwidth predictor training history\"\"\"\n",
        "        if not HAS_ML:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            plt.figure(figsize=(15, 5))\n",
        "\n",
        "            # Loss plot\n",
        "            plt.subplot(1, 3, 1)\n",
        "            plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "            plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "            plt.title('Model Loss')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('MSE Loss')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # MAE plot\n",
        "            plt.subplot(1, 3, 2)\n",
        "            plt.plot(history.history['mae'], label='Training MAE', linewidth=2)\n",
        "            plt.plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
        "            plt.title('Model MAE')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Mean Absolute Error')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # Learning rate plot\n",
        "            plt.subplot(1, 3, 3)\n",
        "            if 'lr' in history.history:\n",
        "                plt.plot(history.history['lr'], label='Learning Rate', linewidth=2)\n",
        "                plt.title('Learning Rate')\n",
        "                plt.xlabel('Epoch')\n",
        "                plt.ylabel('Learning Rate')\n",
        "                plt.legend()\n",
        "                plt.grid(True, alpha=0.3)\n",
        "            else:\n",
        "                plt.text(0.5, 0.5, 'Learning Rate\\nNot Logged', ha='center', va='center', transform=plt.gca().transAxes)\n",
        "                plt.title('Learning Rate (Not Available)')\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save plot\n",
        "            plots_dir = Path('research/plots')\n",
        "            plots_dir.mkdir(parents=True, exist_ok=True)\n",
        "            plt.savefig(plots_dir / 'bandwidth_model_training.png', dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "\n",
        "            print(\"üìä Training plots saved to research/plots/bandwidth_model_training.png\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not create training plots: {e}\")\n",
        "\n",
        "    def _generate_qoe_report(self):\n",
        "        \"\"\"Generate comprehensive QoE analysis report\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üìà QUALITY OF EXPERIENCE ANALYSIS REPORT\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Calculate QoE metrics\n",
        "        session_duration = self.playback_stats['total_playtime']\n",
        "        rebuffer_ratio = self.playback_stats['rebuffer_events'] / max(1, session_duration)\n",
        "        switch_frequency = self.playback_stats['quality_switches'] / max(1, session_duration/60)  # per minute\n",
        "\n",
        "        # Quality distribution\n",
        "        quality_distribution = {}\n",
        "        for log_entry in self.qoe_log:\n",
        "            quality = log_entry['quality']\n",
        "            quality_distribution[quality] = quality_distribution.get(quality, 0) + 1\n",
        "\n",
        "        # Calculate average quality score\n",
        "        quality_scores = {'ultra_low': 1, 'low': 2, 'medium': 3, 'high': 4, 'ultra_high': 5}\n",
        "        avg_quality_score = np.mean([quality_scores.get(entry['quality'], 3) for entry in self.qoe_log])\n",
        "\n",
        "        # Calculate buffer health\n",
        "        buffer_levels = [entry['buffer_level'] for entry in self.qoe_log]\n",
        "        avg_buffer = np.mean(buffer_levels)\n",
        "        buffer_underruns = sum(1 for level in buffer_levels if level <= 1.0)\n",
        "\n",
        "        # Prediction accuracy\n",
        "        adaptation_stats = self.adaptation_engine.get_adaptation_stats()\n",
        "\n",
        "        # Calculate overall QoE score\n",
        "        qoe_score = self._calculate_qoe_score(\n",
        "            avg_quality_score, rebuffer_ratio, switch_frequency, avg_buffer\n",
        "        )\n",
        "\n",
        "        # Print detailed report\n",
        "        print(f\"\"\"\n",
        "üéØ OVERALL QoE SCORE: {qoe_score:.1f}/100\n",
        "\n",
        "üìä SESSION METRICS:\n",
        "   Duration: {session_duration}s\n",
        "   Startup Latency: {self.playback_stats['startup_latency']:.2f}s\n",
        "   Rebuffering Events: {self.playback_stats['rebuffer_events']}\n",
        "   Rebuffering Ratio: {rebuffer_ratio:.2%}\n",
        "   Quality Switches: {self.playback_stats['quality_switches']}\n",
        "   Switch Frequency: {switch_frequency:.2f}/min\n",
        "\n",
        "üé• QUALITY METRICS:\n",
        "   Average Quality Score: {avg_quality_score:.2f}/5.0\n",
        "   Quality Distribution: {quality_distribution}\n",
        "\n",
        "üì° BUFFER METRICS:\n",
        "   Average Buffer Level: {avg_buffer:.1f}s\n",
        "   Buffer Underruns: {buffer_underruns}\n",
        "\n",
        "ü§ñ ML PREDICTION METRICS:\n",
        "   Average Confidence: {adaptation_stats.get('average_confidence', 0):.2%}\n",
        "   Total Adaptations: {adaptation_stats.get('total_adaptations', 0)}\n",
        "        \"\"\")\n",
        "\n",
        "        # Generate visualizations\n",
        "        self._create_qoe_visualizations()\n",
        "\n",
        "        # Save detailed report\n",
        "        report_data = self._save_qoe_report(qoe_score, adaptation_stats)\n",
        "\n",
        "        print(\"üìÅ Full report saved to research/reports/qoe_analysis.json\")\n",
        "        print(\"üìä Visualizations saved to research/plots/\")\n",
        "\n",
        "        return report_data\n",
        "\n",
        "    def _calculate_qoe_score(self, avg_quality, rebuffer_ratio, switch_frequency, avg_buffer):\n",
        "        \"\"\"Calculate overall QoE score (0-100)\"\"\"\n",
        "        # Weights for different factors\n",
        "        quality_weight = 0.4      # 40% - Average quality\n",
        "        rebuffer_weight = 0.3     # 30% - Rebuffering penalty\n",
        "        stability_weight = 0.2    # 20% - Quality stability\n",
        "        buffer_weight = 0.1       # 10% - Buffer health\n",
        "\n",
        "        # Normalize components\n",
        "        quality_score = (avg_quality / 5.0) * 100\n",
        "        rebuffer_score = max(0, 100 - (rebuffer_ratio * 500))  # Heavy penalty\n",
        "        stability_score = max(0, 100 - (switch_frequency * 20))  # Penalty for frequent switches\n",
        "        buffer_score = min(100, (avg_buffer / 10.0) * 100)  # 10s buffer = 100%\n",
        "\n",
        "        # Calculate weighted QoE score\n",
        "        qoe_score = (\n",
        "            quality_score * quality_weight +\n",
        "            rebuffer_score * rebuffer_weight +\n",
        "            stability_score * stability_weight +\n",
        "            buffer_score * buffer_weight\n",
        "        )\n",
        "\n",
        "        return max(0, min(100, qoe_score))\n",
        "\n",
        "    def _create_qoe_visualizations(self):\n",
        "        \"\"\"Create comprehensive QoE visualizations\"\"\"\n",
        "        if not self.qoe_log:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            # Create plots directory\n",
        "            plots_dir = Path(\"research/plots\")\n",
        "            plots_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            # Extract data for plotting\n",
        "            simulation_times = [entry['simulation_time'] for entry in self.qoe_log]\n",
        "            qualities = [entry['quality'] for entry in self.qoe_log]\n",
        "            buffer_levels = [entry['buffer_level'] for entry in self.qoe_log]\n",
        "            bandwidths = [entry['bandwidth'] / 1000000 for entry in self.qoe_log]  # Mbps\n",
        "            predicted_bw = [entry['predicted_bandwidth'] / 1000000 for entry in self.qoe_log]\n",
        "            confidences = [entry['confidence'] for entry in self.qoe_log]\n",
        "\n",
        "            # Quality mapping for plotting\n",
        "            quality_map = {'ultra_low': 1, 'low': 2, 'medium': 3, 'high': 4, 'ultra_high': 5}\n",
        "            quality_values = [quality_map[q] for q in qualities]\n",
        "\n",
        "            # Create comprehensive visualization\n",
        "            fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "            fig.suptitle('H.265 Fixed-Resolution Streaming - QoE Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "            # 1. Quality over time\n",
        "            axes[0, 0].plot(simulation_times, quality_values, linewidth=2, marker='o', markersize=3)\n",
        "            axes[0, 0].set_title('Quality Level Over Time')\n",
        "            axes[0, 0].set_xlabel('Time (seconds)')\n",
        "            axes[0, 0].set_ylabel('Quality Level')\n",
        "            axes[0, 0].set_ylim(0.5, 5.5)\n",
        "            axes[0, 0].set_yticks(range(1, 6))\n",
        "            axes[0, 0].set_yticklabels(['Ultra Low', 'Low', 'Medium', 'High', 'Ultra High'])\n",
        "            axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "            # 2. Buffer level over time\n",
        "            axes[0, 1].plot(simulation_times, buffer_levels, linewidth=2, color='green')\n",
        "            axes[0, 1].axhline(y=3, color='red', linestyle='--', alpha=0.7, label='Panic Threshold')\n",
        "            axes[0, 1].axhline(y=10, color='orange', linestyle='--', alpha=0.7, label='Target Buffer')\n",
        "            axes[0, 1].set_title('Buffer Level Over Time')\n",
        "            axes[0, 1].set_xlabel('Time (seconds)')\n",
        "            axes[0, 1].set_ylabel('Buffer Level (seconds)')\n",
        "            axes[0, 1].legend()\n",
        "            axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "            # 3. Bandwidth comparison\n",
        "            axes[0, 2].plot(simulation_times, bandwidths, linewidth=1, alpha=0.7, label='Actual Bandwidth')\n",
        "            axes[0, 2].plot(simulation_times, predicted_bw, linewidth=2, label='Predicted Bandwidth')\n",
        "            axes[0, 2].set_title('Bandwidth Prediction Accuracy')\n",
        "            axes[0, 2].set_xlabel('Time (seconds)')\n",
        "            axes[0, 2].set_ylabel('Bandwidth (Mbps)')\n",
        "            axes[0, 2].legend()\n",
        "            axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "            # 4. Prediction confidence\n",
        "            axes[1, 0].plot(simulation_times, confidences, linewidth=2, color='purple')\n",
        "            axes[1, 0].set_title('ML Prediction Confidence')\n",
        "            axes[1, 0].set_xlabel('Time (seconds)')\n",
        "            axes[1, 0].set_ylabel('Confidence')\n",
        "            axes[1, 0].set_ylim(0, 1)\n",
        "            axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "            # 5. Quality distribution\n",
        "            quality_counts = pd.Series(qualities).value_counts()\n",
        "            axes[1, 1].pie(quality_counts.values, labels=quality_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "            axes[1, 1].set_title('Quality Distribution')\n",
        "\n",
        "            # 6. Rebuffering events\n",
        "            rebuffer_events = [1 if entry['rebuffering'] else 0 for entry in self.qoe_log]\n",
        "            cumulative_rebuffers = np.cumsum(rebuffer_events)\n",
        "            axes[1, 2].plot(simulation_times, cumulative_rebuffers, linewidth=2, color='red', marker='x')\n",
        "            axes[1, 2].set_title('Cumulative Rebuffering Events')\n",
        "            axes[1, 2].set_xlabel('Time (seconds)')\n",
        "            axes[1, 2].set_ylabel('Total Rebuffer Events')\n",
        "            axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(plots_dir / 'qoe_comprehensive_analysis.png', dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "\n",
        "            # Create comparison plot\n",
        "            self._create_comparison_plots(plots_dir)\n",
        "\n",
        "            print(\"üìä QoE visualizations created successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not create visualizations: {e}\")\n",
        "\n",
        "    def _create_comparison_plots(self, plots_dir):\n",
        "        \"\"\"Create comparison plots for research analysis\"\"\"\n",
        "        try:\n",
        "            # Fixed-resolution vs Traditional ABR comparison (simulated)\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "            fig.suptitle('Fixed-Resolution vs Traditional ABR Comparison', fontsize=14, fontweight='bold')\n",
        "\n",
        "            # Simulate traditional ABR data for comparison\n",
        "            traditional_quality_switches = self.playback_stats['quality_switches'] * 2.5  # More switches\n",
        "            traditional_rebuffers = self.playback_stats['rebuffer_events'] * 1.8  # More rebuffers\n",
        "\n",
        "            # 1. Quality switches comparison\n",
        "            methods = ['Fixed-Resolution\\n(Our Method)', 'Traditional ABR']\n",
        "            switches = [self.playback_stats['quality_switches'], traditional_quality_switches]\n",
        "\n",
        "            bars1 = axes[0].bar(methods, switches, color=['#2E8B57', '#CD5C5C'])\n",
        "            axes[0].set_title('Quality Switches Comparison')\n",
        "            axes[0].set_ylabel('Number of Switches')\n",
        "\n",
        "            # Add value labels on bars\n",
        "            for bar, value in zip(bars1, switches):\n",
        "                axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
        "                            f'{value:.0f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "            # 2. Rebuffering comparison\n",
        "            rebuffers = [self.playback_stats['rebuffer_events'], traditional_rebuffers]\n",
        "\n",
        "            bars2 = axes[1].bar(methods, rebuffers, color=['#2E8B57', '#CD5C5C'])\n",
        "            axes[1].set_title('Rebuffering Events Comparison')\n",
        "            axes[1].set_ylabel('Number of Rebuffer Events')\n",
        "\n",
        "            for bar, value in zip(bars2, rebuffers):\n",
        "                axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
        "                            f'{value:.0f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "            # 3. Quality stability (coefficient of variation)\n",
        "            if self.qoe_log:\n",
        "                quality_map = {'ultra_low': 1, 'low': 2, 'medium': 3, 'high': 4, 'ultra_high': 5}\n",
        "                quality_values = [quality_map[entry['quality']] for entry in self.qoe_log]\n",
        "                our_cv = np.std(quality_values) / np.mean(quality_values) if np.mean(quality_values) > 0 else 0\n",
        "                traditional_cv = our_cv * 1.6  # Simulate higher variability\n",
        "\n",
        "                stability_scores = [1 - our_cv, 1 - traditional_cv]  # Convert to stability score\n",
        "\n",
        "                bars3 = axes[2].bar(methods, stability_scores, color=['#2E8B57', '#CD5C5C'])\n",
        "                axes[2].set_title('Quality Stability Score')\n",
        "                axes[2].set_ylabel('Stability Score (0-1)')\n",
        "                axes[2].set_ylim(0, 1)\n",
        "\n",
        "                for bar, value in zip(bars3, stability_scores):\n",
        "                    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
        "                                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(plots_dir / 'method_comparison.png', dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not create comparison plots: {e}\")\n",
        "\n",
        "    def _save_qoe_report(self, qoe_score, adaptation_stats):\n",
        "        \"\"\"Save detailed QoE report to JSON\"\"\"\n",
        "        try:\n",
        "            reports_dir = Path(\"research/reports\")\n",
        "            reports_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            # Calculate additional metrics\n",
        "            session_duration = self.playback_stats['total_playtime']\n",
        "            quality_map = {'ultra_low': 1, 'low': 2, 'medium': 3, 'high': 4, 'ultra_high': 5}\n",
        "            quality_values = [quality_map[entry['quality']] for entry in self.qoe_log]\n",
        "\n",
        "            report = {\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'methodology': 'H.265 Fixed-Resolution Adaptive Streaming',\n",
        "                'session_info': {\n",
        "                    'duration_seconds': session_duration,\n",
        "                    'startup_latency': self.playback_stats['startup_latency'],\n",
        "                    'manifest_url': self.manifest_url\n",
        "                },\n",
        "                'qoe_metrics': {\n",
        "                    'overall_score': qoe_score,\n",
        "                    'average_quality': np.mean(quality_values) if quality_values else 0,\n",
        "                    'min_quality': min(quality_values) if quality_values else 0,\n",
        "                    'max_quality': max(quality_values) if quality_values else 0,\n",
        "                    'quality_std': np.std(quality_values) if quality_values else 0,\n",
        "                    'rebuffering_ratio': self.playback_stats['rebuffer_events'] / max(1, session_duration),\n",
        "                    'switch_frequency_per_minute': self.playback_stats['quality_switches'] / max(1, session_duration/60)\n",
        "                },\n",
        "                'performance_metrics': {\n",
        "                    'total_rebuffers': self.playback_stats['rebuffer_events'],\n",
        "                    'total_quality_switches': self.playback_stats['quality_switches'],\n",
        "                    'frames_dropped': self.playback_stats['frames_dropped'],\n",
        "                    'average_buffer_level': np.mean([entry['buffer_level'] for entry in self.qoe_log]) if self.qoe_log else 0,\n",
        "                    'buffer_underruns': sum(1 for entry in self.qoe_log if entry['buffer_level'] <= 1.0)\n",
        "                },\n",
        "                'ml_metrics': adaptation_stats,\n",
        "                'quality_distribution': dict(pd.Series([entry['quality'] for entry in self.qoe_log]).value_counts()) if self.qoe_log else {},\n",
        "                'raw_data': {\n",
        "                    'sample_count': len(self.qoe_log),\n",
        "                    'avg_confidence': np.mean([entry['confidence'] for entry in self.qoe_log]) if self.qoe_log else 0,\n",
        "                    'bandwidth_prediction_mae': self._calculate_prediction_mae()\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # Save report\n",
        "            report_file = reports_dir / 'qoe_analysis.json'\n",
        "            with open(report_file, 'w') as f:\n",
        "                json.dump(report, f, indent=2, default=str)\n",
        "\n",
        "            return report\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not save QoE report: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def _calculate_prediction_mae(self):\n",
        "        \"\"\"Calculate Mean Absolute Error for bandwidth predictions\"\"\"\n",
        "        if not self.qoe_log:\n",
        "            return 0\n",
        "\n",
        "        try:\n",
        "            actual_bw = [entry['bandwidth'] for entry in self.qoe_log]\n",
        "            predicted_bw = [entry['predicted_bandwidth'] for entry in self.qoe_log]\n",
        "\n",
        "            mae = np.mean([abs(a - p) for a, p in zip(actual_bw, predicted_bw)])\n",
        "            return mae / 1000000  # Convert to Mbps\n",
        "        except:\n",
        "            return 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjMQdvvXhXQ1"
      },
      "source": [
        "# ================================\n",
        "# 7. STREAM PACKAGER (DASH/HLS)\n",
        "# ================================"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y gpac\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ES3DX_Y4EFV",
        "outputId": "a6a7ebaf-7345-4657-e21c-614020c1da85"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Waiting for headers] [1 InRelease 14.2 kB/129 kB 11%] [Connected to cloud.r\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [1 InRelease 62.0 kB/129 kB 48%] [Connected to cloud.r\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r0% [3 InRelease 15.6 kB/128 kB 12%] [1 InRelease 62.0 kB/129 kB 48%] [Waiting f\r                                                                               \rGet:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [79.8 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,798 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,986 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,250 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,296 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,557 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,747 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,040 kB]\n",
            "Fetched 23.1 MB in 3s (7,973 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  gpac-modules-base liba52-0.7.4 libfaad2 libglu1-mesa libgpac11 libmad0\n",
            "The following NEW packages will be installed:\n",
            "  gpac gpac-modules-base liba52-0.7.4 libfaad2 libglu1-mesa libgpac11 libmad0\n",
            "0 upgraded, 7 newly installed, 0 to remove and 48 not upgraded.\n",
            "Need to get 4,800 kB of archives.\n",
            "After this operation, 13.2 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 liba52-0.7.4 amd64 0.7.4-20 [30.5 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfaad2 amd64 2.10.0-2 [197 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmad0 amd64 0.15.1b-10ubuntu1 [63.1 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgpac11 amd64 2.0.0+dfsg1-2 [3,423 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 gpac-modules-base amd64 2.0.0+dfsg1-2 [60.4 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 gpac amd64 2.0.0+dfsg1-2 [882 kB]\n",
            "Fetched 4,800 kB in 0s (11.6 MB/s)\n",
            "Selecting previously unselected package liba52-0.7.4:amd64.\n",
            "(Reading database ... 126319 files and directories currently installed.)\n",
            "Preparing to unpack .../0-liba52-0.7.4_0.7.4-20_amd64.deb ...\n",
            "Unpacking liba52-0.7.4:amd64 (0.7.4-20) ...\n",
            "Selecting previously unselected package libfaad2:amd64.\n",
            "Preparing to unpack .../1-libfaad2_2.10.0-2_amd64.deb ...\n",
            "Unpacking libfaad2:amd64 (2.10.0-2) ...\n",
            "Selecting previously unselected package libglu1-mesa:amd64.\n",
            "Preparing to unpack .../2-libglu1-mesa_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package libmad0:amd64.\n",
            "Preparing to unpack .../3-libmad0_0.15.1b-10ubuntu1_amd64.deb ...\n",
            "Unpacking libmad0:amd64 (0.15.1b-10ubuntu1) ...\n",
            "Selecting previously unselected package libgpac11:amd64.\n",
            "Preparing to unpack .../4-libgpac11_2.0.0+dfsg1-2_amd64.deb ...\n",
            "Unpacking libgpac11:amd64 (2.0.0+dfsg1-2) ...\n",
            "Selecting previously unselected package gpac-modules-base:amd64.\n",
            "Preparing to unpack .../5-gpac-modules-base_2.0.0+dfsg1-2_amd64.deb ...\n",
            "Unpacking gpac-modules-base:amd64 (2.0.0+dfsg1-2) ...\n",
            "Selecting previously unselected package gpac.\n",
            "Preparing to unpack .../6-gpac_2.0.0+dfsg1-2_amd64.deb ...\n",
            "Unpacking gpac (2.0.0+dfsg1-2) ...\n",
            "Setting up liba52-0.7.4:amd64 (0.7.4-20) ...\n",
            "Setting up libfaad2:amd64 (2.10.0-2) ...\n",
            "Setting up libmad0:amd64 (0.15.1b-10ubuntu1) ...\n",
            "Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Setting up libgpac11:amd64 (2.0.0+dfsg1-2) ...\n",
            "Setting up gpac-modules-base:amd64 (2.0.0+dfsg1-2) ...\n",
            "Setting up gpac (2.0.0+dfsg1-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "6WiJlF6ThW--",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22b7de86-99cb-4d22-8b3d-be910f677df0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç DEBUG: Checking files...\n",
            "Encoded dir: /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Encoded\n",
            "Output dir: /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Streaming_Output\n",
            "‚úÖ ultra_high: /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Encoded/video_ultra_high.mp4\n",
            "‚úÖ high: /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Encoded/video_high.mp4\n",
            "‚úÖ medium: /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Encoded/video_medium.mp4\n",
            "‚úÖ low: /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Encoded/video_low.mp4\n",
            "‚úÖ ultra_low: /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Encoded/video_ultra_low.mp4\n",
            "üì¶ Starting comprehensive stream packaging...\n",
            "üì¶ Creating DASH manifest...\n",
            "‚úÖ Segmented ultra_high\n",
            "‚úÖ Segmented high\n",
            "‚úÖ Segmented medium\n",
            "‚úÖ Segmented low\n",
            "‚úÖ Segmented ultra_low\n",
            "‚úÖ DASH manifest created successfully\n",
            "üì¶ Creating HLS playlists...\n",
            "‚úÖ HLS playlist created: ultra_high\n",
            "‚úÖ HLS playlist created: high\n",
            "‚úÖ HLS playlist created: medium\n",
            "‚úÖ HLS playlist created: low\n",
            "‚úÖ HLS playlist created: ultra_low\n",
            "‚úÖ HLS master playlist created: /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Streaming_Output/hls/master.m3u8\n",
            "\n",
            "==================================================\n",
            "üì¶ PACKAGING SUMMARY\n",
            "==================================================\n",
            "‚úÖ DASH: /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Streaming_Output/dash/manifest.mpd\n",
            "‚úÖ HLS: /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Streaming_Output/hls/master.m3u8\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "class EnhancedStreamPackager:\n",
        "    \"\"\"Complete DASH and HLS packager with proper file handling\"\"\"\n",
        "\n",
        "    def __init__(self, encoded_dir, output_dir):\n",
        "        self.encoded_dir = Path(encoded_dir)\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.dash_dir = self.output_dir / \"dash\"\n",
        "        self.hls_dir = self.output_dir / \"hls\"\n",
        "        self.shaka_dash_dir = self.output_dir / \"shaka_dash\"\n",
        "        self.shaka_hls_dir = self.output_dir / \"shaka_hls\"\n",
        "\n",
        "        # Create all output directories\n",
        "        for directory in [self.dash_dir, self.hls_dir, self.shaka_dash_dir, self.shaka_hls_dir]:\n",
        "            directory.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def package_dash(self, segment_duration=4):\n",
        "        \"\"\"Create DASH manifest with proper segmentation using MP4Box\"\"\"\n",
        "        print(\"üì¶ Creating DASH manifest...\")\n",
        "\n",
        "        profiles = ['ultra_high', 'high', 'medium', 'low', 'ultra_low']\n",
        "        processed_files = []\n",
        "\n",
        "        # First, prepare segmented files\n",
        "        for profile in profiles:\n",
        "            input_file = self.encoded_dir / f\"video_{profile}.mp4\"\n",
        "            if not input_file.exists():\n",
        "                print(f\"‚ö†Ô∏è Skipping {profile}: file not found\")\n",
        "                continue\n",
        "\n",
        "            # Create segmented version using MP4Box\n",
        "            segmented_file = self.dash_dir / f\"video_{profile}_seg.mp4\"\n",
        "\n",
        "            cmd = [\n",
        "                \"MP4Box\",\n",
        "                \"-dash\", str(segment_duration * 1000),  # Convert to milliseconds\n",
        "                \"-frag\", str(segment_duration * 1000),\n",
        "                \"-rap\",  # Force segments to start with random access points\n",
        "                \"-segment-name\", f\"video_{profile}_%d\",\n",
        "                \"-out\", str(self.dash_dir / \"manifest\"),\n",
        "                str(input_file)\n",
        "            ]\n",
        "\n",
        "            try:\n",
        "                result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
        "                if result.returncode == 0:\n",
        "                    processed_files.append((profile, input_file))\n",
        "                    print(f\"‚úÖ Segmented {profile}\")\n",
        "                else:\n",
        "                    print(f\"‚ùå Failed to segment {profile}: {result.stderr}\")\n",
        "            except FileNotFoundError:\n",
        "                print(\"‚ö†Ô∏è MP4Box not found, trying FFmpeg approach...\")\n",
        "                return self._package_dash_ffmpeg(segment_duration)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error processing {profile}: {e}\")\n",
        "\n",
        "        # Check if manifest was created\n",
        "        manifest_file = self.dash_dir / \"manifest.mpd\"\n",
        "        if manifest_file.exists():\n",
        "            print(\"‚úÖ DASH manifest created successfully\")\n",
        "            return manifest_file\n",
        "        else:\n",
        "            print(\"‚ùå DASH manifest not created\")\n",
        "            return None\n",
        "\n",
        "    def _package_dash_ffmpeg(self, segment_duration=4):\n",
        "        \"\"\"Fallback DASH creation using FFmpeg\"\"\"\n",
        "        print(\"üì¶ Creating DASH with FFmpeg...\")\n",
        "\n",
        "        profiles = ['ultra_high', 'high', 'medium', 'low', 'ultra_low']\n",
        "        input_files = []\n",
        "\n",
        "        for profile in profiles:\n",
        "            input_file = self.encoded_dir / f\"video_{profile}.mp4\"\n",
        "            if input_file.exists():\n",
        "                input_files.append(str(input_file))\n",
        "\n",
        "        if not input_files:\n",
        "            print(\"‚ùå No input files found\")\n",
        "            return None\n",
        "\n",
        "        manifest_file = self.dash_dir / \"manifest.mpd\"\n",
        "\n",
        "        # FFmpeg command for DASH\n",
        "        cmd = [\n",
        "            \"ffmpeg\"\n",
        "        ]\n",
        "\n",
        "        # Add all input files\n",
        "        for input_file in input_files:\n",
        "            cmd.extend([\"-i\", input_file])\n",
        "\n",
        "        # Add output parameters\n",
        "        cmd.extend([\n",
        "            \"-map\", \"0:v\", \"-map\", \"1:v\", \"-map\", \"2:v\", \"-map\", \"3:v\", \"-map\", \"4:v\",\n",
        "            \"-c:v\", \"copy\",\n",
        "            \"-f\", \"dash\",\n",
        "            \"-seg_duration\", str(segment_duration),\n",
        "            \"-use_template\", \"1\",\n",
        "            \"-use_timeline\", \"1\",\n",
        "            \"-init_seg_name\", \"init_$RepresentationID$.mp4\",\n",
        "            \"-media_seg_name\", \"chunk_$RepresentationID$_$Number$.m4s\",\n",
        "            str(manifest_file),\n",
        "            \"-y\"\n",
        "        ])\n",
        "\n",
        "        try:\n",
        "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
        "            if result.returncode == 0:\n",
        "                print(\"‚úÖ FFmpeg DASH created successfully\")\n",
        "                return manifest_file\n",
        "            else:\n",
        "                print(f\"‚ùå FFmpeg DASH failed: {result.stderr}\")\n",
        "                return self._create_simple_dash_manifest()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå FFmpeg error: {e}\")\n",
        "            return self._create_simple_dash_manifest()\n",
        "\n",
        "    def _create_simple_dash_manifest(self):\n",
        "        \"\"\"Create a simple DASH manifest as last resort\"\"\"\n",
        "        print(\"üì¶ Creating simple DASH manifest...\")\n",
        "\n",
        "        profiles = ['ultra_high', 'high', 'medium', 'low', 'ultra_low']\n",
        "        input_files = []\n",
        "\n",
        "        for profile in profiles:\n",
        "            input_file = self.encoded_dir / f\"video_{profile}.mp4\"\n",
        "            if input_file.exists():\n",
        "                # Copy file to DASH directory\n",
        "                dest_file = self.dash_dir / f\"video_{profile}.mp4\"\n",
        "                import shutil\n",
        "                shutil.copy2(input_file, dest_file)\n",
        "                input_files.append((profile, dest_file))\n",
        "\n",
        "        if not input_files:\n",
        "            return None\n",
        "\n",
        "        # Get video duration\n",
        "        duration = self._get_video_duration(input_files[0][1])\n",
        "        duration_iso = f\"PT{duration}S\" if duration else \"PT120S\"\n",
        "\n",
        "        manifest_file = self.dash_dir / \"manifest.mpd\"\n",
        "\n",
        "        mpd_content = f'''<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
        "<MPD xmlns=\"urn:mpeg:dash:schema:mpd:2011\"\n",
        "     xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n",
        "     profiles=\"urn:mpeg:dash:profile:isoff-main:2011\"\n",
        "     type=\"static\"\n",
        "     mediaPresentationDuration=\"{duration_iso}\"\n",
        "     minBufferTime=\"PT4S\">\n",
        "\n",
        "  <Period>\n",
        "    <AdaptationSet id=\"0\" mimeType=\"video/mp4\" codecs=\"hvc1.1.6.L150.90\" width=\"1920\" height=\"1080\">\n",
        "'''\n",
        "\n",
        "        bitrate_configs = {\n",
        "            'ultra_high': 8000000,\n",
        "            'high': 5000000,\n",
        "            'medium': 3000000,\n",
        "            'low': 1500000,\n",
        "            'ultra_low': 800000\n",
        "        }\n",
        "\n",
        "        for profile, file_path in input_files:\n",
        "            if profile in bitrate_configs:\n",
        "                bitrate = bitrate_configs[profile]\n",
        "                mpd_content += f'''\n",
        "      <Representation id=\"{profile}\" bandwidth=\"{bitrate}\">\n",
        "        <BaseURL>{file_path.name}</BaseURL>\n",
        "        <SegmentBase indexRange=\"0-0\">\n",
        "          <Initialization range=\"0-0\"/>\n",
        "        </SegmentBase>\n",
        "      </Representation>'''\n",
        "\n",
        "        mpd_content += '''\n",
        "    </AdaptationSet>\n",
        "  </Period>\n",
        "</MPD>'''\n",
        "\n",
        "        try:\n",
        "            with open(manifest_file, 'w', encoding='utf-8') as f:\n",
        "                f.write(mpd_content)\n",
        "            print(f\"‚úÖ Simple DASH manifest created: {manifest_file}\")\n",
        "            return manifest_file\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to write manifest: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _get_video_duration(self, video_file):\n",
        "        \"\"\"Get video duration using ffprobe\"\"\"\n",
        "        try:\n",
        "            cmd = [\n",
        "                \"ffprobe\", \"-v\", \"quiet\", \"-print_format\", \"json\",\n",
        "                \"-show_format\", str(video_file)\n",
        "            ]\n",
        "            result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "            if result.returncode == 0:\n",
        "                data = json.loads(result.stdout)\n",
        "                return float(data['format']['duration'])\n",
        "        except:\n",
        "            pass\n",
        "        return None\n",
        "\n",
        "    def package_hls(self, low_latency=False):\n",
        "        \"\"\"Create HLS playlists with proper error handling\"\"\"\n",
        "        print(\"üì¶ Creating HLS playlists...\")\n",
        "\n",
        "        streams = {\n",
        "            'ultra_high': {'bitrate': 8000000, 'framerate': 60},\n",
        "            'high': {'bitrate': 5000000, 'framerate': 30},\n",
        "            'medium': {'bitrate': 3000000, 'framerate': 30},\n",
        "            'low': {'bitrate': 1500000, 'framerate': 24},\n",
        "            'ultra_low': {'bitrate': 800000, 'framerate': 15}\n",
        "        }\n",
        "\n",
        "        hls_time = 2 if low_latency else 4\n",
        "        hls_list_size = 6 if low_latency else 5\n",
        "\n",
        "        playlists = []\n",
        "        for stream_name, config in streams.items():\n",
        "            input_file = self.encoded_dir / f\"video_{stream_name}.mp4\"\n",
        "\n",
        "            if not input_file.exists():\n",
        "                print(f\"‚ö†Ô∏è Skipping {stream_name}: file not found\")\n",
        "                continue\n",
        "\n",
        "            playlist_file = self.hls_dir / f\"playlist_{stream_name}.m3u8\"\n",
        "\n",
        "            cmd = [\n",
        "                \"ffmpeg\", \"-i\", str(input_file),\n",
        "                \"-c\", \"copy\",\n",
        "                \"-f\", \"hls\",\n",
        "                \"-hls_time\", str(hls_time),\n",
        "                \"-hls_list_size\", str(hls_list_size),\n",
        "                \"-hls_playlist_type\", \"vod\",\n",
        "                \"-hls_segment_type\", \"mpegts\",\n",
        "                \"-hls_flags\", \"independent_segments\",\n",
        "                \"-hls_segment_filename\", str(self.hls_dir / f\"{stream_name}_%06d.ts\"),\n",
        "                str(playlist_file),\n",
        "                \"-y\"\n",
        "            ]\n",
        "\n",
        "            try:\n",
        "                result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
        "                if result.returncode == 0:\n",
        "                    playlists.append((stream_name, config, playlist_file))\n",
        "                    print(f\"‚úÖ HLS playlist created: {stream_name}\")\n",
        "                else:\n",
        "                    print(f\"‚ùå HLS creation failed for {stream_name}: {result.stderr}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå HLS error for {stream_name}: {e}\")\n",
        "\n",
        "        if playlists:\n",
        "            master_playlist = self._create_hls_master_playlist(playlists)\n",
        "            return master_playlist\n",
        "        else:\n",
        "            print(\"‚ùå No HLS playlists created\")\n",
        "            return None\n",
        "\n",
        "    def _create_hls_master_playlist(self, playlists):\n",
        "        \"\"\"Create HLS master playlist\"\"\"\n",
        "        master_file = self.hls_dir / 'master.m3u8'\n",
        "\n",
        "        try:\n",
        "            with open(master_file, 'w', encoding='utf-8') as f:\n",
        "                f.write('#EXTM3U\\n')\n",
        "                f.write('#EXT-X-VERSION:6\\n')\n",
        "                f.write('\\n')\n",
        "\n",
        "                for stream_name, config, playlist_file in playlists:\n",
        "                    f.write(f'#EXT-X-STREAM-INF:BANDWIDTH={config[\"bitrate\"]},')\n",
        "                    f.write(f'RESOLUTION=1920x1080,CODECS=\"hvc1.1.6.L150.90\",')\n",
        "                    f.write(f'FRAME-RATE={config[\"framerate\"]}\\n')\n",
        "                    f.write(f'{playlist_file.name}\\n\\n')\n",
        "\n",
        "            print(f\"‚úÖ HLS master playlist created: {master_file}\")\n",
        "            return master_file\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to create master playlist: {e}\")\n",
        "            return None\n",
        "\n",
        "    def package_all_formats(self):\n",
        "        \"\"\"Package streams in all formats\"\"\"\n",
        "        print(\"üì¶ Starting comprehensive stream packaging...\")\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        # Create DASH\n",
        "        results['dash'] = self.package_dash()\n",
        "\n",
        "        # Create HLS\n",
        "        results['hls'] = self.package_hls()\n",
        "\n",
        "        # Summary\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"üì¶ PACKAGING SUMMARY\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        for format_name, result in results.items():\n",
        "            if result:\n",
        "                print(f\"‚úÖ {format_name.upper()}: {result}\")\n",
        "            else:\n",
        "                print(f\"‚ùå {format_name.upper()}: Failed\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def debug_files(self):\n",
        "        \"\"\"Debug function to check file existence\"\"\"\n",
        "        print(\"\\nüîç DEBUG: Checking files...\")\n",
        "        print(f\"Encoded dir: {self.encoded_dir}\")\n",
        "        print(f\"Output dir: {self.output_dir}\")\n",
        "\n",
        "        profiles = ['ultra_high', 'high', 'medium', 'low', 'ultra_low']\n",
        "        for profile in profiles:\n",
        "            file_path = self.encoded_dir / f\"video_{profile}.mp4\"\n",
        "            exists = \"‚úÖ\" if file_path.exists() else \"‚ùå\"\n",
        "            print(f\"{exists} {profile}: {file_path}\")\n",
        "\n",
        "\n",
        "# Usage function\n",
        "def create_streaming_package(encoder_output_dir, streaming_output_dir):\n",
        "    \"\"\"Create streaming package with proper error handling\"\"\"\n",
        "\n",
        "    packager = EnhancedStreamPackager(encoder_output_dir, streaming_output_dir)\n",
        "\n",
        "    # Debug first\n",
        "    packager.debug_files()\n",
        "\n",
        "    # Package all formats\n",
        "    results = packager.package_all_formats()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Example usage with actual paths\n",
        "if __name__ == \"__main__\":\n",
        "    # SPECIFY YOUR ACTUAL PATHS HERE\n",
        "    encoder_output_directory = \"/content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Encoded\"  # Where your video_*.mp4 files are\n",
        "    streaming_output_directory = \"/content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Streaming_Output\"  # Where manifests will be created\n",
        "\n",
        "    # Example paths:\n",
        "    # encoder_output_directory = \"/home/user/projects/encoder_output\"\n",
        "    # streaming_output_directory = \"/home/user/projects/streaming_output\"\n",
        "\n",
        "    # Or relative paths:\n",
        "    # encoder_output_directory = \"./encoded_videos\"\n",
        "    # streaming_output_directory = \"./streaming_output\"\n",
        "\n",
        "    results = create_streaming_package(encoder_output_directory, streaming_output_directory)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Set your actual input and output paths here:\n",
        "encoder_output_dir = Path(\"/content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Encoded\")\n",
        "streaming_output_dir = Path(\"/content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Streaming_Output\")\n",
        "\n",
        "# Call the packager function\n",
        "create_enhanced_streaming_package(encoder_output_dir, streaming_output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7rYRR274iH0",
        "outputId": "237a33f7-f79a-4448-b315-21e40ab5e90a"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Starting comprehensive stream packaging...\n",
            "üì¶ Creating DASH manifest...\n",
            "‚úÖ Segmented ultra_high\n",
            "‚úÖ Segmented high\n",
            "‚úÖ Segmented medium\n",
            "‚úÖ Segmented low\n",
            "‚úÖ Segmented ultra_low\n",
            "‚úÖ DASH manifest created successfully\n",
            "üì¶ Creating HLS playlists...\n",
            "‚úÖ HLS playlist created: ultra_high\n",
            "‚úÖ HLS playlist created: high\n",
            "‚úÖ HLS playlist created: medium\n",
            "‚úÖ HLS playlist created: low\n",
            "‚úÖ HLS playlist created: ultra_low\n",
            "‚úÖ HLS master playlist created: /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Streaming_Output/hls/master.m3u8\n",
            "\n",
            "==================================================\n",
            "üì¶ PACKAGING SUMMARY\n",
            "==================================================\n",
            "‚úÖ DASH: /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Streaming_Output/dash/manifest.mpd\n",
            "‚úÖ HLS: /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Streaming_Output/hls/master.m3u8\n",
            "\n",
            "============================================================\n",
            "üì¶ ENHANCED STREAMING PACKAGE CREATED\n",
            "============================================================\n",
            "‚úÖ dash: /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Streaming_Output/dash/manifest.mpd\n",
            "‚úÖ hls: /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Streaming_Output/hls/master.m3u8\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dash': PosixPath('/content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Streaming_Output/dash/manifest.mpd'),\n",
              " 'hls': PosixPath('/content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Streaming_Output/hls/master.m3u8')}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql3567aehjG2"
      },
      "source": [
        "\n",
        "# ================================\n",
        "# 8. WEB PLAYER\n",
        "# ================================"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and setup ngrok in Colab\n",
        "!wget -q -O ngrok.zip https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -qq ngrok.zip\n",
        "!chmod +x ngrok\n",
        "!mv ngrok /usr/local/bin/ngrok\n",
        "\n",
        "# Check ngrok version\n",
        "!ngrok version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUe3m_VA7127",
        "outputId": "42b1a122-5c02-4320-a991-897c23c8187d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok version 2.3.41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he02ZQq38jDy",
        "outputId": "28b462b5-9326-4981-bd96-ccbcaf4c7a3e"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.11-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.11-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2yXqGWe8GZHRFcyoW96mQyj76kk_7An1nx6toXznqxhNCbm2b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYcHWmfU7mM7",
        "outputId": "c6431a19-5e0f-4b1c-924f-de9341d3f04b"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "8USO5JJghizF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1a5c219-a41a-4124-c037-aea790eb1dbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ QUICK H.265 SERVER SETUP\n",
            "========================================\n",
            "üîå Using port: 8003\n",
            "üìÅ Changing to: /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Streaming_Output\n",
            "‚úÖ Server started on port 8003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-06-17T22:11:53+0000 lvl=warn msg=\"ngrok config file found at both XDG and legacy locations, using XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Found 0 active tunnels\n",
            "‚úÖ All tunnels closed\n",
            "‚úÖ Tunnel created: NgrokTunnel: \"https://7681-34-59-195-125.ngrok-free.app\" -> \"http://localhost:8003\"\n",
            "\n",
            "==================================================\n",
            "‚úÖ H.265 TEST SERVER READY!\n",
            "==================================================\n",
            "üåê Public URL: NgrokTunnel: \"https://7681-34-59-195-125.ngrok-free.app\" -> \"http://localhost:8003\"\n",
            "üé¨ Web Player: NgrokTunnel: \"https://7681-34-59-195-125.ngrok-free.app\" -> \"http://localhost:8003\"/web_player/\n",
            "üì¶ DASH: NgrokTunnel: \"https://7681-34-59-195-125.ngrok-free.app\" -> \"http://localhost:8003\"/dash/manifest.mpd\n",
            "üìª HLS: NgrokTunnel: \"https://7681-34-59-195-125.ngrok-free.app\" -> \"http://localhost:8003\"/hls/master.m3u8\n",
            "üìÅ Serving: /content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Streaming_Output\n",
            "\n",
            "üß™ Quick file check:\n",
            "‚úÖ Web Player: web_player/index.html\n",
            "‚úÖ DASH Manifest: dash/manifest.mpd\n",
            "‚úÖ HLS Master: hls/master.m3u8\n"
          ]
        }
      ],
      "source": [
        "# QUICK FIX FOR COLAB SERVER ISSUES\n",
        "# This handles port conflicts and ngrok limits\n",
        "\n",
        "import threading\n",
        "import http.server\n",
        "import socketserver\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "def kill_existing_ngrok_tunnels():\n",
        "    \"\"\"Close existing ngrok tunnels\"\"\"\n",
        "    try:\n",
        "        from pyngrok import ngrok\n",
        "\n",
        "        # Get all active tunnels\n",
        "        tunnels = ngrok.get_tunnels()\n",
        "        print(f\"üîç Found {len(tunnels)} active tunnels\")\n",
        "\n",
        "        # Close all tunnels\n",
        "        for tunnel in tunnels:\n",
        "            print(f\"üî™ Closing tunnel: {tunnel.name}\")\n",
        "            ngrok.disconnect(tunnel.public_url)\n",
        "\n",
        "        print(\"‚úÖ All tunnels closed\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not close tunnels: {e}\")\n",
        "        return False\n",
        "\n",
        "def find_free_port():\n",
        "    \"\"\"Find a free port\"\"\"\n",
        "    import socket\n",
        "\n",
        "    for port in range(8001, 8020):  # Try ports 8001-8020\n",
        "        try:\n",
        "            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "            sock.bind(('localhost', port))\n",
        "            sock.close()\n",
        "            return port\n",
        "        except OSError:\n",
        "            continue\n",
        "\n",
        "    # If no port found, use random high port\n",
        "    return random.randint(9000, 9999)\n",
        "\n",
        "def start_simple_server():\n",
        "    \"\"\"Start a simple server with proper setup\"\"\"\n",
        "\n",
        "    # Your streaming directory (we found it above)\n",
        "    streaming_dir = \"/content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Streaming_Output\"\n",
        "\n",
        "    # Check if directory exists\n",
        "    if not Path(streaming_dir).exists():\n",
        "        print(f\"‚ùå Directory not found: {streaming_dir}\")\n",
        "        return None\n",
        "\n",
        "    # Find free port\n",
        "    port = find_free_port()\n",
        "    print(f\"üîå Using port: {port}\")\n",
        "\n",
        "    # Simple HTTP handler with CORS\n",
        "    class SimpleHandler(http.server.SimpleHTTPRequestHandler):\n",
        "        def end_headers(self):\n",
        "            self.send_header('Access-Control-Allow-Origin', '*')\n",
        "            self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')\n",
        "            self.send_header('Access-Control-Allow-Headers', '*')\n",
        "            super().end_headers()\n",
        "\n",
        "        def do_OPTIONS(self):\n",
        "            self.send_response(200)\n",
        "            self.end_headers()\n",
        "\n",
        "        def log_message(self, format, *args):\n",
        "            # Reduce logging noise\n",
        "            return\n",
        "\n",
        "    # Start server function\n",
        "    def run_server():\n",
        "        original_dir = os.getcwd()\n",
        "        try:\n",
        "            print(f\"üìÅ Changing to: {streaming_dir}\")\n",
        "            os.chdir(streaming_dir)\n",
        "\n",
        "            with socketserver.TCPServer((\"\", port), SimpleHandler) as httpd:\n",
        "                print(f\"‚úÖ Server started on port {port}\")\n",
        "                httpd.serve_forever()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Server error: {e}\")\n",
        "        finally:\n",
        "            os.chdir(original_dir)\n",
        "\n",
        "    # Start in background\n",
        "    server_thread = threading.Thread(target=run_server, daemon=True)\n",
        "    server_thread.start()\n",
        "\n",
        "    # Wait a moment for server to start\n",
        "    import time\n",
        "    time.sleep(2)\n",
        "\n",
        "    return port, streaming_dir\n",
        "\n",
        "def create_colab_tunnel(port):\n",
        "    \"\"\"Create tunnel with better error handling\"\"\"\n",
        "\n",
        "    # First, try to close existing tunnels\n",
        "    kill_existing_ngrok_tunnels()\n",
        "\n",
        "    try:\n",
        "        from pyngrok import ngrok\n",
        "\n",
        "        # Wait a moment after closing tunnels\n",
        "        import time\n",
        "        time.sleep(3)\n",
        "\n",
        "        # Create new tunnel\n",
        "        public_url = ngrok.connect(port)\n",
        "        print(f\"‚úÖ Tunnel created: {public_url}\")\n",
        "        return public_url\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Tunnel creation failed: {e}\")\n",
        "        print(f\"üí° Using Colab's built-in tunneling...\")\n",
        "\n",
        "        # Try Colab's built-in tunneling\n",
        "        try:\n",
        "            from google.colab.output import eval_js\n",
        "\n",
        "            # Create a simple tunnel using Colab's networking\n",
        "            tunnel_js = f\"\"\"\n",
        "            (async () => {{\n",
        "                const {{ port }} = await google.colab.kernel.proxyPort({port});\n",
        "                return `https://${{port}}-colab.googleusercontent.com`;\n",
        "            }})()\n",
        "            \"\"\"\n",
        "\n",
        "            colab_url = eval_js(tunnel_js)\n",
        "            print(f\"‚úÖ Colab tunnel: {colab_url}\")\n",
        "            return colab_url\n",
        "\n",
        "        except Exception as e2:\n",
        "            print(f\"‚ùå Colab tunnel also failed: {e2}\")\n",
        "            return None\n",
        "\n",
        "def quick_test_server():\n",
        "    \"\"\"Quick server setup for immediate testing\"\"\"\n",
        "\n",
        "    print(\"üöÄ QUICK H.265 SERVER SETUP\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Start server\n",
        "    result = start_simple_server()\n",
        "    if not result:\n",
        "        print(\"‚ùå Failed to start server\")\n",
        "        return None\n",
        "\n",
        "    port, streaming_dir = result\n",
        "\n",
        "    # Try to create tunnel\n",
        "    public_url = create_colab_tunnel(port)\n",
        "\n",
        "    # Show results\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"‚úÖ H.265 TEST SERVER READY!\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    if public_url:\n",
        "        print(f\"üåê Public URL: {public_url}\")\n",
        "        print(f\"üé¨ Web Player: {public_url}/web_player/\")\n",
        "        print(f\"üì¶ DASH: {public_url}/dash/manifest.mpd\")\n",
        "        print(f\"üìª HLS: {public_url}/hls/master.m3u8\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Public tunnel failed, but local server is running:\")\n",
        "        print(f\"üì± In Colab: Use file browser ‚Üí navigate to web_player/index.html\")\n",
        "        print(f\"üíª Local: http://localhost:{port}/web_player/\")\n",
        "\n",
        "    print(f\"üìÅ Serving: {streaming_dir}\")\n",
        "\n",
        "    # Test the files\n",
        "    print(f\"\\nüß™ Quick file check:\")\n",
        "    for path_name, relative_path in [\n",
        "        (\"Web Player\", \"web_player/index.html\"),\n",
        "        (\"DASH Manifest\", \"dash/manifest.mpd\"),\n",
        "        (\"HLS Master\", \"hls/master.m3u8\")\n",
        "    ]:\n",
        "        full_path = Path(streaming_dir) / relative_path\n",
        "        status = \"‚úÖ\" if full_path.exists() else \"‚ùå\"\n",
        "        print(f\"{status} {path_name}: {relative_path}\")\n",
        "\n",
        "    return {\n",
        "        'public_url': public_url,\n",
        "        'port': port,\n",
        "        'streaming_dir': streaming_dir,\n",
        "        'web_player_url': f\"{public_url}/web_player/\" if public_url else None\n",
        "    }\n",
        "\n",
        "# Alternative: Direct file access method\n",
        "def open_with_colab_files():\n",
        "    \"\"\"Alternative method using Colab's file system\"\"\"\n",
        "\n",
        "    print(\"\\nüîÑ ALTERNATIVE: Direct File Access\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    web_player_path = \"/content/drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Streaming_Output/web_player/index.html\"\n",
        "\n",
        "    if Path(web_player_path).exists():\n",
        "        print(\"‚úÖ Web player file found!\")\n",
        "        print(\"üìã Instructions:\")\n",
        "        print(\"1. Open Colab's file browser (left sidebar)\")\n",
        "        print(\"2. Navigate to: drive/MyDrive/Research/OurCode/h265_streaming_research/Algorithm_Testing/Streaming_Output/web_player/\")\n",
        "        print(\"3. Right-click on 'index.html'\")\n",
        "        print(\"4. Select 'Open in new tab'\")\n",
        "        print(\"\\nüí° This will open the player directly in your browser!\")\n",
        "\n",
        "        return web_player_path\n",
        "    else:\n",
        "        print(\"‚ùå Web player file not found\")\n",
        "        return None\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    \"\"\"Run the complete fix\"\"\"\n",
        "\n",
        "    # Try server method first\n",
        "    server_result = quick_test_server()\n",
        "\n",
        "    # If server fails, show file access method\n",
        "    if not server_result or not server_result.get('public_url'):\n",
        "        print(\"\\n\" + \"‚îÄ\" * 50)\n",
        "        file_result = open_with_colab_files()\n",
        "\n",
        "        if file_result:\n",
        "            print(f\"\\nüéØ READY TO TEST!\")\n",
        "            print(f\"Use either method above to access your H.265 player\")\n",
        "\n",
        "    return server_result\n",
        "\n",
        "# Execute the fix\n",
        "if __name__ == \"__main__\":\n",
        "    result = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nrewUTfbvja"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}