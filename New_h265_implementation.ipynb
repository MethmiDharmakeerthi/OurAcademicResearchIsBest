{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MethmiDharmakeerthi/OurAcademicResearchIsBest/blob/main/New_h265_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ur94LpAICW8j",
        "outputId": "87dc3b50-f471-4e44-da02-7d16d2f59ce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a state management system.\n",
        "# Set working directory\n",
        "import os\n",
        "\n",
        "# Check what's in Research folder\n",
        "print(\"Contents of Research folder:\")\n",
        "print(os.listdir('/content/drive/MyDrive/Research/'))\n",
        "\n",
        "# Create OurCode folder\n",
        "os.makedirs('/content/drive/MyDrive/Research/OurCode', exist_ok=True)\n",
        "\n",
        "# Verify it was created\n",
        "print(\"\\nAfter creating OurCode:\")\n",
        "print(os.listdir('/content/drive/MyDrive/Research/'))\n",
        "\n",
        "# Change directory\n",
        "os.chdir('/content/drive/MyDrive/Research/OurCode')\n",
        "print(f\"\\nCurrent working directory: {os.getcwd()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zzYxRGZACZyV",
        "outputId": "7d6add91-3e52-4477-f0c2-14dc7f8e00f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of Research folder:\n",
            "['Reference Papers', 'Research Proposal', 'Guidlines -Common By Campus', 'Graphs Created For Proposal ', 'Drafts by Gaya (chatgpt and docs ) ', 'Gaya', 'Dinu', 'Methmi', 'Budget Report', 'üí° Proposed Solution.gdoc', 'OurCode', 'Final Folder Research']\n",
            "\n",
            "After creating OurCode:\n",
            "['Reference Papers', 'Research Proposal', 'Guidlines -Common By Campus', 'Graphs Created For Proposal ', 'Drafts by Gaya (chatgpt and docs ) ', 'Gaya', 'Dinu', 'Methmi', 'Budget Report', 'üí° Proposed Solution.gdoc', 'OurCode', 'Final Folder Research']\n",
            "\n",
            "Current working directory: /content/drive/MyDrive/Research/OurCode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## State management functions\n",
        "\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zOu7Jof4ESMu",
        "outputId": "33eb3121-fb4a-42ba-ea33-9304e36417c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß State management system ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# DAILY STARTUP CELL - Run this first every day\n",
        "# ================================\n",
        "\n",
        "from google.colab import drive\n",
        "import sys\n",
        "import os\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "print(f\"üöÄ Starting research session on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set working directory\n",
        "os.chdir('/content/drive/MyDrive/Research/OurCode')\n",
        "print(f\"üìÅ Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Load state management functions\n",
        "def save_state(data, filename='research_state.pkl'):\n",
        "    \"\"\"Save current research state\"\"\"\n",
        "    os.makedirs('/content/drive/MyDrive/Research/OurCode', exist_ok=True)\n",
        "    state = {\n",
        "        'data': data,\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'session_info': f'Session on {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
        "    }\n",
        "    filepath = f'/content/drive/MyDrive/Research/OurCode/{filename}'\n",
        "    with open(filepath, 'wb') as f:\n",
        "        pickle.dump(state, f)\n",
        "    print(f\"üíæ State saved at {state['timestamp']}\")\n",
        "\n",
        "def load_state(filename='research_state.pkl'):\n",
        "    \"\"\"Load previous research state\"\"\"\n",
        "    filepath = f'/content/drive/MyDrive/Research/OurCode/{filename}'\n",
        "    try:\n",
        "        with open(filepath, 'rb') as f:\n",
        "            state = pickle.load(f)\n",
        "        print(f\"üìÇ State loaded from {state['timestamp']}\")\n",
        "        return state['data']\n",
        "    except FileNotFoundError:\n",
        "        print(\"üÜï No previous state found, starting fresh\")\n",
        "        return None\n",
        "\n",
        "# Load previous state\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"LOADING PREVIOUS SESSION...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "previous_data = load_state()\n",
        "if previous_data:\n",
        "    # Restore your variables\n",
        "    model = previous_data.get('model', None)\n",
        "    processed_data = previous_data.get('processed_data', None)\n",
        "    results = previous_data.get('results', [])\n",
        "    current_step = previous_data.get('current_step', 0)\n",
        "    experiment_params = previous_data.get('experiment_params', {})\n",
        "    notes = previous_data.get('notes', \"\")\n",
        "\n",
        "    print(f\"‚úÖ Restored session from step {current_step}\")\n",
        "    print(f\"üìä Previous results count: {len(results)}\")\n",
        "    print(f\"üìù Last notes: {notes}\")\n",
        "else:\n",
        "    # Initialize fresh session\n",
        "    model = None\n",
        "    processed_data = None\n",
        "    results = []\n",
        "    current_step = 0\n",
        "    experiment_params = {}\n",
        "    notes = \"\"\n",
        "    print(\"üÜï Starting fresh session\")\n",
        "\n",
        "print(\"\\nüî• Ready to continue research!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XBja-7BZgWwK",
        "outputId": "5dc14f0c-3c6d-4255-8683-843ce99c16be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting research session on 2025-06-07 01:13:47\n",
            "Mounted at /content/drive\n",
            "üìÅ Working directory: /content/drive/MyDrive/Research/OurCode\n",
            "\n",
            "==================================================\n",
            "LOADING PREVIOUS SESSION...\n",
            "==================================================\n",
            "üìÇ State loaded from 2025-06-05T19:54:34.703879\n",
            "‚úÖ Restored session from step 1\n",
            "üìä Previous results count: 0\n",
            "üìù Last notes: Good work\n",
            "\n",
            "üî• Ready to continue research!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c"
      ],
      "metadata": {
        "id": "E2pplmyFfDEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# PROGRESS TRACKING CELL\n",
        "# ================================\n",
        "\n",
        "def log_daily_progress(day_number, accomplishments, next_steps, issues=None, key_findings=None):\n",
        "    \"\"\"Log daily research progress\"\"\"\n",
        "    log_entry = {\n",
        "        'day': day_number,\n",
        "        'date': datetime.now().strftime('%Y-%m-%d'),\n",
        "        'start_time': datetime.now().isoformat(),\n",
        "        'accomplishments': accomplishments,\n",
        "        'next_steps': next_steps,\n",
        "        'issues': issues or [],\n",
        "        'key_findings': key_findings or [],\n",
        "        'current_step': current_step,\n",
        "        'results_count': len(results) if results else 0\n",
        "    }\n",
        "\n",
        "    # Load existing logs\n",
        "    log_file = '/content/drive/MyDrive/Research/OurCode/research_log.json'\n",
        "    try:\n",
        "        with open(log_file, 'r') as f:\n",
        "            logs = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        logs = []\n",
        "\n",
        "    logs.append(log_entry)\n",
        "\n",
        "    with open(log_file, 'w') as f:\n",
        "        json.dump(logs, f, indent=2)\n",
        "\n",
        "    print(f\"üìù Day {day_number} progress logged!\")\n",
        "    return log_entry\n",
        "\n",
        "def show_progress_summary():\n",
        "    \"\"\"Show research progress summary\"\"\"\n",
        "    try:\n",
        "        with open('/content/drive/MyDrive/Research/OurCode/research_log.json', 'r') as f:\n",
        "            logs = json.load(f)\n",
        "\n",
        "        print(\"üìà RESEARCH PROGRESS SUMMARY\")\n",
        "        print(\"=\"*40)\n",
        "        for log in logs[-5:]:  # Show last 5 days\n",
        "            print(f\"Day {log['day']} ({log['date']}):\")\n",
        "            print(f\"  ‚úÖ {', '.join(log['accomplishments'])}\")\n",
        "            if log['key_findings']:\n",
        "                print(f\"  üîç Key findings: {', '.join(log['key_findings'])}\")\n",
        "            print()\n",
        "    except FileNotFoundError:\n",
        "        print(\"No progress logs found yet\")\n",
        "\n",
        "# Example usage:\n",
        "# log_daily_progress(\n",
        "#     day_number=1,\n",
        "#     accomplishments=[\"Set up environment\", \"Loaded initial data\"],\n",
        "#     next_steps=[\"Start preprocessing\", \"Run first experiment\"],\n",
        "#     key_findings=[\"Data quality looks good\"]\n",
        "# )"
      ],
      "metadata": {
        "id": "7i7V3pfbgmhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# END OF DAY CELL - Run before closing session\n",
        "# ================================\n",
        "\n",
        "print(\"üåÖ Ending research session...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Update current step and notes\n",
        "current_step += 1  # Increment for next session\n",
        "end_notes = input(\"üìù Brief summary of today's work: \")\n",
        "\n",
        "# Save final state\n",
        "final_state = {\n",
        "    'model': model,\n",
        "    'processed_data': processed_data,\n",
        "    'results': results,\n",
        "    'current_step': current_step,\n",
        "    'experiment_params': experiment_params,\n",
        "    'notes': end_notes,\n",
        "    'session_end_time': datetime.now().isoformat()\n",
        "}\n",
        "\n",
        "save_state(final_state)\n",
        "\n",
        "# Create daily backup\n",
        "backup_filename = f\"backup_{datetime.now().strftime('%Y%m%d')}.pkl\"\n",
        "save_state(final_state, backup_filename)\n",
        "\n",
        "# Show session summary\n",
        "print(f\"\\nüìä SESSION SUMMARY:\")\n",
        "print(f\"   Current Step: {current_step}\")\n",
        "print(f\"   Results Generated: {len(results) if results else 0}\")\n",
        "print(f\"   Notes: {end_notes}\")\n",
        "print(f\"   Session Duration: Full day\")\n",
        "\n",
        "# Optional: Log progress if not done manually\n",
        "day_number = int(input(\"Which research day was this? (1-30): \"))\n",
        "accomplishments = input(\"Key accomplishments (comma-separated): \").split(',')\n",
        "next_steps = input(\"Tomorrow's priorities (comma-separated): \").split(',')\n",
        "\n",
        "log_daily_progress(day_number, accomplishments, next_steps, notes=end_notes)\n",
        "\n",
        "print(\"\\n‚úÖ Session saved successfully!\")\n",
        "print(\"üîÑ Ready for tomorrow's session\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97jKFv8Kg5Rl",
        "outputId": "768be5f9-92a7-43dd-e998-a55721fd0413"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üåÖ Ending research session...\n",
            "==================================================\n",
            "üìù Brief summary of today's work: Good work\n",
            "‚úÖ State saved successfully at 2025-06-05T19:54:34.703879\n",
            "üìÅ File location: /content/drive/MyDrive/Research/OurCode/research_state.pkl\n",
            "‚úÖ State saved successfully at 2025-06-05T19:54:34.715302\n",
            "üìÅ File location: /content/drive/MyDrive/Research/OurCode/backup_20250605.pkl\n",
            "\n",
            "üìä SESSION SUMMARY:\n",
            "   Current Step: 1\n",
            "   Results Generated: 0\n",
            "   Notes: Good work\n",
            "   Session Duration: Full day\n",
            "Which research day was this? (1-30): 1\n",
            "Key accomplishments (comma-separated): eye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UoV_HNiJqMBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zy-Q18QGG5GB",
        "outputId": "bd59608d-f7dd-46fb-c355-3b0374d1f1cd",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,759 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,744 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,017 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,984 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,553 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,295 kB]\n",
            "Fetched 21.7 MB in 3s (7,812 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "35 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.12).\n",
            "python3 is already the newest version (3.10.6-1~22.04.1).\n",
            "python3 set to manually installed.\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "The following additional packages will be installed:\n",
            "  apparmor bridge-utils containerd dns-root-data dnsmasq-base gyp iptables\n",
            "  javascript-common libc-ares2 libip6tc2 libjs-events libjs-highlight.js\n",
            "  libjs-inherits libjs-is-typedarray libjs-psl libjs-source-map\n",
            "  libjs-sprintf-js libjs-typedarray-to-buffer libmediainfo0v5 libmms0\n",
            "  libnetfilter-conntrack3 libnfnetlink0 libnftnl11 libnode-dev libnode72\n",
            "  libnotify-bin libnotify4 libtinyxml2-9 libuv1-dev libzen0v5 netbase netcat\n",
            "  netcat-openbsd node-abab node-abbrev node-agent-base node-ansi-regex\n",
            "  node-ansi-styles node-ansistyles node-aproba node-archy\n",
            "  node-are-we-there-yet node-argparse node-arrify node-asap node-asynckit\n",
            "  node-balanced-match node-brace-expansion node-builtins node-cacache\n",
            "  node-chalk node-chownr node-clean-yaml-object node-cli-table node-clone\n",
            "  node-color-convert node-color-name node-colors node-columnify\n",
            "  node-combined-stream node-commander node-console-control-strings\n",
            "  node-copy-concurrently node-core-util-is node-coveralls node-cssom\n",
            "  node-cssstyle node-debug node-decompress-response node-defaults\n",
            "  node-delayed-stream node-delegates node-depd node-diff node-encoding\n",
            "  node-end-of-stream node-err-code node-escape-string-regexp node-esprima\n",
            "  node-events node-fancy-log node-fetch node-foreground-child node-form-data\n",
            "  node-fs-write-stream-atomic node-fs.realpath node-function-bind node-gauge\n",
            "  node-get-stream node-glob node-got node-graceful-fs node-growl node-gyp\n",
            "  node-has-flag node-has-unicode node-hosted-git-info node-https-proxy-agent\n",
            "  node-iconv-lite node-iferr node-imurmurhash node-indent-string node-inflight\n",
            "  node-inherits node-ini node-ip node-ip-regex node-is-buffer\n",
            "  node-is-plain-obj node-is-typedarray node-isarray node-isexe node-js-yaml\n",
            "  node-jsdom node-json-buffer node-json-parse-better-errors node-jsonparse\n",
            "  node-kind-of node-lcov-parse node-lodash-packages node-log-driver\n",
            "  node-lowercase-keys node-lru-cache node-mime node-mime-types\n",
            "  node-mimic-response node-minimatch node-minimist node-minipass node-mkdirp\n",
            "  node-move-concurrently node-ms node-mute-stream node-negotiator node-nopt\n",
            "  node-normalize-package-data node-npm-bundled node-npm-package-arg\n",
            "  node-npmlog node-object-assign node-once node-opener node-osenv\n",
            "  node-p-cancelable node-p-map node-path-is-absolute node-process-nextick-args\n",
            "  node-promise-inflight node-promise-retry node-promzard node-psl node-pump\n",
            "  node-punycode node-quick-lru node-read node-read-package-json\n",
            "  node-readable-stream node-resolve node-retry node-rimraf node-run-queue\n",
            "  node-safe-buffer node-semver node-set-blocking node-signal-exit node-slash\n",
            "  node-slice-ansi node-source-map node-source-map-support node-spdx-correct\n",
            "  node-spdx-exceptions node-spdx-expression-parse node-spdx-license-ids\n",
            "  node-sprintf-js node-ssri node-stack-utils node-stealthy-require\n",
            "  node-string-decoder node-string-width node-strip-ansi node-supports-color\n",
            "  node-tap node-tap-mocha-reporter node-tap-parser node-tar node-text-table\n",
            "  node-time-stamp node-tmatch node-tough-cookie node-typedarray-to-buffer\n",
            "  node-unique-filename node-universalify node-util-deprecate\n",
            "  node-validate-npm-package-license node-validate-npm-package-name\n",
            "  node-wcwidth.js node-webidl-conversions node-whatwg-fetch node-which\n",
            "  node-wide-align node-wrappy node-write-file-atomic node-ws node-yallist\n",
            "  nodejs-doc python3-pkg-resources python3-setuptools python3-wheel runc\n",
            "  ubuntu-fan\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils ifupdown aufs-tools btrfs-progs\n",
            "  cgroupfs-mount | cgroup-lite debootstrap docker-buildx docker-compose-v2\n",
            "  docker-doc rinse zfs-fuse | zfsutils firewalld nftables apache2 | lighttpd\n",
            "  | httpd libjs-angularjs gnome-shell | notification-daemon mediainfo-gui\n",
            "  node-nyc python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  apparmor bridge-utils containerd dns-root-data dnsmasq-base docker.io gyp\n",
            "  iptables javascript-common libc-ares2 libip6tc2 libjs-events\n",
            "  libjs-highlight.js libjs-inherits libjs-is-typedarray libjs-psl\n",
            "  libjs-source-map libjs-sprintf-js libjs-typedarray-to-buffer libmediainfo0v5\n",
            "  libmms0 libnetfilter-conntrack3 libnfnetlink0 libnftnl11 libnode-dev\n",
            "  libnode72 libnotify-bin libnotify4 libtinyxml2-9 libuv1-dev libzen0v5\n",
            "  mediainfo netbase netcat netcat-openbsd node-abab node-abbrev\n",
            "  node-agent-base node-ansi-regex node-ansi-styles node-ansistyles node-aproba\n",
            "  node-archy node-are-we-there-yet node-argparse node-arrify node-asap\n",
            "  node-asynckit node-balanced-match node-brace-expansion node-builtins\n",
            "  node-cacache node-chalk node-chownr node-clean-yaml-object node-cli-table\n",
            "  node-clone node-color-convert node-color-name node-colors node-columnify\n",
            "  node-combined-stream node-commander node-console-control-strings\n",
            "  node-copy-concurrently node-core-util-is node-coveralls node-cssom\n",
            "  node-cssstyle node-debug node-decompress-response node-defaults\n",
            "  node-delayed-stream node-delegates node-depd node-diff node-encoding\n",
            "  node-end-of-stream node-err-code node-escape-string-regexp node-esprima\n",
            "  node-events node-fancy-log node-fetch node-foreground-child node-form-data\n",
            "  node-fs-write-stream-atomic node-fs.realpath node-function-bind node-gauge\n",
            "  node-get-stream node-glob node-got node-graceful-fs node-growl node-gyp\n",
            "  node-has-flag node-has-unicode node-hosted-git-info node-https-proxy-agent\n",
            "  node-iconv-lite node-iferr node-imurmurhash node-indent-string node-inflight\n",
            "  node-inherits node-ini node-ip node-ip-regex node-is-buffer\n",
            "  node-is-plain-obj node-is-typedarray node-isarray node-isexe node-js-yaml\n",
            "  node-jsdom node-json-buffer node-json-parse-better-errors node-jsonparse\n",
            "  node-kind-of node-lcov-parse node-lodash-packages node-log-driver\n",
            "  node-lowercase-keys node-lru-cache node-mime node-mime-types\n",
            "  node-mimic-response node-minimatch node-minimist node-minipass node-mkdirp\n",
            "  node-move-concurrently node-ms node-mute-stream node-negotiator node-nopt\n",
            "  node-normalize-package-data node-npm-bundled node-npm-package-arg\n",
            "  node-npmlog node-object-assign node-once node-opener node-osenv\n",
            "  node-p-cancelable node-p-map node-path-is-absolute node-process-nextick-args\n",
            "  node-promise-inflight node-promise-retry node-promzard node-psl node-pump\n",
            "  node-punycode node-quick-lru node-read node-read-package-json\n",
            "  node-readable-stream node-resolve node-retry node-rimraf node-run-queue\n",
            "  node-safe-buffer node-semver node-set-blocking node-signal-exit node-slash\n",
            "  node-slice-ansi node-source-map node-source-map-support node-spdx-correct\n",
            "  node-spdx-exceptions node-spdx-expression-parse node-spdx-license-ids\n",
            "  node-sprintf-js node-ssri node-stack-utils node-stealthy-require\n",
            "  node-string-decoder node-string-width node-strip-ansi node-supports-color\n",
            "  node-tap node-tap-mocha-reporter node-tap-parser node-tar node-text-table\n",
            "  node-time-stamp node-tmatch node-tough-cookie node-typedarray-to-buffer\n",
            "  node-unique-filename node-universalify node-util-deprecate\n",
            "  node-validate-npm-package-license node-validate-npm-package-name\n",
            "  node-wcwidth.js node-webidl-conversions node-whatwg-fetch node-which\n",
            "  node-wide-align node-wrappy node-write-file-atomic node-ws node-yallist\n",
            "  nodejs nodejs-doc npm python3-pip python3-setuptools python3-wheel runc\n",
            "  ubuntu-fan x265\n",
            "The following packages will be upgraded:\n",
            "  python3-pkg-resources\n",
            "1 upgraded, 215 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 104 MB of archives.\n",
            "After this operation, 414 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 netbase all 6.3 [12.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 netcat-openbsd amd64 1.218-4ubuntu1 [39.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n",
            "Get:4 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 python3-pkg-resources all 68.1.2-2~jammy3 [216 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libip6tc2 amd64 1.8.7-1ubuntu5.2 [20.3 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnfnetlink0 amd64 1.0.1-3build3 [14.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnetfilter-conntrack3 amd64 1.0.9-1 [45.3 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnftnl11 amd64 1.2.1-1build1 [65.5 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 iptables amd64 1.8.7-1ubuntu5.2 [455 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 bridge-utils amd64 1.7-1ubuntu3 [34.4 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 runc amd64 1.2.5-0ubuntu1~22.04.1 [8,093 kB]\n",
            "Get:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 python3-setuptools all 68.1.2-2~jammy3 [465 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 containerd amd64 1.7.27-0ubuntu1~22.04.1 [37.8 MB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 dns-root-data all 2024071801~ubuntu0.22.04.1 [6,132 B]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 dnsmasq-base amd64 2.90-0ubuntu0.22.04.1 [374 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 docker.io amd64 27.5.1-0ubuntu3~22.04.2 [33.3 MB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/universe amd64 gyp all 0.1+20210831gitd6c5dd5-5 [238 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 javascript-common all 11+nmu1 [5,936 B]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libjs-events all 3.3.0+~3.0.0-2 [9,734 B]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libjs-highlight.js all 9.18.5+dfsg1-1 [367 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libjs-is-typedarray all 1.0.0-4 [3,804 B]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libjs-psl all 1.8.0+ds-6 [76.3 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libjs-sprintf-js all 1.1.2+ds1+~1.1.2-1 [12.8 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libjs-typedarray-to-buffer all 4.0.0-2 [4,658 B]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmms0 amd64 0.6.4-3 [27.3 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libtinyxml2-9 amd64 9.0.0+dfsg-3 [32.5 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libzen0v5 amd64 0.4.39-1 [97.5 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmediainfo0v5 amd64 21.09+dfsg-4 [2,255 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libuv1-dev amd64 1.43.0-1ubuntu0.1 [130 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libc-ares2 amd64 1.18.1-1ubuntu0.22.04.3 [45.1 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libnode72 amd64 12.22.9~dfsg-1ubuntu3.6 [10.8 MB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libnode-dev amd64 12.22.9~dfsg-1ubuntu3.6 [609 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libnotify4 amd64 0.7.9-3ubuntu5.22.04.1 [20.3 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libnotify-bin amd64 0.7.9-3ubuntu5.22.04.1 [7,560 B]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu jammy/universe amd64 mediainfo amd64 22.03-1 [28.5 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu jammy/universe amd64 netcat all 1.218-4ubuntu1 [2,044 B]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 nodejs amd64 12.22.9~dfsg-1ubuntu3.6 [122 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-abab all 2.0.5-2 [6,578 B]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-ms all 2.1.3+~cs0.7.31-2 [5,782 B]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-debug all 4.3.2+~cs4.1.7-1 [17.6 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-yallist all 4.0.0+~4.0.1-1 [8,322 B]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-lru-cache all 6.0.0+~5.1.1-1 [11.3 kB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-semver all 7.3.5+~7.3.8-1 [41.5 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-agent-base all 6.0.2+~cs5.4.2-1 [17.9 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-ansi-regex all 5.0.1-1 [4,984 B]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-ansistyles all 0.1.3-5 [4,546 B]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-aproba all 2.0.0-2 [5,620 B]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-delegates all 1.0.0-3 [4,280 B]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libjs-inherits all 2.0.4-4 [3,468 B]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-inherits all 2.0.4-4 [3,010 B]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-core-util-is all 1.0.3-1 [4,066 B]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-safe-buffer all 5.2.1+~cs2.1.2-2 [15.7 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-string-decoder all 1.3.0-5 [7,046 B]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-process-nextick-args all 2.0.1-2 [3,730 B]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-util-deprecate all 1.0.2-3 [4,202 B]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-isarray all 2.0.5-3 [3,934 B]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-readable-stream all 3.6.0+~cs3.0.0-1 [32.6 kB]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-are-we-there-yet all 3.0.0+~1.1.0-1 [8,920 B]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-arrify all 2.0.1-2 [3,610 B]\n",
            "Get:60 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-asap all 2.0.6+~2.0.0-1 [14.4 kB]\n",
            "Get:61 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-asynckit all 0.4.0-4 [10.6 kB]\n",
            "Get:62 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-builtins all 4.0.0-1 [3,860 B]\n",
            "Get:63 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-chownr all 2.0.0-1 [4,404 B]\n",
            "Get:64 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-fs.realpath all 1.0.0-2 [6,106 B]\n",
            "Get:65 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-wrappy all 1.0.2-2 [3,658 B]\n",
            "Get:66 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-once all 1.4.0-4 [4,708 B]\n",
            "Get:67 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-inflight all 1.0.6-2 [3,940 B]\n",
            "Get:68 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-balanced-match all 2.0.0-1 [4,910 B]\n",
            "Get:69 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-brace-expansion all 2.0.1-1 [7,458 B]\n",
            "Get:70 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-minimatch all 3.1.1+~3.0.5-1 [16.9 kB]\n",
            "Get:71 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-path-is-absolute all 2.0.0-2 [4,062 B]\n",
            "Get:72 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-glob all 7.2.1+~cs7.6.15-1 [131 kB]\n",
            "Get:73 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-graceful-fs all 4.2.4+repack-1 [12.5 kB]\n",
            "Get:74 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-mkdirp all 1.0.4+~1.0.2-1 [11.4 kB]\n",
            "Get:75 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-iferr all 1.0.2+~1.0.2-1 [4,610 B]\n",
            "Get:76 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-imurmurhash all 0.1.4+dfsg+~0.1.1-1 [8,510 B]\n",
            "Get:77 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-fs-write-stream-atomic all 1.0.10-5 [5,256 B]\n",
            "Get:78 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-rimraf all 3.0.2-1 [10.1 kB]\n",
            "Get:79 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-run-queue all 2.0.0-2 [5,092 B]\n",
            "Get:80 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-copy-concurrently all 1.0.5-8 [7,118 B]\n",
            "Get:81 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-move-concurrently all 1.0.1-4 [5,120 B]\n",
            "Get:82 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-escape-string-regexp all 4.0.0-2 [4,328 B]\n",
            "Get:83 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-indent-string all 4.0.0-2 [4,122 B]\n",
            "Get:84 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-p-map all 4.0.0+~3.1.0+~3.0.1-1 [8,058 B]\n",
            "Get:85 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-promise-inflight all 1.0.1+~1.0.0-1 [4,896 B]\n",
            "Get:86 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-ssri all 8.0.1-2 [19.6 kB]\n",
            "Get:87 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-unique-filename all 1.1.1+ds-1 [3,832 B]\n",
            "Get:88 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-cacache all 15.0.5+~cs13.9.21-3 [34.9 kB]\n",
            "Get:89 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-clean-yaml-object all 0.1.0-5 [4,718 B]\n",
            "Get:90 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-clone all 2.1.2-3 [8,344 B]\n",
            "Get:91 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-color-name all 1.1.4+~1.1.1-2 [6,076 B]\n",
            "Get:92 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-color-convert all 2.0.1-1 [10.2 kB]\n",
            "Get:93 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-colors all 1.4.0-3 [12.3 kB]\n",
            "Get:94 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-strip-ansi all 6.0.1-1 [4,184 B]\n",
            "Get:95 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-defaults all 1.0.3+~1.0.3-1 [4,288 B]\n",
            "Get:96 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-wcwidth.js all 1.0.2-1 [7,278 B]\n",
            "Get:97 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-columnify all 1.5.4+~1.5.1-1 [12.6 kB]\n",
            "Get:98 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-console-control-strings all 1.1.0-2 [5,428 B]\n",
            "Get:99 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-growl all 1.10.5-4 [7,064 B]\n",
            "Get:100 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-sprintf-js all 1.1.2+ds1+~1.1.2-1 [3,916 B]\n",
            "Get:101 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-argparse all 2.0.1-2 [33.2 kB]\n",
            "Get:102 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-esprima all 4.0.1+ds+~4.0.3-2 [69.3 kB]\n",
            "Get:103 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-js-yaml all 4.1.0+dfsg+~4.0.5-6 [62.7 kB]\n",
            "Get:104 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-lcov-parse all 1.0.0+20170612git80d039574ed9-5 [5,084 B]\n",
            "Get:105 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-log-driver all 1.2.7+git+20180219+bba1761737-7 [5,436 B]\n",
            "Get:106 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-is-plain-obj all 3.0.0-2 [3,994 B]\n",
            "Get:107 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-is-buffer all 2.0.5-2 [4,128 B]\n",
            "Get:108 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-kind-of all 6.0.3+dfsg-2 [8,628 B]\n",
            "Get:109 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-minimist all 1.2.5+~cs5.3.2-1 [9,434 B]\n",
            "Get:110 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-cssom all 0.4.4-3 [14.1 kB]\n",
            "Get:111 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-cssstyle all 2.3.0-2 [30.3 kB]\n",
            "Get:112 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-delayed-stream all 1.0.0-5 [5,464 B]\n",
            "Get:113 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-combined-stream all 1.0.8+~1.0.3-1 [7,432 B]\n",
            "Get:114 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-mime all 3.0.0+dfsg+~cs3.96.1-1 [38.1 kB]\n",
            "Get:115 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-mime-types all 2.1.33-1 [6,944 B]\n",
            "Get:116 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-form-data all 3.0.1-1 [13.4 kB]\n",
            "Get:117 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-events all 3.3.0+~3.0.0-2 [3,090 B]\n",
            "Get:118 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-https-proxy-agent all 5.0.0+~cs8.0.0-3 [16.4 kB]\n",
            "Get:119 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-iconv-lite all 0.6.3-2 [167 kB]\n",
            "Get:120 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-lodash-packages all 4.17.21+dfsg+~cs8.31.198.20210220-5 [166 kB]\n",
            "Get:121 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-stealthy-require all 1.1.1-5 [7,176 B]\n",
            "Get:122 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-punycode all 2.1.1-5 [9,902 B]\n",
            "Get:123 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-psl all 1.8.0+ds-6 [39.6 kB]\n",
            "Get:124 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-universalify all 2.0.0-3 [4,266 B]\n",
            "Get:125 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-tough-cookie all 4.0.0-2 [31.7 kB]\n",
            "Get:126 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-webidl-conversions all 7.0.0~1.1.0+~cs15.1.20180823-2 [27.5 kB]\n",
            "Get:127 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-commander all 9.0.0-2 [48.0 kB]\n",
            "Get:128 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-mute-stream all 0.0.8+~0.0.1-1 [6,448 B]\n",
            "Get:129 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-read all 1.0.7-3 [5,478 B]\n",
            "Get:130 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-ws all 8.5.0+~cs13.3.3-2 [49.5 kB]\n",
            "Get:131 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-jsdom all 19.0.0+~cs90.11.27-1 [446 kB]\n",
            "Get:132 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-fetch all 2.6.7+~2.5.12-1 [27.1 kB]\n",
            "Get:133 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-coveralls all 3.1.1-1 [14.2 kB]\n",
            "Get:134 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-mimic-response all 3.1.0-7 [5,430 B]\n",
            "Get:135 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-decompress-response all 6.0.0-2 [4,656 B]\n",
            "Get:136 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-diff all 5.0.0~dfsg+~5.0.1-3 [77.4 kB]\n",
            "Get:137 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-err-code all 2.0.3+dfsg-3 [4,918 B]\n",
            "Get:138 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-time-stamp all 2.2.0-1 [5,984 B]\n",
            "Get:139 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-fancy-log all 1.3.3+~cs1.3.1-2 [8,102 B]\n",
            "Get:140 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-signal-exit all 3.0.6+~3.0.1-1 [7,000 B]\n",
            "Get:141 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-foreground-child all 2.0.0-3 [5,542 B]\n",
            "Get:142 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-function-bind all 1.1.1+repacked+~1.0.3-1 [5,244 B]\n",
            "Get:143 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-has-unicode all 2.0.1-4 [3,948 B]\n",
            "Get:144 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-ansi-styles all 4.3.0+~4.2.0-1 [8,968 B]\n",
            "Get:145 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-slice-ansi all 5.0.0+~cs9.0.0-4 [8,044 B]\n",
            "Get:146 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-string-width all 4.2.3+~cs13.2.3-1 [11.4 kB]\n",
            "Get:147 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-wide-align all 1.1.3-4 [4,228 B]\n",
            "Get:148 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-gauge all 4.0.2-1 [16.3 kB]\n",
            "Get:149 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-end-of-stream all 1.4.4+~1.4.1-1 [5,340 B]\n",
            "Get:150 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-pump all 3.0.0-5 [5,160 B]\n",
            "Get:151 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-get-stream all 6.0.1-1 [7,324 B]\n",
            "Get:152 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-lowercase-keys all 2.0.0-2 [3,754 B]\n",
            "Get:153 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-json-buffer all 3.0.1-1 [3,812 B]\n",
            "Get:154 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-p-cancelable all 2.1.1-1 [7,358 B]\n",
            "Get:155 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-quick-lru all 5.1.1-1 [5,532 B]\n",
            "Get:156 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-got all 11.8.3+~cs58.7.37-1 [122 kB]\n",
            "Get:157 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-has-flag all 4.0.0-2 [4,228 B]\n",
            "Get:158 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-hosted-git-info all 4.0.2-1 [9,006 B]\n",
            "Get:159 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-ip all 1.1.5+~1.1.0-1 [8,140 B]\n",
            "Get:160 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-ip-regex all 4.3.0+~4.1.1-1 [5,254 B]\n",
            "Get:161 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-is-typedarray all 1.0.0-4 [2,072 B]\n",
            "Get:162 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-isexe all 2.0.0+~2.0.1-4 [6,102 B]\n",
            "Get:163 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-json-parse-better-errors all 1.0.2+~cs3.3.1-1 [7,328 B]\n",
            "Get:164 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-encoding all 0.1.13-2 [4,366 B]\n",
            "Get:165 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-jsonparse all 1.3.1-10 [8,060 B]\n",
            "Get:166 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-minipass all 3.1.6+~cs8.7.18-1 [32.9 kB]\n",
            "Get:167 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-npm-bundled all 1.1.2-1 [6,228 B]\n",
            "Get:168 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-osenv all 0.1.5+~0.1.0-1 [5,896 B]\n",
            "Get:169 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-validate-npm-package-name all 3.0.0-4 [5,058 B]\n",
            "Get:170 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-npm-package-arg all 8.1.5-1 [8,132 B]\n",
            "Get:171 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-object-assign all 4.1.1-6 [4,754 B]\n",
            "Get:172 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-opener all 1.5.2+~1.4.0-1 [6,000 B]\n",
            "Get:173 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-retry all 0.13.1+~0.12.1-1 [11.5 kB]\n",
            "Get:174 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-promise-retry all 2.0.1-2 [5,010 B]\n",
            "Get:175 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-promzard all 0.3.0-2 [6,888 B]\n",
            "Get:176 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-set-blocking all 2.0.0-2 [3,766 B]\n",
            "Get:177 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-slash all 3.0.0-2 [3,922 B]\n",
            "Get:178 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libjs-source-map all 0.7.0++dfsg2+really.0.6.1-9 [93.9 kB]\n",
            "Get:179 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-source-map all 0.7.0++dfsg2+really.0.6.1-9 [33.6 kB]\n",
            "Get:180 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-source-map-support all 0.5.21+ds+~0.5.4-1 [14.2 kB]\n",
            "Get:181 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-spdx-license-ids all 3.0.11-1 [7,306 B]\n",
            "Get:182 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-spdx-exceptions all 2.3.0-2 [3,978 B]\n",
            "Get:183 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-spdx-expression-parse all 3.0.1+~3.0.1-1 [7,658 B]\n",
            "Get:184 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-spdx-correct all 3.1.1-2 [5,476 B]\n",
            "Get:185 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-stack-utils all 2.0.5+~2.0.1-1 [9,368 B]\n",
            "Get:186 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-supports-color all 8.1.1+~8.1.1-1 [7,048 B]\n",
            "Get:187 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-tap-parser all 7.0.0+ds1-6 [19.4 kB]\n",
            "Get:188 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-tap-mocha-reporter all 3.0.7+ds-2 [39.2 kB]\n",
            "Get:189 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-text-table all 0.2.0-4 [4,762 B]\n",
            "Get:190 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-tmatch all 5.0.0-4 [6,002 B]\n",
            "Get:191 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-typedarray-to-buffer all 4.0.0-2 [2,242 B]\n",
            "Get:192 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-validate-npm-package-license all 3.0.4-2 [4,252 B]\n",
            "Get:193 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-whatwg-fetch all 3.6.2-5 [15.0 kB]\n",
            "Get:194 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-write-file-atomic all 3.0.3+~3.0.2-1 [7,690 B]\n",
            "Get:195 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 nodejs-doc all 12.22.9~dfsg-1ubuntu3.6 [2,411 kB]\n",
            "Get:196 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-wheel all 0.37.1-2ubuntu0.22.04.1 [32.0 kB]\n",
            "Get:197 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-pip all 22.0.2+dfsg-1ubuntu0.5 [1,306 kB]\n",
            "Get:198 http://archive.ubuntu.com/ubuntu jammy/universe amd64 ubuntu-fan all 0.12.16 [35.2 kB]\n",
            "Get:199 http://archive.ubuntu.com/ubuntu jammy/universe amd64 x265 amd64 3.5-2 [56.8 kB]\n",
            "Get:200 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-abbrev all 1.1.1+~1.1.2-1 [5,784 B]\n",
            "Get:201 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-archy all 1.0.0-4 [4,728 B]\n",
            "Get:202 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-chalk all 4.1.2-1 [15.9 kB]\n",
            "Get:203 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-cli-table all 0.3.11+~cs0.13.3-1 [23.2 kB]\n",
            "Get:204 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-depd all 2.0.0-2 [10.5 kB]\n",
            "Get:205 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-nopt all 5.0.0-2 [11.3 kB]\n",
            "Get:206 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-npmlog all 6.0.1+~4.1.4-1 [9,968 B]\n",
            "Get:207 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-tar all 6.1.11+ds1+~cs6.0.6-1 [38.8 kB]\n",
            "Get:208 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-which all 2.0.2+~cs1.3.2-2 [7,374 B]\n",
            "Get:209 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-gyp all 8.4.1-1 [34.7 kB]\n",
            "Get:210 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-ini all 2.0.1-1 [6,528 B]\n",
            "Get:211 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-negotiator all 0.6.2+~0.6.1-1 [10.3 kB]\n",
            "Get:212 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-resolve all 1.20.0+~cs5.27.9-1 [20.7 kB]\n",
            "Get:213 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-normalize-package-data all 3.0.3+~2.4.1-1 [12.8 kB]\n",
            "Get:214 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-read-package-json all 4.1.1-1 [10.4 kB]\n",
            "Get:215 http://archive.ubuntu.com/ubuntu jammy/universe amd64 node-tap all 12.0.1+ds-4 [43.6 kB]\n",
            "Get:216 http://archive.ubuntu.com/ubuntu jammy/universe amd64 npm all 8.5.1~ds-1 [894 kB]\n",
            "Fetched 104 MB in 6s (16.2 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 216.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package netbase.\n",
            "(Reading database ... 126111 files and directories currently installed.)\n",
            "Preparing to unpack .../000-netbase_6.3_all.deb ...\n",
            "Unpacking netbase (6.3) ...\n",
            "Selecting previously unselected package netcat-openbsd.\n",
            "Preparing to unpack .../001-netcat-openbsd_1.218-4ubuntu1_amd64.deb ...\n",
            "Unpacking netcat-openbsd (1.218-4ubuntu1) ...\n",
            "Selecting previously unselected package apparmor.\n",
            "Preparing to unpack .../002-apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package libip6tc2:amd64.\n",
            "Preparing to unpack .../003-libip6tc2_1.8.7-1ubuntu5.2_amd64.deb ...\n",
            "Unpacking libip6tc2:amd64 (1.8.7-1ubuntu5.2) ...\n",
            "Selecting previously unselected package libnfnetlink0:amd64.\n",
            "Preparing to unpack .../004-libnfnetlink0_1.0.1-3build3_amd64.deb ...\n",
            "Unpacking libnfnetlink0:amd64 (1.0.1-3build3) ...\n",
            "Selecting previously unselected package libnetfilter-conntrack3:amd64.\n",
            "Preparing to unpack .../005-libnetfilter-conntrack3_1.0.9-1_amd64.deb ...\n",
            "Unpacking libnetfilter-conntrack3:amd64 (1.0.9-1) ...\n",
            "Selecting previously unselected package libnftnl11:amd64.\n",
            "Preparing to unpack .../006-libnftnl11_1.2.1-1build1_amd64.deb ...\n",
            "Unpacking libnftnl11:amd64 (1.2.1-1build1) ...\n",
            "Selecting previously unselected package iptables.\n",
            "Preparing to unpack .../007-iptables_1.8.7-1ubuntu5.2_amd64.deb ...\n",
            "Unpacking iptables (1.8.7-1ubuntu5.2) ...\n",
            "Selecting previously unselected package bridge-utils.\n",
            "Preparing to unpack .../008-bridge-utils_1.7-1ubuntu3_amd64.deb ...\n",
            "Unpacking bridge-utils (1.7-1ubuntu3) ...\n",
            "Selecting previously unselected package runc.\n",
            "Preparing to unpack .../009-runc_1.2.5-0ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking runc (1.2.5-0ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package containerd.\n",
            "Preparing to unpack .../010-containerd_1.7.27-0ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking containerd (1.7.27-0ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package dns-root-data.\n",
            "Preparing to unpack .../011-dns-root-data_2024071801~ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking dns-root-data (2024071801~ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package dnsmasq-base.\n",
            "Preparing to unpack .../012-dnsmasq-base_2.90-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking dnsmasq-base (2.90-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package docker.io.\n",
            "Preparing to unpack .../013-docker.io_27.5.1-0ubuntu3~22.04.2_amd64.deb ...\n",
            "Unpacking docker.io (27.5.1-0ubuntu3~22.04.2) ...\n",
            "Preparing to unpack .../014-python3-pkg-resources_68.1.2-2~jammy3_all.deb ...\n",
            "Unpacking python3-pkg-resources (68.1.2-2~jammy3) over (59.6.0-1.2ubuntu0.22.04.3) ...\n",
            "Selecting previously unselected package gyp.\n",
            "Preparing to unpack .../015-gyp_0.1+20210831gitd6c5dd5-5_all.deb ...\n",
            "Unpacking gyp (0.1+20210831gitd6c5dd5-5) ...\n",
            "Selecting previously unselected package javascript-common.\n",
            "Preparing to unpack .../016-javascript-common_11+nmu1_all.deb ...\n",
            "Unpacking javascript-common (11+nmu1) ...\n",
            "Selecting previously unselected package libjs-events.\n",
            "Preparing to unpack .../017-libjs-events_3.3.0+~3.0.0-2_all.deb ...\n",
            "Unpacking libjs-events (3.3.0+~3.0.0-2) ...\n",
            "Selecting previously unselected package libjs-highlight.js.\n",
            "Preparing to unpack .../018-libjs-highlight.js_9.18.5+dfsg1-1_all.deb ...\n",
            "Unpacking libjs-highlight.js (9.18.5+dfsg1-1) ...\n",
            "Selecting previously unselected package libjs-is-typedarray.\n",
            "Preparing to unpack .../019-libjs-is-typedarray_1.0.0-4_all.deb ...\n",
            "Unpacking libjs-is-typedarray (1.0.0-4) ...\n",
            "Selecting previously unselected package libjs-psl.\n",
            "Preparing to unpack .../020-libjs-psl_1.8.0+ds-6_all.deb ...\n",
            "Unpacking libjs-psl (1.8.0+ds-6) ...\n",
            "Selecting previously unselected package libjs-sprintf-js.\n",
            "Preparing to unpack .../021-libjs-sprintf-js_1.1.2+ds1+~1.1.2-1_all.deb ...\n",
            "Unpacking libjs-sprintf-js (1.1.2+ds1+~1.1.2-1) ...\n",
            "Selecting previously unselected package libjs-typedarray-to-buffer.\n",
            "Preparing to unpack .../022-libjs-typedarray-to-buffer_4.0.0-2_all.deb ...\n",
            "Unpacking libjs-typedarray-to-buffer (4.0.0-2) ...\n",
            "Selecting previously unselected package libmms0:amd64.\n",
            "Preparing to unpack .../023-libmms0_0.6.4-3_amd64.deb ...\n",
            "Unpacking libmms0:amd64 (0.6.4-3) ...\n",
            "Selecting previously unselected package libtinyxml2-9:amd64.\n",
            "Preparing to unpack .../024-libtinyxml2-9_9.0.0+dfsg-3_amd64.deb ...\n",
            "Unpacking libtinyxml2-9:amd64 (9.0.0+dfsg-3) ...\n",
            "Selecting previously unselected package libzen0v5:amd64.\n",
            "Preparing to unpack .../025-libzen0v5_0.4.39-1_amd64.deb ...\n",
            "Unpacking libzen0v5:amd64 (0.4.39-1) ...\n",
            "Selecting previously unselected package libmediainfo0v5:amd64.\n",
            "Preparing to unpack .../026-libmediainfo0v5_21.09+dfsg-4_amd64.deb ...\n",
            "Unpacking libmediainfo0v5:amd64 (21.09+dfsg-4) ...\n",
            "Selecting previously unselected package libuv1-dev:amd64.\n",
            "Preparing to unpack .../027-libuv1-dev_1.43.0-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libuv1-dev:amd64 (1.43.0-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "Preparing to unpack .../028-libc-ares2_1.18.1-1ubuntu0.22.04.3_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Selecting previously unselected package libnode72:amd64.\n",
            "Preparing to unpack .../029-libnode72_12.22.9~dfsg-1ubuntu3.6_amd64.deb ...\n",
            "Unpacking libnode72:amd64 (12.22.9~dfsg-1ubuntu3.6) ...\n",
            "Selecting previously unselected package libnode-dev.\n",
            "Preparing to unpack .../030-libnode-dev_12.22.9~dfsg-1ubuntu3.6_amd64.deb ...\n",
            "Unpacking libnode-dev (12.22.9~dfsg-1ubuntu3.6) ...\n",
            "Selecting previously unselected package libnotify4:amd64.\n",
            "Preparing to unpack .../031-libnotify4_0.7.9-3ubuntu5.22.04.1_amd64.deb ...\n",
            "Unpacking libnotify4:amd64 (0.7.9-3ubuntu5.22.04.1) ...\n",
            "Selecting previously unselected package libnotify-bin.\n",
            "Preparing to unpack .../032-libnotify-bin_0.7.9-3ubuntu5.22.04.1_amd64.deb ...\n",
            "Unpacking libnotify-bin (0.7.9-3ubuntu5.22.04.1) ...\n",
            "Selecting previously unselected package mediainfo.\n",
            "Preparing to unpack .../033-mediainfo_22.03-1_amd64.deb ...\n",
            "Unpacking mediainfo (22.03-1) ...\n",
            "Selecting previously unselected package netcat.\n",
            "Preparing to unpack .../034-netcat_1.218-4ubuntu1_all.deb ...\n",
            "Unpacking netcat (1.218-4ubuntu1) ...\n",
            "Selecting previously unselected package nodejs.\n",
            "Preparing to unpack .../035-nodejs_12.22.9~dfsg-1ubuntu3.6_amd64.deb ...\n",
            "Unpacking nodejs (12.22.9~dfsg-1ubuntu3.6) ...\n",
            "Selecting previously unselected package node-abab.\n",
            "Preparing to unpack .../036-node-abab_2.0.5-2_all.deb ...\n",
            "Unpacking node-abab (2.0.5-2) ...\n",
            "Selecting previously unselected package node-ms.\n",
            "Preparing to unpack .../037-node-ms_2.1.3+~cs0.7.31-2_all.deb ...\n",
            "Unpacking node-ms (2.1.3+~cs0.7.31-2) ...\n",
            "Selecting previously unselected package node-debug.\n",
            "Preparing to unpack .../038-node-debug_4.3.2+~cs4.1.7-1_all.deb ...\n",
            "Unpacking node-debug (4.3.2+~cs4.1.7-1) ...\n",
            "Selecting previously unselected package node-yallist.\n",
            "Preparing to unpack .../039-node-yallist_4.0.0+~4.0.1-1_all.deb ...\n",
            "Unpacking node-yallist (4.0.0+~4.0.1-1) ...\n",
            "Selecting previously unselected package node-lru-cache.\n",
            "Preparing to unpack .../040-node-lru-cache_6.0.0+~5.1.1-1_all.deb ...\n",
            "Unpacking node-lru-cache (6.0.0+~5.1.1-1) ...\n",
            "Selecting previously unselected package node-semver.\n",
            "Preparing to unpack .../041-node-semver_7.3.5+~7.3.8-1_all.deb ...\n",
            "Unpacking node-semver (7.3.5+~7.3.8-1) ...\n",
            "Selecting previously unselected package node-agent-base.\n",
            "Preparing to unpack .../042-node-agent-base_6.0.2+~cs5.4.2-1_all.deb ...\n",
            "Unpacking node-agent-base (6.0.2+~cs5.4.2-1) ...\n",
            "Selecting previously unselected package node-ansi-regex.\n",
            "Preparing to unpack .../043-node-ansi-regex_5.0.1-1_all.deb ...\n",
            "Unpacking node-ansi-regex (5.0.1-1) ...\n",
            "Selecting previously unselected package node-ansistyles.\n",
            "Preparing to unpack .../044-node-ansistyles_0.1.3-5_all.deb ...\n",
            "Unpacking node-ansistyles (0.1.3-5) ...\n",
            "Selecting previously unselected package node-aproba.\n",
            "Preparing to unpack .../045-node-aproba_2.0.0-2_all.deb ...\n",
            "Unpacking node-aproba (2.0.0-2) ...\n",
            "Selecting previously unselected package node-delegates.\n",
            "Preparing to unpack .../046-node-delegates_1.0.0-3_all.deb ...\n",
            "Unpacking node-delegates (1.0.0-3) ...\n",
            "Selecting previously unselected package libjs-inherits.\n",
            "Preparing to unpack .../047-libjs-inherits_2.0.4-4_all.deb ...\n",
            "Unpacking libjs-inherits (2.0.4-4) ...\n",
            "Selecting previously unselected package node-inherits.\n",
            "Preparing to unpack .../048-node-inherits_2.0.4-4_all.deb ...\n",
            "Unpacking node-inherits (2.0.4-4) ...\n",
            "Selecting previously unselected package node-core-util-is.\n",
            "Preparing to unpack .../049-node-core-util-is_1.0.3-1_all.deb ...\n",
            "Unpacking node-core-util-is (1.0.3-1) ...\n",
            "Selecting previously unselected package node-safe-buffer.\n",
            "Preparing to unpack .../050-node-safe-buffer_5.2.1+~cs2.1.2-2_all.deb ...\n",
            "Unpacking node-safe-buffer (5.2.1+~cs2.1.2-2) ...\n",
            "Selecting previously unselected package node-string-decoder.\n",
            "Preparing to unpack .../051-node-string-decoder_1.3.0-5_all.deb ...\n",
            "Unpacking node-string-decoder (1.3.0-5) ...\n",
            "Selecting previously unselected package node-process-nextick-args.\n",
            "Preparing to unpack .../052-node-process-nextick-args_2.0.1-2_all.deb ...\n",
            "Unpacking node-process-nextick-args (2.0.1-2) ...\n",
            "Selecting previously unselected package node-util-deprecate.\n",
            "Preparing to unpack .../053-node-util-deprecate_1.0.2-3_all.deb ...\n",
            "Unpacking node-util-deprecate (1.0.2-3) ...\n",
            "Selecting previously unselected package node-isarray.\n",
            "Preparing to unpack .../054-node-isarray_2.0.5-3_all.deb ...\n",
            "Unpacking node-isarray (2.0.5-3) ...\n",
            "Selecting previously unselected package node-readable-stream.\n",
            "Preparing to unpack .../055-node-readable-stream_3.6.0+~cs3.0.0-1_all.deb ...\n",
            "Unpacking node-readable-stream (3.6.0+~cs3.0.0-1) ...\n",
            "Selecting previously unselected package node-are-we-there-yet.\n",
            "Preparing to unpack .../056-node-are-we-there-yet_3.0.0+~1.1.0-1_all.deb ...\n",
            "Unpacking node-are-we-there-yet (3.0.0+~1.1.0-1) ...\n",
            "Selecting previously unselected package node-arrify.\n",
            "Preparing to unpack .../057-node-arrify_2.0.1-2_all.deb ...\n",
            "Unpacking node-arrify (2.0.1-2) ...\n",
            "Selecting previously unselected package node-asap.\n",
            "Preparing to unpack .../058-node-asap_2.0.6+~2.0.0-1_all.deb ...\n",
            "Unpacking node-asap (2.0.6+~2.0.0-1) ...\n",
            "Selecting previously unselected package node-asynckit.\n",
            "Preparing to unpack .../059-node-asynckit_0.4.0-4_all.deb ...\n",
            "Unpacking node-asynckit (0.4.0-4) ...\n",
            "Selecting previously unselected package node-builtins.\n",
            "Preparing to unpack .../060-node-builtins_4.0.0-1_all.deb ...\n",
            "Unpacking node-builtins (4.0.0-1) ...\n",
            "Selecting previously unselected package node-chownr.\n",
            "Preparing to unpack .../061-node-chownr_2.0.0-1_all.deb ...\n",
            "Unpacking node-chownr (2.0.0-1) ...\n",
            "Selecting previously unselected package node-fs.realpath.\n",
            "Preparing to unpack .../062-node-fs.realpath_1.0.0-2_all.deb ...\n",
            "Unpacking node-fs.realpath (1.0.0-2) ...\n",
            "Selecting previously unselected package node-wrappy.\n",
            "Preparing to unpack .../063-node-wrappy_1.0.2-2_all.deb ...\n",
            "Unpacking node-wrappy (1.0.2-2) ...\n",
            "Selecting previously unselected package node-once.\n",
            "Preparing to unpack .../064-node-once_1.4.0-4_all.deb ...\n",
            "Unpacking node-once (1.4.0-4) ...\n",
            "Selecting previously unselected package node-inflight.\n",
            "Preparing to unpack .../065-node-inflight_1.0.6-2_all.deb ...\n",
            "Unpacking node-inflight (1.0.6-2) ...\n",
            "Selecting previously unselected package node-balanced-match.\n",
            "Preparing to unpack .../066-node-balanced-match_2.0.0-1_all.deb ...\n",
            "Unpacking node-balanced-match (2.0.0-1) ...\n",
            "Selecting previously unselected package node-brace-expansion.\n",
            "Preparing to unpack .../067-node-brace-expansion_2.0.1-1_all.deb ...\n",
            "Unpacking node-brace-expansion (2.0.1-1) ...\n",
            "Selecting previously unselected package node-minimatch.\n",
            "Preparing to unpack .../068-node-minimatch_3.1.1+~3.0.5-1_all.deb ...\n",
            "Unpacking node-minimatch (3.1.1+~3.0.5-1) ...\n",
            "Selecting previously unselected package node-path-is-absolute.\n",
            "Preparing to unpack .../069-node-path-is-absolute_2.0.0-2_all.deb ...\n",
            "Unpacking node-path-is-absolute (2.0.0-2) ...\n",
            "Selecting previously unselected package node-glob.\n",
            "Preparing to unpack .../070-node-glob_7.2.1+~cs7.6.15-1_all.deb ...\n",
            "Unpacking node-glob (7.2.1+~cs7.6.15-1) ...\n",
            "Selecting previously unselected package node-graceful-fs.\n",
            "Preparing to unpack .../071-node-graceful-fs_4.2.4+repack-1_all.deb ...\n",
            "Unpacking node-graceful-fs (4.2.4+repack-1) ...\n",
            "Selecting previously unselected package node-mkdirp.\n",
            "Preparing to unpack .../072-node-mkdirp_1.0.4+~1.0.2-1_all.deb ...\n",
            "Unpacking node-mkdirp (1.0.4+~1.0.2-1) ...\n",
            "Selecting previously unselected package node-iferr.\n",
            "Preparing to unpack .../073-node-iferr_1.0.2+~1.0.2-1_all.deb ...\n",
            "Unpacking node-iferr (1.0.2+~1.0.2-1) ...\n",
            "Selecting previously unselected package node-imurmurhash.\n",
            "Preparing to unpack .../074-node-imurmurhash_0.1.4+dfsg+~0.1.1-1_all.deb ...\n",
            "Unpacking node-imurmurhash (0.1.4+dfsg+~0.1.1-1) ...\n",
            "Selecting previously unselected package node-fs-write-stream-atomic.\n",
            "Preparing to unpack .../075-node-fs-write-stream-atomic_1.0.10-5_all.deb ...\n",
            "Unpacking node-fs-write-stream-atomic (1.0.10-5) ...\n",
            "Selecting previously unselected package node-rimraf.\n",
            "Preparing to unpack .../076-node-rimraf_3.0.2-1_all.deb ...\n",
            "Unpacking node-rimraf (3.0.2-1) ...\n",
            "Selecting previously unselected package node-run-queue.\n",
            "Preparing to unpack .../077-node-run-queue_2.0.0-2_all.deb ...\n",
            "Unpacking node-run-queue (2.0.0-2) ...\n",
            "Selecting previously unselected package node-copy-concurrently.\n",
            "Preparing to unpack .../078-node-copy-concurrently_1.0.5-8_all.deb ...\n",
            "Unpacking node-copy-concurrently (1.0.5-8) ...\n",
            "Selecting previously unselected package node-move-concurrently.\n",
            "Preparing to unpack .../079-node-move-concurrently_1.0.1-4_all.deb ...\n",
            "Unpacking node-move-concurrently (1.0.1-4) ...\n",
            "Selecting previously unselected package node-escape-string-regexp.\n",
            "Preparing to unpack .../080-node-escape-string-regexp_4.0.0-2_all.deb ...\n",
            "Unpacking node-escape-string-regexp (4.0.0-2) ...\n",
            "Selecting previously unselected package node-indent-string.\n",
            "Preparing to unpack .../081-node-indent-string_4.0.0-2_all.deb ...\n",
            "Unpacking node-indent-string (4.0.0-2) ...\n",
            "Selecting previously unselected package node-p-map.\n",
            "Preparing to unpack .../082-node-p-map_4.0.0+~3.1.0+~3.0.1-1_all.deb ...\n",
            "Unpacking node-p-map (4.0.0+~3.1.0+~3.0.1-1) ...\n",
            "Selecting previously unselected package node-promise-inflight.\n",
            "Preparing to unpack .../083-node-promise-inflight_1.0.1+~1.0.0-1_all.deb ...\n",
            "Unpacking node-promise-inflight (1.0.1+~1.0.0-1) ...\n",
            "Selecting previously unselected package node-ssri.\n",
            "Preparing to unpack .../084-node-ssri_8.0.1-2_all.deb ...\n",
            "Unpacking node-ssri (8.0.1-2) ...\n",
            "Selecting previously unselected package node-unique-filename.\n",
            "Preparing to unpack .../085-node-unique-filename_1.1.1+ds-1_all.deb ...\n",
            "Unpacking node-unique-filename (1.1.1+ds-1) ...\n",
            "Selecting previously unselected package node-cacache.\n",
            "Preparing to unpack .../086-node-cacache_15.0.5+~cs13.9.21-3_all.deb ...\n",
            "Unpacking node-cacache (15.0.5+~cs13.9.21-3) ...\n",
            "Selecting previously unselected package node-clean-yaml-object.\n",
            "Preparing to unpack .../087-node-clean-yaml-object_0.1.0-5_all.deb ...\n",
            "Unpacking node-clean-yaml-object (0.1.0-5) ...\n",
            "Selecting previously unselected package node-clone.\n",
            "Preparing to unpack .../088-node-clone_2.1.2-3_all.deb ...\n",
            "Unpacking node-clone (2.1.2-3) ...\n",
            "Selecting previously unselected package node-color-name.\n",
            "Preparing to unpack .../089-node-color-name_1.1.4+~1.1.1-2_all.deb ...\n",
            "Unpacking node-color-name (1.1.4+~1.1.1-2) ...\n",
            "Selecting previously unselected package node-color-convert.\n",
            "Preparing to unpack .../090-node-color-convert_2.0.1-1_all.deb ...\n",
            "Unpacking node-color-convert (2.0.1-1) ...\n",
            "Selecting previously unselected package node-colors.\n",
            "Preparing to unpack .../091-node-colors_1.4.0-3_all.deb ...\n",
            "Unpacking node-colors (1.4.0-3) ...\n",
            "Selecting previously unselected package node-strip-ansi.\n",
            "Preparing to unpack .../092-node-strip-ansi_6.0.1-1_all.deb ...\n",
            "Unpacking node-strip-ansi (6.0.1-1) ...\n",
            "Selecting previously unselected package node-defaults.\n",
            "Preparing to unpack .../093-node-defaults_1.0.3+~1.0.3-1_all.deb ...\n",
            "Unpacking node-defaults (1.0.3+~1.0.3-1) ...\n",
            "Selecting previously unselected package node-wcwidth.js.\n",
            "Preparing to unpack .../094-node-wcwidth.js_1.0.2-1_all.deb ...\n",
            "Unpacking node-wcwidth.js (1.0.2-1) ...\n",
            "Selecting previously unselected package node-columnify.\n",
            "Preparing to unpack .../095-node-columnify_1.5.4+~1.5.1-1_all.deb ...\n",
            "Unpacking node-columnify (1.5.4+~1.5.1-1) ...\n",
            "Selecting previously unselected package node-console-control-strings.\n",
            "Preparing to unpack .../096-node-console-control-strings_1.1.0-2_all.deb ...\n",
            "Unpacking node-console-control-strings (1.1.0-2) ...\n",
            "Selecting previously unselected package node-growl.\n",
            "Preparing to unpack .../097-node-growl_1.10.5-4_all.deb ...\n",
            "Unpacking node-growl (1.10.5-4) ...\n",
            "Selecting previously unselected package node-sprintf-js.\n",
            "Preparing to unpack .../098-node-sprintf-js_1.1.2+ds1+~1.1.2-1_all.deb ...\n",
            "Unpacking node-sprintf-js (1.1.2+ds1+~1.1.2-1) ...\n",
            "Selecting previously unselected package node-argparse.\n",
            "Preparing to unpack .../099-node-argparse_2.0.1-2_all.deb ...\n",
            "Unpacking node-argparse (2.0.1-2) ...\n",
            "Selecting previously unselected package node-esprima.\n",
            "Preparing to unpack .../100-node-esprima_4.0.1+ds+~4.0.3-2_all.deb ...\n",
            "Unpacking node-esprima (4.0.1+ds+~4.0.3-2) ...\n",
            "Selecting previously unselected package node-js-yaml.\n",
            "Preparing to unpack .../101-node-js-yaml_4.1.0+dfsg+~4.0.5-6_all.deb ...\n",
            "Unpacking node-js-yaml (4.1.0+dfsg+~4.0.5-6) ...\n",
            "Selecting previously unselected package node-lcov-parse.\n",
            "Preparing to unpack .../102-node-lcov-parse_1.0.0+20170612git80d039574ed9-5_all.deb ...\n",
            "Unpacking node-lcov-parse (1.0.0+20170612git80d039574ed9-5) ...\n",
            "Selecting previously unselected package node-log-driver.\n",
            "Preparing to unpack .../103-node-log-driver_1.2.7+git+20180219+bba1761737-7_all.deb ...\n",
            "Unpacking node-log-driver (1.2.7+git+20180219+bba1761737-7) ...\n",
            "Selecting previously unselected package node-is-plain-obj.\n",
            "Preparing to unpack .../104-node-is-plain-obj_3.0.0-2_all.deb ...\n",
            "Unpacking node-is-plain-obj (3.0.0-2) ...\n",
            "Selecting previously unselected package node-is-buffer.\n",
            "Preparing to unpack .../105-node-is-buffer_2.0.5-2_all.deb ...\n",
            "Unpacking node-is-buffer (2.0.5-2) ...\n",
            "Selecting previously unselected package node-kind-of.\n",
            "Preparing to unpack .../106-node-kind-of_6.0.3+dfsg-2_all.deb ...\n",
            "Unpacking node-kind-of (6.0.3+dfsg-2) ...\n",
            "Selecting previously unselected package node-minimist.\n",
            "Preparing to unpack .../107-node-minimist_1.2.5+~cs5.3.2-1_all.deb ...\n",
            "Unpacking node-minimist (1.2.5+~cs5.3.2-1) ...\n",
            "Selecting previously unselected package node-cssom.\n",
            "Preparing to unpack .../108-node-cssom_0.4.4-3_all.deb ...\n",
            "Unpacking node-cssom (0.4.4-3) ...\n",
            "Selecting previously unselected package node-cssstyle.\n",
            "Preparing to unpack .../109-node-cssstyle_2.3.0-2_all.deb ...\n",
            "Unpacking node-cssstyle (2.3.0-2) ...\n",
            "Selecting previously unselected package node-delayed-stream.\n",
            "Preparing to unpack .../110-node-delayed-stream_1.0.0-5_all.deb ...\n",
            "Unpacking node-delayed-stream (1.0.0-5) ...\n",
            "Selecting previously unselected package node-combined-stream.\n",
            "Preparing to unpack .../111-node-combined-stream_1.0.8+~1.0.3-1_all.deb ...\n",
            "Unpacking node-combined-stream (1.0.8+~1.0.3-1) ...\n",
            "Selecting previously unselected package node-mime.\n",
            "Preparing to unpack .../112-node-mime_3.0.0+dfsg+~cs3.96.1-1_all.deb ...\n",
            "Unpacking node-mime (3.0.0+dfsg+~cs3.96.1-1) ...\n",
            "Selecting previously unselected package node-mime-types.\n",
            "Preparing to unpack .../113-node-mime-types_2.1.33-1_all.deb ...\n",
            "Unpacking node-mime-types (2.1.33-1) ...\n",
            "Selecting previously unselected package node-form-data.\n",
            "Preparing to unpack .../114-node-form-data_3.0.1-1_all.deb ...\n",
            "Unpacking node-form-data (3.0.1-1) ...\n",
            "Selecting previously unselected package node-events.\n",
            "Preparing to unpack .../115-node-events_3.3.0+~3.0.0-2_all.deb ...\n",
            "Unpacking node-events (3.3.0+~3.0.0-2) ...\n",
            "Selecting previously unselected package node-https-proxy-agent.\n",
            "Preparing to unpack .../116-node-https-proxy-agent_5.0.0+~cs8.0.0-3_all.deb ...\n",
            "Unpacking node-https-proxy-agent (5.0.0+~cs8.0.0-3) ...\n",
            "Selecting previously unselected package node-iconv-lite.\n",
            "Preparing to unpack .../117-node-iconv-lite_0.6.3-2_all.deb ...\n",
            "Unpacking node-iconv-lite (0.6.3-2) ...\n",
            "Selecting previously unselected package node-lodash-packages.\n",
            "Preparing to unpack .../118-node-lodash-packages_4.17.21+dfsg+~cs8.31.198.20210220-5_all.deb ...\n",
            "Unpacking node-lodash-packages (4.17.21+dfsg+~cs8.31.198.20210220-5) ...\n",
            "Selecting previously unselected package node-stealthy-require.\n",
            "Preparing to unpack .../119-node-stealthy-require_1.1.1-5_all.deb ...\n",
            "Unpacking node-stealthy-require (1.1.1-5) ...\n",
            "Selecting previously unselected package node-punycode.\n",
            "Preparing to unpack .../120-node-punycode_2.1.1-5_all.deb ...\n",
            "Unpacking node-punycode (2.1.1-5) ...\n",
            "Selecting previously unselected package node-psl.\n",
            "Preparing to unpack .../121-node-psl_1.8.0+ds-6_all.deb ...\n",
            "Unpacking node-psl (1.8.0+ds-6) ...\n",
            "Selecting previously unselected package node-universalify.\n",
            "Preparing to unpack .../122-node-universalify_2.0.0-3_all.deb ...\n",
            "Unpacking node-universalify (2.0.0-3) ...\n",
            "Selecting previously unselected package node-tough-cookie.\n",
            "Preparing to unpack .../123-node-tough-cookie_4.0.0-2_all.deb ...\n",
            "Unpacking node-tough-cookie (4.0.0-2) ...\n",
            "Selecting previously unselected package node-webidl-conversions.\n",
            "Preparing to unpack .../124-node-webidl-conversions_7.0.0~1.1.0+~cs15.1.20180823-2_all.deb ...\n",
            "Unpacking node-webidl-conversions (7.0.0~1.1.0+~cs15.1.20180823-2) ...\n",
            "Selecting previously unselected package node-commander.\n",
            "Preparing to unpack .../125-node-commander_9.0.0-2_all.deb ...\n",
            "Unpacking node-commander (9.0.0-2) ...\n",
            "Selecting previously unselected package node-mute-stream.\n",
            "Preparing to unpack .../126-node-mute-stream_0.0.8+~0.0.1-1_all.deb ...\n",
            "Unpacking node-mute-stream (0.0.8+~0.0.1-1) ...\n",
            "Selecting previously unselected package node-read.\n",
            "Preparing to unpack .../127-node-read_1.0.7-3_all.deb ...\n",
            "Unpacking node-read (1.0.7-3) ...\n",
            "Selecting previously unselected package node-ws.\n",
            "Preparing to unpack .../128-node-ws_8.5.0+~cs13.3.3-2_all.deb ...\n",
            "Unpacking node-ws (8.5.0+~cs13.3.3-2) ...\n",
            "Selecting previously unselected package node-jsdom.\n",
            "Preparing to unpack .../129-node-jsdom_19.0.0+~cs90.11.27-1_all.deb ...\n",
            "Unpacking node-jsdom (19.0.0+~cs90.11.27-1) ...\n",
            "Selecting previously unselected package node-fetch.\n",
            "Preparing to unpack .../130-node-fetch_2.6.7+~2.5.12-1_all.deb ...\n",
            "Unpacking node-fetch (2.6.7+~2.5.12-1) ...\n",
            "Selecting previously unselected package node-coveralls.\n",
            "Preparing to unpack .../131-node-coveralls_3.1.1-1_all.deb ...\n",
            "Unpacking node-coveralls (3.1.1-1) ...\n",
            "Selecting previously unselected package node-mimic-response.\n",
            "Preparing to unpack .../132-node-mimic-response_3.1.0-7_all.deb ...\n",
            "Unpacking node-mimic-response (3.1.0-7) ...\n",
            "Selecting previously unselected package node-decompress-response.\n",
            "Preparing to unpack .../133-node-decompress-response_6.0.0-2_all.deb ...\n",
            "Unpacking node-decompress-response (6.0.0-2) ...\n",
            "Selecting previously unselected package node-diff.\n",
            "Preparing to unpack .../134-node-diff_5.0.0~dfsg+~5.0.1-3_all.deb ...\n",
            "Unpacking node-diff (5.0.0~dfsg+~5.0.1-3) ...\n",
            "Selecting previously unselected package node-err-code.\n",
            "Preparing to unpack .../135-node-err-code_2.0.3+dfsg-3_all.deb ...\n",
            "Unpacking node-err-code (2.0.3+dfsg-3) ...\n",
            "Selecting previously unselected package node-time-stamp.\n",
            "Preparing to unpack .../136-node-time-stamp_2.2.0-1_all.deb ...\n",
            "Unpacking node-time-stamp (2.2.0-1) ...\n",
            "Selecting previously unselected package node-fancy-log.\n",
            "Preparing to unpack .../137-node-fancy-log_1.3.3+~cs1.3.1-2_all.deb ...\n",
            "Unpacking node-fancy-log (1.3.3+~cs1.3.1-2) ...\n",
            "Selecting previously unselected package node-signal-exit.\n",
            "Preparing to unpack .../138-node-signal-exit_3.0.6+~3.0.1-1_all.deb ...\n",
            "Unpacking node-signal-exit (3.0.6+~3.0.1-1) ...\n",
            "Selecting previously unselected package node-foreground-child.\n",
            "Preparing to unpack .../139-node-foreground-child_2.0.0-3_all.deb ...\n",
            "Unpacking node-foreground-child (2.0.0-3) ...\n",
            "Selecting previously unselected package node-function-bind.\n",
            "Preparing to unpack .../140-node-function-bind_1.1.1+repacked+~1.0.3-1_all.deb ...\n",
            "Unpacking node-function-bind (1.1.1+repacked+~1.0.3-1) ...\n",
            "Selecting previously unselected package node-has-unicode.\n",
            "Preparing to unpack .../141-node-has-unicode_2.0.1-4_all.deb ...\n",
            "Unpacking node-has-unicode (2.0.1-4) ...\n",
            "Selecting previously unselected package node-ansi-styles.\n",
            "Preparing to unpack .../142-node-ansi-styles_4.3.0+~4.2.0-1_all.deb ...\n",
            "Unpacking node-ansi-styles (4.3.0+~4.2.0-1) ...\n",
            "Selecting previously unselected package node-slice-ansi.\n",
            "Preparing to unpack .../143-node-slice-ansi_5.0.0+~cs9.0.0-4_all.deb ...\n",
            "Unpacking node-slice-ansi (5.0.0+~cs9.0.0-4) ...\n",
            "Selecting previously unselected package node-string-width.\n",
            "Preparing to unpack .../144-node-string-width_4.2.3+~cs13.2.3-1_all.deb ...\n",
            "Unpacking node-string-width (4.2.3+~cs13.2.3-1) ...\n",
            "Selecting previously unselected package node-wide-align.\n",
            "Preparing to unpack .../145-node-wide-align_1.1.3-4_all.deb ...\n",
            "Unpacking node-wide-align (1.1.3-4) ...\n",
            "Selecting previously unselected package node-gauge.\n",
            "Preparing to unpack .../146-node-gauge_4.0.2-1_all.deb ...\n",
            "Unpacking node-gauge (4.0.2-1) ...\n",
            "Selecting previously unselected package node-end-of-stream.\n",
            "Preparing to unpack .../147-node-end-of-stream_1.4.4+~1.4.1-1_all.deb ...\n",
            "Unpacking node-end-of-stream (1.4.4+~1.4.1-1) ...\n",
            "Selecting previously unselected package node-pump.\n",
            "Preparing to unpack .../148-node-pump_3.0.0-5_all.deb ...\n",
            "Unpacking node-pump (3.0.0-5) ...\n",
            "Selecting previously unselected package node-get-stream.\n",
            "Preparing to unpack .../149-node-get-stream_6.0.1-1_all.deb ...\n",
            "Unpacking node-get-stream (6.0.1-1) ...\n",
            "Selecting previously unselected package node-lowercase-keys.\n",
            "Preparing to unpack .../150-node-lowercase-keys_2.0.0-2_all.deb ...\n",
            "Unpacking node-lowercase-keys (2.0.0-2) ...\n",
            "Selecting previously unselected package node-json-buffer.\n",
            "Preparing to unpack .../151-node-json-buffer_3.0.1-1_all.deb ...\n",
            "Unpacking node-json-buffer (3.0.1-1) ...\n",
            "Selecting previously unselected package node-p-cancelable.\n",
            "Preparing to unpack .../152-node-p-cancelable_2.1.1-1_all.deb ...\n",
            "Unpacking node-p-cancelable (2.1.1-1) ...\n",
            "Selecting previously unselected package node-quick-lru.\n",
            "Preparing to unpack .../153-node-quick-lru_5.1.1-1_all.deb ...\n",
            "Unpacking node-quick-lru (5.1.1-1) ...\n",
            "Selecting previously unselected package node-got.\n",
            "Preparing to unpack .../154-node-got_11.8.3+~cs58.7.37-1_all.deb ...\n",
            "Unpacking node-got (11.8.3+~cs58.7.37-1) ...\n",
            "Selecting previously unselected package node-has-flag.\n",
            "Preparing to unpack .../155-node-has-flag_4.0.0-2_all.deb ...\n",
            "Unpacking node-has-flag (4.0.0-2) ...\n",
            "Selecting previously unselected package node-hosted-git-info.\n",
            "Preparing to unpack .../156-node-hosted-git-info_4.0.2-1_all.deb ...\n",
            "Unpacking node-hosted-git-info (4.0.2-1) ...\n",
            "Selecting previously unselected package node-ip.\n",
            "Preparing to unpack .../157-node-ip_1.1.5+~1.1.0-1_all.deb ...\n",
            "Unpacking node-ip (1.1.5+~1.1.0-1) ...\n",
            "Selecting previously unselected package node-ip-regex.\n",
            "Preparing to unpack .../158-node-ip-regex_4.3.0+~4.1.1-1_all.deb ...\n",
            "Unpacking node-ip-regex (4.3.0+~4.1.1-1) ...\n",
            "Selecting previously unselected package node-is-typedarray.\n",
            "Preparing to unpack .../159-node-is-typedarray_1.0.0-4_all.deb ...\n",
            "Unpacking node-is-typedarray (1.0.0-4) ...\n",
            "Selecting previously unselected package node-isexe.\n",
            "Preparing to unpack .../160-node-isexe_2.0.0+~2.0.1-4_all.deb ...\n",
            "Unpacking node-isexe (2.0.0+~2.0.1-4) ...\n",
            "Selecting previously unselected package node-json-parse-better-errors.\n",
            "Preparing to unpack .../161-node-json-parse-better-errors_1.0.2+~cs3.3.1-1_all.deb ...\n",
            "Unpacking node-json-parse-better-errors (1.0.2+~cs3.3.1-1) ...\n",
            "Selecting previously unselected package node-encoding.\n",
            "Preparing to unpack .../162-node-encoding_0.1.13-2_all.deb ...\n",
            "Unpacking node-encoding (0.1.13-2) ...\n",
            "Selecting previously unselected package node-jsonparse.\n",
            "Preparing to unpack .../163-node-jsonparse_1.3.1-10_all.deb ...\n",
            "Unpacking node-jsonparse (1.3.1-10) ...\n",
            "Selecting previously unselected package node-minipass.\n",
            "Preparing to unpack .../164-node-minipass_3.1.6+~cs8.7.18-1_all.deb ...\n",
            "Unpacking node-minipass (3.1.6+~cs8.7.18-1) ...\n",
            "Selecting previously unselected package node-npm-bundled.\n",
            "Preparing to unpack .../165-node-npm-bundled_1.1.2-1_all.deb ...\n",
            "Unpacking node-npm-bundled (1.1.2-1) ...\n",
            "Selecting previously unselected package node-osenv.\n",
            "Preparing to unpack .../166-node-osenv_0.1.5+~0.1.0-1_all.deb ...\n",
            "Unpacking node-osenv (0.1.5+~0.1.0-1) ...\n",
            "Selecting previously unselected package node-validate-npm-package-name.\n",
            "Preparing to unpack .../167-node-validate-npm-package-name_3.0.0-4_all.deb ...\n",
            "Unpacking node-validate-npm-package-name (3.0.0-4) ...\n",
            "Selecting previously unselected package node-npm-package-arg.\n",
            "Preparing to unpack .../168-node-npm-package-arg_8.1.5-1_all.deb ...\n",
            "Unpacking node-npm-package-arg (8.1.5-1) ...\n",
            "Selecting previously unselected package node-object-assign.\n",
            "Preparing to unpack .../169-node-object-assign_4.1.1-6_all.deb ...\n",
            "Unpacking node-object-assign (4.1.1-6) ...\n",
            "Selecting previously unselected package node-opener.\n",
            "Preparing to unpack .../170-node-opener_1.5.2+~1.4.0-1_all.deb ...\n",
            "Unpacking node-opener (1.5.2+~1.4.0-1) ...\n",
            "Selecting previously unselected package node-retry.\n",
            "Preparing to unpack .../171-node-retry_0.13.1+~0.12.1-1_all.deb ...\n",
            "Unpacking node-retry (0.13.1+~0.12.1-1) ...\n",
            "Selecting previously unselected package node-promise-retry.\n",
            "Preparing to unpack .../172-node-promise-retry_2.0.1-2_all.deb ...\n",
            "Unpacking node-promise-retry (2.0.1-2) ...\n",
            "Selecting previously unselected package node-promzard.\n",
            "Preparing to unpack .../173-node-promzard_0.3.0-2_all.deb ...\n",
            "Unpacking node-promzard (0.3.0-2) ...\n",
            "Selecting previously unselected package node-set-blocking.\n",
            "Preparing to unpack .../174-node-set-blocking_2.0.0-2_all.deb ...\n",
            "Unpacking node-set-blocking (2.0.0-2) ...\n",
            "Selecting previously unselected package node-slash.\n",
            "Preparing to unpack .../175-node-slash_3.0.0-2_all.deb ...\n",
            "Unpacking node-slash (3.0.0-2) ...\n",
            "Selecting previously unselected package libjs-source-map.\n",
            "Preparing to unpack .../176-libjs-source-map_0.7.0++dfsg2+really.0.6.1-9_all.deb ...\n",
            "Unpacking libjs-source-map (0.7.0++dfsg2+really.0.6.1-9) ...\n",
            "Selecting previously unselected package node-source-map.\n",
            "Preparing to unpack .../177-node-source-map_0.7.0++dfsg2+really.0.6.1-9_all.deb ...\n",
            "Unpacking node-source-map (0.7.0++dfsg2+really.0.6.1-9) ...\n",
            "Selecting previously unselected package node-source-map-support.\n",
            "Preparing to unpack .../178-node-source-map-support_0.5.21+ds+~0.5.4-1_all.deb ...\n",
            "Unpacking node-source-map-support (0.5.21+ds+~0.5.4-1) ...\n",
            "Selecting previously unselected package node-spdx-license-ids.\n",
            "Preparing to unpack .../179-node-spdx-license-ids_3.0.11-1_all.deb ...\n",
            "Unpacking node-spdx-license-ids (3.0.11-1) ...\n",
            "Selecting previously unselected package node-spdx-exceptions.\n",
            "Preparing to unpack .../180-node-spdx-exceptions_2.3.0-2_all.deb ...\n",
            "Unpacking node-spdx-exceptions (2.3.0-2) ...\n",
            "Selecting previously unselected package node-spdx-expression-parse.\n",
            "Preparing to unpack .../181-node-spdx-expression-parse_3.0.1+~3.0.1-1_all.deb ...\n",
            "Unpacking node-spdx-expression-parse (3.0.1+~3.0.1-1) ...\n",
            "Selecting previously unselected package node-spdx-correct.\n",
            "Preparing to unpack .../182-node-spdx-correct_3.1.1-2_all.deb ...\n",
            "Unpacking node-spdx-correct (3.1.1-2) ...\n",
            "Selecting previously unselected package node-stack-utils.\n",
            "Preparing to unpack .../183-node-stack-utils_2.0.5+~2.0.1-1_all.deb ...\n",
            "Unpacking node-stack-utils (2.0.5+~2.0.1-1) ...\n",
            "Selecting previously unselected package node-supports-color.\n",
            "Preparing to unpack .../184-node-supports-color_8.1.1+~8.1.1-1_all.deb ...\n",
            "Unpacking node-supports-color (8.1.1+~8.1.1-1) ...\n",
            "Selecting previously unselected package node-tap-parser.\n",
            "Preparing to unpack .../185-node-tap-parser_7.0.0+ds1-6_all.deb ...\n",
            "Unpacking node-tap-parser (7.0.0+ds1-6) ...\n",
            "Selecting previously unselected package node-tap-mocha-reporter.\n",
            "Preparing to unpack .../186-node-tap-mocha-reporter_3.0.7+ds-2_all.deb ...\n",
            "Unpacking node-tap-mocha-reporter (3.0.7+ds-2) ...\n",
            "Selecting previously unselected package node-text-table.\n",
            "Preparing to unpack .../187-node-text-table_0.2.0-4_all.deb ...\n",
            "Unpacking node-text-table (0.2.0-4) ...\n",
            "Selecting previously unselected package node-tmatch.\n",
            "Preparing to unpack .../188-node-tmatch_5.0.0-4_all.deb ...\n",
            "Unpacking node-tmatch (5.0.0-4) ...\n",
            "Selecting previously unselected package node-typedarray-to-buffer.\n",
            "Preparing to unpack .../189-node-typedarray-to-buffer_4.0.0-2_all.deb ...\n",
            "Unpacking node-typedarray-to-buffer (4.0.0-2) ...\n",
            "Selecting previously unselected package node-validate-npm-package-license.\n",
            "Preparing to unpack .../190-node-validate-npm-package-license_3.0.4-2_all.deb ...\n",
            "Unpacking node-validate-npm-package-license (3.0.4-2) ...\n",
            "Selecting previously unselected package node-whatwg-fetch.\n",
            "Preparing to unpack .../191-node-whatwg-fetch_3.6.2-5_all.deb ...\n",
            "Unpacking node-whatwg-fetch (3.6.2-5) ...\n",
            "Selecting previously unselected package node-write-file-atomic.\n",
            "Preparing to unpack .../192-node-write-file-atomic_3.0.3+~3.0.2-1_all.deb ...\n",
            "Unpacking node-write-file-atomic (3.0.3+~3.0.2-1) ...\n",
            "Selecting previously unselected package nodejs-doc.\n",
            "Preparing to unpack .../193-nodejs-doc_12.22.9~dfsg-1ubuntu3.6_all.deb ...\n",
            "Unpacking nodejs-doc (12.22.9~dfsg-1ubuntu3.6) ...\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "Preparing to unpack .../194-python3-setuptools_68.1.2-2~jammy3_all.deb ...\n",
            "Unpacking python3-setuptools (68.1.2-2~jammy3) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../195-python3-wheel_0.37.1-2ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../196-python3-pip_22.0.2+dfsg-1ubuntu0.5_all.deb ...\n",
            "Unpacking python3-pip (22.0.2+dfsg-1ubuntu0.5) ...\n",
            "Selecting previously unselected package ubuntu-fan.\n",
            "Preparing to unpack .../197-ubuntu-fan_0.12.16_all.deb ...\n",
            "Unpacking ubuntu-fan (0.12.16) ...\n",
            "Selecting previously unselected package x265.\n",
            "Preparing to unpack .../198-x265_3.5-2_amd64.deb ...\n",
            "Unpacking x265 (3.5-2) ...\n",
            "Selecting previously unselected package node-abbrev.\n",
            "Preparing to unpack .../199-node-abbrev_1.1.1+~1.1.2-1_all.deb ...\n",
            "Unpacking node-abbrev (1.1.1+~1.1.2-1) ...\n",
            "Selecting previously unselected package node-archy.\n",
            "Preparing to unpack .../200-node-archy_1.0.0-4_all.deb ...\n",
            "Unpacking node-archy (1.0.0-4) ...\n",
            "Selecting previously unselected package node-chalk.\n",
            "Preparing to unpack .../201-node-chalk_4.1.2-1_all.deb ...\n",
            "Unpacking node-chalk (4.1.2-1) ...\n",
            "Selecting previously unselected package node-cli-table.\n",
            "Preparing to unpack .../202-node-cli-table_0.3.11+~cs0.13.3-1_all.deb ...\n",
            "Unpacking node-cli-table (0.3.11+~cs0.13.3-1) ...\n",
            "Selecting previously unselected package node-depd.\n",
            "Preparing to unpack .../203-node-depd_2.0.0-2_all.deb ...\n",
            "Unpacking node-depd (2.0.0-2) ...\n",
            "Selecting previously unselected package node-nopt.\n",
            "Preparing to unpack .../204-node-nopt_5.0.0-2_all.deb ...\n",
            "Unpacking node-nopt (5.0.0-2) ...\n",
            "Selecting previously unselected package node-npmlog.\n",
            "Preparing to unpack .../205-node-npmlog_6.0.1+~4.1.4-1_all.deb ...\n",
            "Unpacking node-npmlog (6.0.1+~4.1.4-1) ...\n",
            "Selecting previously unselected package node-tar.\n",
            "Preparing to unpack .../206-node-tar_6.1.11+ds1+~cs6.0.6-1_all.deb ...\n",
            "Unpacking node-tar (6.1.11+ds1+~cs6.0.6-1) ...\n",
            "Selecting previously unselected package node-which.\n",
            "Preparing to unpack .../207-node-which_2.0.2+~cs1.3.2-2_all.deb ...\n",
            "Unpacking node-which (2.0.2+~cs1.3.2-2) ...\n",
            "Selecting previously unselected package node-gyp.\n",
            "Preparing to unpack .../208-node-gyp_8.4.1-1_all.deb ...\n",
            "Unpacking node-gyp (8.4.1-1) ...\n",
            "Selecting previously unselected package node-ini.\n",
            "Preparing to unpack .../209-node-ini_2.0.1-1_all.deb ...\n",
            "Unpacking node-ini (2.0.1-1) ...\n",
            "Selecting previously unselected package node-negotiator.\n",
            "Preparing to unpack .../210-node-negotiator_0.6.2+~0.6.1-1_all.deb ...\n",
            "Unpacking node-negotiator (0.6.2+~0.6.1-1) ...\n",
            "Selecting previously unselected package node-resolve.\n",
            "Preparing to unpack .../211-node-resolve_1.20.0+~cs5.27.9-1_all.deb ...\n",
            "Unpacking node-resolve (1.20.0+~cs5.27.9-1) ...\n",
            "Selecting previously unselected package node-normalize-package-data.\n",
            "Preparing to unpack .../212-node-normalize-package-data_3.0.3+~2.4.1-1_all.deb ...\n",
            "Unpacking node-normalize-package-data (3.0.3+~2.4.1-1) ...\n",
            "Selecting previously unselected package node-read-package-json.\n",
            "Preparing to unpack .../213-node-read-package-json_4.1.1-1_all.deb ...\n",
            "Unpacking node-read-package-json (4.1.1-1) ...\n",
            "Selecting previously unselected package node-tap.\n",
            "Preparing to unpack .../214-node-tap_12.0.1+ds-4_all.deb ...\n",
            "Unpacking node-tap (12.0.1+ds-4) ...\n",
            "Selecting previously unselected package npm.\n",
            "Preparing to unpack .../215-npm_8.5.1~ds-1_all.deb ...\n",
            "Unpacking npm (8.5.1~ds-1) ...\n",
            "Setting up python3-pkg-resources (68.1.2-2~jammy3) ...\n",
            "Setting up node-delayed-stream (1.0.0-5) ...\n",
            "Setting up javascript-common (11+nmu1) ...\n",
            "Setting up libuv1-dev:amd64 (1.43.0-1ubuntu0.1) ...\n",
            "Setting up node-fs.realpath (1.0.0-2) ...\n",
            "Setting up node-diff (5.0.0~dfsg+~5.0.1-3) ...\n",
            "Setting up node-abbrev (1.1.1+~1.1.2-1) ...\n",
            "Setting up libjs-sprintf-js (1.1.2+ds1+~1.1.2-1) ...\n",
            "Setting up node-yallist (4.0.0+~4.0.1-1) ...\n",
            "Setting up libjs-inherits (2.0.4-4) ...\n",
            "Setting up node-p-cancelable (2.1.1-1) ...\n",
            "Setting up python3-setuptools (68.1.2-2~jammy3) ...\n",
            "Setting up node-ansi-regex (5.0.1-1) ...\n",
            "Setting up node-slash (3.0.0-2) ...\n",
            "Setting up node-util-deprecate (1.0.2-3) ...\n",
            "Setting up node-retry (0.13.1+~0.12.1-1) ...\n",
            "Setting up node-arrify (2.0.1-2) ...\n",
            "Setting up libip6tc2:amd64 (1.8.7-1ubuntu5.2) ...\n",
            "Setting up node-ansistyles (0.1.3-5) ...\n",
            "Setting up node-delegates (1.0.0-3) ...\n",
            "Setting up node-depd (2.0.0-2) ...\n",
            "Setting up node-isexe (2.0.0+~2.0.1-4) ...\n",
            "Setting up node-jsonparse (1.3.1-10) ...\n",
            "Setting up node-escape-string-regexp (4.0.0-2) ...\n",
            "Setting up netcat-openbsd (1.218-4ubuntu1) ...\n",
            "update-alternatives: using /bin/nc.openbsd to provide /bin/nc (nc) in auto mode\n",
            "Setting up libjs-source-map (0.7.0++dfsg2+really.0.6.1-9) ...\n",
            "Setting up node-negotiator (0.6.2+~0.6.1-1) ...\n",
            "Setting up x265 (3.5-2) ...\n",
            "Setting up node-color-name (1.1.4+~1.1.1-2) ...\n",
            "Setting up libnftnl11:amd64 (1.2.1-1build1) ...\n",
            "Setting up netcat (1.218-4ubuntu1) ...\n",
            "Setting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Setting up python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Setting up node-indent-string (4.0.0-2) ...\n",
            "Setting up libnode72:amd64 (12.22.9~dfsg-1ubuntu3.6) ...\n",
            "Setting up libmms0:amd64 (0.6.4-3) ...\n",
            "Setting up runc (1.2.5-0ubuntu1~22.04.1) ...\n",
            "Setting up node-function-bind (1.1.1+repacked+~1.0.3-1) ...\n",
            "Setting up node-p-map (4.0.0+~3.1.0+~3.0.1-1) ...\n",
            "Setting up node-iferr (1.0.2+~1.0.2-1) ...\n",
            "Setting up node-chownr (2.0.0-1) ...\n",
            "Setting up node-has-flag (4.0.0-2) ...\n",
            "Setting up dns-root-data (2024071801~ubuntu0.22.04.1) ...\n",
            "Setting up node-lodash-packages (4.17.21+dfsg+~cs8.31.198.20210220-5) ...\n",
            "Setting up libjs-psl (1.8.0+ds-6) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service ‚Üí /lib/systemd/system/apparmor.service.\n",
            "Setting up node-asap (2.0.6+~2.0.0-1) ...\n",
            "Setting up node-inherits (2.0.4-4) ...\n",
            "Setting up node-path-is-absolute (2.0.0-2) ...\n",
            "Setting up node-universalify (2.0.0-3) ...\n",
            "Setting up node-ini (2.0.1-1) ...\n",
            "Setting up node-safe-buffer (5.2.1+~cs2.1.2-2) ...\n",
            "Setting up node-promise-inflight (1.0.1+~1.0.0-1) ...\n",
            "Setting up node-json-parse-better-errors (1.0.2+~cs3.3.1-1) ...\n",
            "Setting up node-sprintf-js (1.1.2+ds1+~1.1.2-1) ...\n",
            "Setting up node-tmatch (5.0.0-4) ...\n",
            "Setting up node-err-code (2.0.3+dfsg-3) ...\n",
            "Setting up node-balanced-match (2.0.0-1) ...\n",
            "Setting up node-brace-expansion (2.0.1-1) ...\n",
            "Setting up libnotify4:amd64 (0.7.9-3ubuntu5.22.04.1) ...\n",
            "Setting up node-spdx-exceptions (2.3.0-2) ...\n",
            "Setting up node-set-blocking (2.0.0-2) ...\n",
            "Setting up libzen0v5:amd64 (0.4.39-1) ...\n",
            "Setting up libtinyxml2-9:amd64 (9.0.0+dfsg-3) ...\n",
            "Setting up node-npm-bundled (1.1.2-1) ...\n",
            "Setting up node-signal-exit (3.0.6+~3.0.1-1) ...\n",
            "Setting up python3-pip (22.0.2+dfsg-1ubuntu0.5) ...\n",
            "Setting up node-source-map (0.7.0++dfsg2+really.0.6.1-9) ...\n",
            "Setting up node-wrappy (1.0.2-2) ...\n",
            "Setting up bridge-utils (1.7-1ubuntu3) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up node-text-table (0.2.0-4) ...\n",
            "Setting up node-asynckit (0.4.0-4) ...\n",
            "Setting up node-ip (1.1.5+~1.1.0-1) ...\n",
            "Setting up node-quick-lru (5.1.1-1) ...\n",
            "Setting up libnotify-bin (0.7.9-3ubuntu5.22.04.1) ...\n",
            "Setting up node-mute-stream (0.0.8+~0.0.1-1) ...\n",
            "Setting up node-mimic-response (3.1.0-7) ...\n",
            "Setting up node-commander (9.0.0-2) ...\n",
            "Setting up node-whatwg-fetch (3.6.2-5) ...\n",
            "Setting up libjs-typedarray-to-buffer (4.0.0-2) ...\n",
            "Setting up libjs-highlight.js (9.18.5+dfsg1-1) ...\n",
            "Setting up libnfnetlink0:amd64 (1.0.1-3build3) ...\n",
            "Setting up node-clean-yaml-object (0.1.0-5) ...\n",
            "Setting up node-ip-regex (4.3.0+~4.1.1-1) ...\n",
            "Setting up node-stealthy-require (1.1.1-5) ...\n",
            "Setting up node-spdx-license-ids (3.0.11-1) ...\n",
            "Setting up node-string-decoder (1.3.0-5) ...\n",
            "Setting up netbase (6.3) ...\n",
            "Setting up node-time-stamp (2.2.0-1) ...\n",
            "Setting up libjs-events (3.3.0+~3.0.0-2) ...\n",
            "Setting up node-core-util-is (1.0.3-1) ...\n",
            "Setting up node-minimatch (3.1.1+~3.0.5-1) ...\n",
            "Setting up node-imurmurhash (0.1.4+dfsg+~0.1.1-1) ...\n",
            "Setting up node-foreground-child (2.0.0-3) ...\n",
            "Setting up node-read (1.0.7-3) ...\n",
            "Setting up containerd (1.7.27-0ubuntu1~22.04.1) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service ‚Üí /lib/systemd/system/containerd.service.\n",
            "Setting up libmediainfo0v5:amd64 (21.09+dfsg-4) ...\n",
            "Setting up mediainfo (22.03-1) ...\n",
            "Setting up node-is-buffer (2.0.5-2) ...\n",
            "Setting up node-color-convert (2.0.1-1) ...\n",
            "Setting up node-webidl-conversions (7.0.0~1.1.0+~cs15.1.20180823-2) ...\n",
            "Setting up node-isarray (2.0.5-3) ...\n",
            "Setting up node-osenv (0.1.5+~0.1.0-1) ...\n",
            "Setting up node-is-plain-obj (3.0.0-2) ...\n",
            "Setting up libjs-is-typedarray (1.0.0-4) ...\n",
            "Setting up node-lowercase-keys (2.0.0-2) ...\n",
            "Setting up node-decompress-response (6.0.0-2) ...\n",
            "Setting up node-process-nextick-args (2.0.1-2) ...\n",
            "Setting up node-has-unicode (2.0.1-4) ...\n",
            "Setting up gyp (0.1+20210831gitd6c5dd5-5) ...\n",
            "Setting up node-readable-stream (3.6.0+~cs3.0.0-1) ...\n",
            "Setting up node-lru-cache (6.0.0+~5.1.1-1) ...\n",
            "Setting up node-promise-retry (2.0.1-2) ...\n",
            "Setting up node-supports-color (8.1.1+~8.1.1-1) ...\n",
            "Setting up node-once (1.4.0-4) ...\n",
            "Setting up libnode-dev (12.22.9~dfsg-1ubuntu3.6) ...\n",
            "Setting up node-resolve (1.20.0+~cs5.27.9-1) ...\n",
            "Setting up node-are-we-there-yet (3.0.0+~1.1.0-1) ...\n",
            "Setting up node-kind-of (6.0.3+dfsg-2) ...\n",
            "Setting up node-growl (1.10.5-4) ...\n",
            "Setting up nodejs (12.22.9~dfsg-1ubuntu3.6) ...\n",
            "update-alternatives: using /usr/bin/nodejs to provide /usr/bin/js (js) in auto mode\n",
            "Setting up node-minimist (1.2.5+~cs5.3.2-1) ...\n",
            "Setting up node-abab (2.0.5-2) ...\n",
            "Setting up node-argparse (2.0.1-2) ...\n",
            "Setting up node-fancy-log (1.3.3+~cs1.3.1-2) ...\n",
            "Setting up node-clone (2.1.2-3) ...\n",
            "Setting up node-promzard (0.3.0-2) ...\n",
            "Setting up node-mime (3.0.0+dfsg+~cs3.96.1-1) ...\n",
            "Setting up node-source-map-support (0.5.21+ds+~0.5.4-1) ...\n",
            "Setting up node-iconv-lite (0.6.3-2) ...\n",
            "Setting up node-combined-stream (1.0.8+~1.0.3-1) ...\n",
            "Setting up node-unique-filename (1.1.1+ds-1) ...\n",
            "Setting up node-ansi-styles (4.3.0+~4.2.0-1) ...\n",
            "Setting up node-mime-types (2.1.33-1) ...\n",
            "Setting up node-lcov-parse (1.0.0+20170612git80d039574ed9-5) ...\n",
            "Setting up node-cssom (0.4.4-3) ...\n",
            "Setting up node-form-data (3.0.1-1) ...\n",
            "Setting up node-strip-ansi (6.0.1-1) ...\n",
            "Setting up node-chalk (4.1.2-1) ...\n",
            "Setting up node-spdx-expression-parse (3.0.1+~3.0.1-1) ...\n",
            "Setting up node-which (2.0.2+~cs1.3.2-2) ...\n",
            "Setting up nodejs-doc (12.22.9~dfsg-1ubuntu3.6) ...\n",
            "Setting up node-punycode (2.1.1-5) ...\n",
            "Setting up node-defaults (1.0.3+~1.0.3-1) ...\n",
            "Setting up node-is-typedarray (1.0.0-4) ...\n",
            "Setting up node-graceful-fs (4.2.4+repack-1) ...\n",
            "Setting up libnetfilter-conntrack3:amd64 (1.0.9-1) ...\n",
            "Setting up node-inflight (1.0.6-2) ...\n",
            "Setting up node-hosted-git-info (4.0.2-1) ...\n",
            "Setting up node-aproba (2.0.0-2) ...\n",
            "Setting up node-esprima (4.0.1+ds+~4.0.3-2) ...\n",
            "Setting up node-mkdirp (1.0.4+~1.0.2-1) ...\n",
            "Setting up node-run-queue (2.0.0-2) ...\n",
            "Setting up node-opener (1.5.2+~1.4.0-1) ...\n",
            "Setting up node-archy (1.0.0-4) ...\n",
            "Setting up node-encoding (0.1.13-2) ...\n",
            "Setting up node-js-yaml (4.1.0+dfsg+~4.0.5-6) ...\n",
            "Setting up node-nopt (5.0.0-2) ...\n",
            "Setting up node-slice-ansi (5.0.0+~cs9.0.0-4) ...\n",
            "Setting up node-ms (2.1.3+~cs0.7.31-2) ...\n",
            "Setting up node-semver (7.3.5+~7.3.8-1) ...\n",
            "Setting up node-fs-write-stream-atomic (1.0.10-5) ...\n",
            "Setting up node-builtins (4.0.0-1) ...\n",
            "Setting up node-colors (1.4.0-3) ...\n",
            "Setting up node-log-driver (1.2.7+git+20180219+bba1761737-7) ...\n",
            "Setting up node-ssri (8.0.1-2) ...\n",
            "Setting up node-object-assign (4.1.1-6) ...\n",
            "Setting up node-end-of-stream (1.4.4+~1.4.1-1) ...\n",
            "Setting up node-pump (3.0.0-5) ...\n",
            "Setting up node-psl (1.8.0+ds-6) ...\n",
            "Setting up iptables (1.8.7-1ubuntu5.2) ...\n",
            "update-alternatives: using /usr/sbin/iptables-legacy to provide /usr/sbin/iptables (iptables) in auto mode\n",
            "update-alternatives: using /usr/sbin/ip6tables-legacy to provide /usr/sbin/ip6tables (ip6tables) in auto mode\n",
            "update-alternatives: using /usr/sbin/iptables-nft to provide /usr/sbin/iptables (iptables) in auto mode\n",
            "update-alternatives: using /usr/sbin/ip6tables-nft to provide /usr/sbin/ip6tables (ip6tables) in auto mode\n",
            "update-alternatives: using /usr/sbin/arptables-nft to provide /usr/sbin/arptables (arptables) in auto mode\n",
            "update-alternatives: using /usr/sbin/ebtables-nft to provide /usr/sbin/ebtables (ebtables) in auto mode\n",
            "Setting up node-stack-utils (2.0.5+~2.0.1-1) ...\n",
            "Setting up node-json-buffer (3.0.1-1) ...\n",
            "Setting up node-console-control-strings (1.1.0-2) ...\n",
            "Setting up docker.io (27.5.1-0ubuntu3~22.04.2) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Adding group `docker' (GID 107) ...\n",
            "Done.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/docker.service ‚Üí /lib/systemd/system/docker.service.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/docker.socket ‚Üí /lib/systemd/system/docker.socket.\n",
            "invoke-rc.d: unknown initscript, /etc/init.d/docker not found.\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "Setting up node-debug (4.3.2+~cs4.1.7-1) ...\n",
            "Setting up node-events (3.3.0+~3.0.0-2) ...\n",
            "Setting up dnsmasq-base (2.90-0ubuntu0.22.04.1) ...\n",
            "Setting up node-agent-base (6.0.2+~cs5.4.2-1) ...\n",
            "Setting up node-validate-npm-package-name (3.0.0-4) ...\n",
            "Setting up node-wcwidth.js (1.0.2-1) ...\n",
            "Setting up node-cssstyle (2.3.0-2) ...\n",
            "Setting up node-spdx-correct (3.1.1-2) ...\n",
            "Setting up node-glob (7.2.1+~cs7.6.15-1) ...\n",
            "Setting up node-get-stream (6.0.1-1) ...\n",
            "Setting up node-got (11.8.3+~cs58.7.37-1) ...\n",
            "Setting up node-typedarray-to-buffer (4.0.0-2) ...\n",
            "Setting up node-tap-parser (7.0.0+ds1-6) ...\n",
            "Setting up node-minipass (3.1.6+~cs8.7.18-1) ...\n",
            "Setting up node-tough-cookie (4.0.0-2) ...\n",
            "Setting up node-npm-package-arg (8.1.5-1) ...\n",
            "Setting up node-https-proxy-agent (5.0.0+~cs8.0.0-3) ...\n",
            "Setting up node-rimraf (3.0.2-1) ...\n",
            "Setting up node-string-width (4.2.3+~cs13.2.3-1) ...\n",
            "Setting up node-validate-npm-package-license (3.0.4-2) ...\n",
            "Setting up ubuntu-fan (0.12.16) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/ubuntu-fan.service ‚Üí /lib/systemd/system/ubuntu-fan.service.\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up node-write-file-atomic (3.0.3+~3.0.2-1) ...\n",
            "Setting up node-columnify (1.5.4+~1.5.1-1) ...\n",
            "Setting up node-copy-concurrently (1.0.5-8) ...\n",
            "Setting up node-move-concurrently (1.0.1-4) ...\n",
            "Setting up node-tap-mocha-reporter (3.0.7+ds-2) ...\n",
            "Setting up node-normalize-package-data (3.0.3+~2.4.1-1) ...\n",
            "Setting up node-ws (8.5.0+~cs13.3.3-2) ...\n",
            "Setting up node-cli-table (0.3.11+~cs0.13.3-1) ...\n",
            "Setting up node-jsdom (19.0.0+~cs90.11.27-1) ...\n",
            "Setting up node-tar (6.1.11+ds1+~cs6.0.6-1) ...\n",
            "Setting up node-wide-align (1.1.3-4) ...\n",
            "Setting up node-tap (12.0.1+ds-4) ...\n",
            "Setting up node-cacache (15.0.5+~cs13.9.21-3) ...\n",
            "Setting up node-read-package-json (4.1.1-1) ...\n",
            "Setting up node-fetch (2.6.7+~2.5.12-1) ...\n",
            "Setting up node-gauge (4.0.2-1) ...\n",
            "Setting up node-npmlog (6.0.1+~4.1.4-1) ...\n",
            "Setting up node-coveralls (3.1.1-1) ...\n",
            "Setting up node-gyp (8.4.1-1) ...\n",
            "Setting up npm (8.5.1~ds-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "x265 is already the newest version (3.5-2).\n",
            "The following additional packages will be installed:\n",
            "  libigdgmm12 libnvidia-compute-470\n",
            "Suggested packages:\n",
            "  nvidia-driver-470\n",
            "The following NEW packages will be installed:\n",
            "  intel-media-va-driver-non-free libigdgmm12 libnvidia-compute-470\n",
            "  nvidia-utils-470\n",
            "0 upgraded, 4 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 40.8 MB of archives.\n",
            "After this operation, 170 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libigdgmm12 amd64 22.1.2+ds1-1 [139 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 intel-media-va-driver-non-free amd64 22.3.1+ds1-1ubuntu0.1 [5,611 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 libnvidia-compute-470 amd64 470.256.02-0ubuntu0.22.04.1 [34.7 MB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 nvidia-utils-470 amd64 470.256.02-0ubuntu0.22.04.1 [408 kB]\n",
            "Fetched 40.8 MB in 2s (21.3 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 4.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libigdgmm12:amd64.\n",
            "(Reading database ... 135499 files and directories currently installed.)\n",
            "Preparing to unpack .../libigdgmm12_22.1.2+ds1-1_amd64.deb ...\n",
            "Unpacking libigdgmm12:amd64 (22.1.2+ds1-1) ...\n",
            "Selecting previously unselected package intel-media-va-driver-non-free:amd64.\n",
            "Preparing to unpack .../intel-media-va-driver-non-free_22.3.1+ds1-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking intel-media-va-driver-non-free:amd64 (22.3.1+ds1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libnvidia-compute-470:amd64.\n",
            "Preparing to unpack .../libnvidia-compute-470_470.256.02-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libnvidia-compute-470:amd64 (470.256.02-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package nvidia-utils-470.\n",
            "Preparing to unpack .../nvidia-utils-470_470.256.02-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking nvidia-utils-470 (470.256.02-0ubuntu0.22.04.1) ...\n",
            "Setting up libnvidia-compute-470:amd64 (470.256.02-0ubuntu0.22.04.1) ...\n",
            "Setting up nvidia-utils-470 (470.256.02-0ubuntu0.22.04.1) ...\n",
            "Setting up libigdgmm12:amd64 (22.1.2+ds1-1) ...\n",
            "Setting up intel-media-va-driver-non-free:amd64 (22.3.1+ds1-1ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.72.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Collecting kaleido\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.15.3)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.2.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.6.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
            "Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kaleido\n",
            "Successfully installed kaleido-0.2.1\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m \u001b[0m\u001b[35mcode\u001b[0m E404\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m \u001b[0m\u001b[35m404\u001b[0m Not Found - GET https://registry.npmjs.org/@shaka-project%2fpackager - Not found\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m \u001b[0m\u001b[35m404\u001b[0m \n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m \u001b[0m\u001b[35m404\u001b[0m  '@shaka-project/packager@*' is not in this registry.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m \u001b[0m\u001b[35m404\u001b[0m You should bug the author to publish it (or use the name yourself!)\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m \u001b[0m\u001b[35m404\u001b[0m \n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m \u001b[0m\u001b[35m404\u001b[0m Note that you can also install from a\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m \u001b[0m\u001b[35m404\u001b[0m tarball, folder, http url, or git url.\n",
            "\u001b[0m\n",
            "\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m A complete log of this run can be found in:\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m     /root/.npm/_logs/2025-06-06T17_50_04_837Z-debug-0.log\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "\u001b[1;31mE: \u001b[0mUnable to locate package mp4box\u001b[0m\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement xml.etree.ElementTree (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for xml.etree.ElementTree\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement concurrent.futures (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for concurrent.futures\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement collections (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for collections\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Ubuntu/Debian complete setup\n",
        "!sudo apt update\n",
        "!sudo apt install -y ffmpeg python3 python3-pip nodejs npm docker.io git x265 mediainfo\n",
        "\n",
        "# Install x265 with hardware acceleration\n",
        "!sudo apt install -y x265 intel-media-va-driver-non-free nvidia-utils-470\n",
        "\n",
        "# Install Python dependencies\n",
        "!pip3 install numpy opencv-python matplotlib pandas scikit-learn tensorflow torch torchvision\n",
        "!pip install seaborn plotly kaleido scikit-image\n",
        "\n",
        "\n",
        "# Install Node.js dependencies\n",
        "!npm install -g http-server pm2 @shaka-project/packager\n",
        "\n",
        "# Install additional tools\n",
        "!sudo apt install -y mediainfo gpac mp4box\n",
        "# Additional packages for your advanced features\n",
        "!pip install xml.etree.ElementTree  # For DASH manifest enhancement\n",
        "!pip install concurrent.futures     # For parallel processing\n",
        "!pip install collections           # For deque in bandwidth predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NN75hU4tJk24",
        "outputId": "305dc3e1-a7fb-4aee-92f2-780430d07814",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-05 19:44:52--  https://github.com/shaka-project/shaka-packager/releases/download/v2.6.1/packager-linux-x64\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/19000603/209d95c0-d559-4d9e-a214-e209ea31317f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250605%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250605T194452Z&X-Amz-Expires=300&X-Amz-Signature=9c8352bff46312ce74600a576b7ab857ea23ee9374b4171d6c321afe47ca0ecc&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dpackager-linux-x64&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-06-05 19:44:52--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/19000603/209d95c0-d559-4d9e-a214-e209ea31317f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250605%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250605T194452Z&X-Amz-Expires=300&X-Amz-Signature=9c8352bff46312ce74600a576b7ab857ea23ee9374b4171d6c321afe47ca0ecc&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dpackager-linux-x64&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6907536 (6.6M) [application/octet-stream]\n",
            "Saving to: ‚Äòpackager‚Äô\n",
            "\n",
            "packager            100%[===================>]   6.59M  42.5MB/s    in 0.2s    \n",
            "\n",
            "2025-06-05 19:44:52 (42.5 MB/s) - ‚Äòpackager‚Äô saved [6907536/6907536]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/shaka-project/shaka-packager/releases/download/v2.6.1/packager-linux-x64 -O packager\n",
        "!chmod +x packager\n",
        "!mv packager /usr/local/bin/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xnv6BlurJoHv",
        "outputId": "ebda8a91-14e7-48f5-9dd8-3bd8593fa017",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-05 19:45:07--  https://github.com/gpac/gpac/releases/download/v2.2.1/gpac-2.2.1-Ubuntu20.04-x64.tar.xz\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-06-05 19:45:08 ERROR 404: Not Found.\n",
            "\n",
            "tar: gpac-2.2.1-Ubuntu20.04-x64.tar.xz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "mv: missing destination file operand after 'gpac-2.2.1-Ubuntu20.04-x64/bin/gcc'\n",
            "Try 'mv --help' for more information.\n"
          ]
        }
      ],
      "source": [
        "# Download GPAC with MP4Box prebuilt binary\n",
        "!wget https://github.com/gpac/gpac/releases/download/v2.2.1/gpac-2.2.1-Ubuntu20.04-x64.tar.xz\n",
        "\n",
        "# Extract it\n",
        "!tar -xf gpac-2.2.1-Ubuntu20.04-x64.tar.xz\n",
        "\n",
        "# Move the binary to a known location\n",
        "!mv gpac-2.2.1-Ubuntu20.04-x64/bin/gcc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxzR2jyIJs5I"
      },
      "outputs": [],
      "source": [
        "!which MP4Box\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASklfYDgJyC5",
        "outputId": "eb9c53cc-aea6-491c-b93d-5850b4116c2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: MP4Box: command not found\n"
          ]
        }
      ],
      "source": [
        "!MP4Box -add output_h265.mp4 -dash 4000 -profile live -out manifest.mpd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSrM30zAM7i1"
      },
      "outputs": [],
      "source": [
        "!mkdir -p h265-streaming-research\n",
        "!cd h265-streaming-research\n",
        "\n",
        "# Create subfolders\n",
        "!mkdir -p h265-streaming-research/{src,content,encoded,packaged,web,logs,config,research,benchmarks}\n",
        "!mkdir -p h265-streaming-research/src/{encoding,packaging,streaming,client,analytics,ml-models}\n",
        "!mkdir -p h265-streaming-research/web/{player,assets,css,js,components}\n",
        "!mkdir -p h265-streaming-research/research/{data,plots,reports}\n",
        "!mkdir -p h265-streaming-research/benchmarks/{quality-metrics,performance}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z00UQJwfoXKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create project structure\n",
        "project_dirs = [\n",
        "    'src/encoding', 'src/packaging', 'src/streaming', 'src/client',\n",
        "    'src/analytics', 'src/ml-models', 'content', 'encoded', 'packaged',\n",
        "    'web', 'logs', 'config', 'research', 'benchmarks/quality-metrics',\n",
        "    'benchmarks/performance'\n",
        "]\n",
        "\n",
        "for dir_path in project_dirs:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Environment setup complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82nlhgFCoWt3",
        "outputId": "55b87548-fa32-4791-80ad-fb5ba8f4094f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Environment setup complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# State management functions\n",
        "\n",
        "def save_state(data, filename='research_state.pkl'):\n",
        "    \"\"\"Save current research state\"\"\"\n",
        "    # Ensure directory exists\n",
        "    os.makedirs('/content/drive/MyDrive/Research/OurCode', exist_ok=True)\n",
        "\n",
        "    state = {\n",
        "        'data': data,\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'session_info': f'Session on {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
        "    }\n",
        "\n",
        "    filepath = f'/content/drive/MyDrive/Research/OurCode/{filename}'\n",
        "    try:\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump(state, f)\n",
        "        print(f\"‚úÖ State saved successfully at {state['timestamp']}\")\n",
        "        print(f\"üìÅ File location: {filepath}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error saving state: {e}\")\n",
        "\n",
        "def load_state(filename='research_state.pkl'):\n",
        "    \"\"\"Load previous research state\"\"\"\n",
        "    filepath = f'/content/drive/MyDrive/Research/OurCode/{filename}'\n",
        "    try:\n",
        "        with open(filepath, 'rb') as f:\n",
        "            state = pickle.load(f)\n",
        "        print(f\"‚úÖ State loaded from {state['timestamp']}\")\n",
        "        return state['data']\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ÑπÔ∏è  No previous state found, starting fresh\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading state: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"üîß State management system ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6E2sPUUpis5",
        "outputId": "e96ed064-9c2a-40d7-b260-0f0bb6a89564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß State management system ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gRSrUmW_XRDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ================================\n",
        "# DEBUGGED BANDWIDTH PREDICTOR FOR H.265 RESEARCH\n",
        "# Fixed all import dependencies and added error handling\n",
        "# ================================\n",
        "\n",
        "# ================================\n",
        "# CELL 1: IMPORTS AND SETUP\n",
        "# ================================\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "import json\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from collections import deque\n",
        "import time\n",
        "import threading\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "# Install required packages\n",
        "print(\"üì¶ Installing required packages...\")\n",
        "os.system('pip install tensorflow scikit-learn matplotlib pandas plotly -qq')\n",
        "\n",
        "# Create project structure\n",
        "project_dirs = [\n",
        "    'src/ml-models', 'research', 'logs', 'experiments'\n",
        "]\n",
        "for dir_path in project_dirs:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-CYQnU2YYeX",
        "outputId": "48e84314-008e-4a40-8e3c-40fec821c39b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Installing required packages...\n",
            "‚úÖ Setup complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# CELL 2: DEBUGGED BANDWIDTH PREDICTOR\n",
        "# ================================\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "class BandwidthPredictor:\n",
        "    \"\"\"\n",
        "    LSTM-based bandwidth predictor for adaptive streaming\n",
        "    Research Contribution: Neural network approach to bandwidth prediction\n",
        "\n",
        "    DEBUG FIXES APPLIED:\n",
        "    - Added missing imports\n",
        "    - Fixed deque import issue\n",
        "    - Added comprehensive error handling\n",
        "    - Enhanced model architecture\n",
        "    - Added model validation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sequence_length=10):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.history = deque(maxlen=sequence_length)  # Fixed: deque imported above\n",
        "        self.is_trained = False\n",
        "        self.prediction_accuracy = deque(maxlen=50)\n",
        "        self.model_path = 'src/ml-models/bandwidth_model.h5'\n",
        "        self.scaler_path = 'src/ml-models/scaler.pkl'\n",
        "\n",
        "        # Debug: Ensure directories exist\n",
        "        os.makedirs('src/ml-models', exist_ok=True)\n",
        "        os.makedirs('research', exist_ok=True)\n",
        "\n",
        "        print(\"ü§ñ BandwidthPredictor initialized\")\n",
        "\n",
        "    def build_lstm_model(self):\n",
        "        \"\"\"Build LSTM model architecture for bandwidth prediction\"\"\"\n",
        "        print(\"üß† Building LSTM bandwidth prediction model...\")\n",
        "\n",
        "        try:\n",
        "            model = models.Sequential([\n",
        "                # First LSTM layer with return sequences\n",
        "                layers.LSTM(64, return_sequences=True,\n",
        "                           input_shape=(self.sequence_length, 4),\n",
        "                           dropout=0.1, recurrent_dropout=0.1),\n",
        "\n",
        "                # Second LSTM layer\n",
        "                layers.LSTM(32, return_sequences=False,\n",
        "                           dropout=0.1, recurrent_dropout=0.1),\n",
        "\n",
        "                # Dense layers for final prediction\n",
        "                layers.Dense(16, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(8, activation='relu'),\n",
        "                layers.Dense(1, activation='linear')  # Single bandwidth prediction\n",
        "            ])\n",
        "\n",
        "            # Compile with appropriate optimizer and loss\n",
        "            model.compile(\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                loss='mse',\n",
        "                metrics=['mae', 'mape']\n",
        "            )\n",
        "\n",
        "            print(\"‚úÖ LSTM model architecture created\")\n",
        "            print(f\"üìä Model parameters: {model.count_params():,}\")\n",
        "            return model\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error building LSTM model: {e}\")\n",
        "            return None\n",
        "\n",
        "    def generate_synthetic_training_data(self, num_samples=1000):\n",
        "        \"\"\"\n",
        "        Generate synthetic bandwidth data for training\n",
        "        Simulates realistic network conditions\n",
        "        \"\"\"\n",
        "        print(f\"üìä Generating {num_samples} synthetic training samples...\")\n",
        "\n",
        "        try:\n",
        "            np.random.seed(42)  # For reproducibility\n",
        "            training_data = []\n",
        "\n",
        "            # Simulate different network scenarios\n",
        "            scenarios = [\n",
        "                {'base_bw': 1000000, 'variance': 0.3, 'trend': 0.02, 'name': 'Poor'},\n",
        "                {'base_bw': 3000000, 'variance': 0.2, 'trend': 0.01, 'name': 'Medium'},\n",
        "                {'base_bw': 8000000, 'variance': 0.15, 'trend': -0.005, 'name': 'Good'},\n",
        "                {'base_bw': 15000000, 'variance': 0.1, 'trend': 0.0, 'name': 'Excellent'},\n",
        "            ]\n",
        "\n",
        "            for i in range(num_samples):\n",
        "                scenario = scenarios[i % len(scenarios)]\n",
        "\n",
        "                # Generate bandwidth with trend and noise\n",
        "                base_bandwidth = scenario['base_bw']\n",
        "                variance = scenario['variance']\n",
        "                trend = scenario['trend']\n",
        "\n",
        "                # Add time-based patterns (daily usage patterns)\n",
        "                time_factor = np.sin(2 * np.pi * i / 100) * 0.2 + 1\n",
        "\n",
        "                bandwidth = base_bandwidth * time_factor * (1 + np.random.normal(0, variance))\n",
        "                bandwidth = max(100000, bandwidth)  # Minimum 100 Kbps\n",
        "\n",
        "                # Generate correlated RTT (higher bandwidth usually means lower RTT)\n",
        "                base_rtt = 200 - (bandwidth / 100000)  # Inverse relationship\n",
        "                rtt = max(5, base_rtt + np.random.normal(0, 20))\n",
        "\n",
        "                # Buffer level simulation\n",
        "                buffer_level = np.random.uniform(0, 30)\n",
        "\n",
        "                # Timestamp\n",
        "                timestamp = time.time() + i\n",
        "\n",
        "                training_data.append({\n",
        "                    'bandwidth': bandwidth,\n",
        "                    'rtt': rtt,\n",
        "                    'buffer_level': buffer_level,\n",
        "                    'timestamp': timestamp,\n",
        "                    'scenario': scenario['name']\n",
        "                })\n",
        "\n",
        "            print(\"‚úÖ Synthetic training data generated\")\n",
        "            print(f\"üìà Scenarios: {[s['name'] for s in scenarios]}\")\n",
        "            return training_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error generating training data: {e}\")\n",
        "            return []\n",
        "\n",
        "    def preprocess_training_data(self, bandwidth_history):\n",
        "        \"\"\"Preprocess bandwidth data into sequences for LSTM training\"\"\"\n",
        "        print(\"üîÑ Preprocessing training data...\")\n",
        "\n",
        "        try:\n",
        "            X, y = [], []\n",
        "\n",
        "            for i in range(len(bandwidth_history) - self.sequence_length):\n",
        "                # Create sequence of features\n",
        "                sequence = bandwidth_history[i:i + self.sequence_length]\n",
        "                target = bandwidth_history[i + self.sequence_length]['bandwidth']\n",
        "\n",
        "                # Extract features for each timestep\n",
        "                features = []\n",
        "                for sample in sequence:\n",
        "                    features.append([\n",
        "                        sample['bandwidth'] / 1000000,  # Convert to Mbps\n",
        "                        sample['rtt'] / 100,            # Normalize RTT\n",
        "                        sample['buffer_level'] / 30,    # Normalize buffer\n",
        "                        (sample['timestamp'] % 86400) / 86400  # Time of day feature\n",
        "                    ])\n",
        "\n",
        "                X.append(features)\n",
        "                y.append(target / 1000000)  # Target in Mbps\n",
        "\n",
        "            X = np.array(X, dtype=np.float32)\n",
        "            y = np.array(y, dtype=np.float32)\n",
        "\n",
        "            print(f\"‚úÖ Preprocessed {len(X)} training sequences\")\n",
        "            print(f\"üìä Input shape: {X.shape}, Output shape: {y.shape}\")\n",
        "            return X, y\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error preprocessing data: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    def train_model(self, training_data=None, epochs=50, batch_size=32):\n",
        "        \"\"\"Train the bandwidth prediction model\"\"\"\n",
        "        print(\"üéØ Training bandwidth prediction model...\")\n",
        "\n",
        "        try:\n",
        "            if training_data is None:\n",
        "                training_data = self.generate_synthetic_training_data(2000)\n",
        "\n",
        "            if len(training_data) < self.sequence_length + 10:\n",
        "                print(\"‚ùå Insufficient training data\")\n",
        "                return False\n",
        "\n",
        "            # Preprocess data\n",
        "            X, y = self.preprocess_training_data(training_data)\n",
        "            if X is None or y is None:\n",
        "                return False\n",
        "\n",
        "            # Initialize and fit scaler\n",
        "            self.scaler = StandardScaler()\n",
        "            X_reshaped = X.reshape(-1, X.shape[-1])\n",
        "            X_scaled = self.scaler.fit_transform(X_reshaped)\n",
        "            X_scaled = X_scaled.reshape(X.shape)\n",
        "\n",
        "            # Build model\n",
        "            self.model = self.build_lstm_model()\n",
        "            if self.model is None:\n",
        "                return False\n",
        "\n",
        "            # Train/validation split\n",
        "            split_idx = int(len(X_scaled) * 0.8)\n",
        "            X_train, X_val = X_scaled[:split_idx], X_scaled[split_idx:]\n",
        "            y_train, y_val = y[:split_idx], y[split_idx:]\n",
        "\n",
        "            print(f\"üìä Training set: {X_train.shape}, Validation set: {X_val.shape}\")\n",
        "\n",
        "            # Train model with callbacks\n",
        "            callbacks = [\n",
        "                tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
        "                tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5),\n",
        "                tf.keras.callbacks.ModelCheckpoint(self.model_path, save_best_only=True)\n",
        "            ]\n",
        "\n",
        "            print(\"üî• Starting model training...\")\n",
        "            history = self.model.fit(\n",
        "                X_train, y_train,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                validation_data=(X_val, y_val),\n",
        "                callbacks=callbacks,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            # Evaluate model\n",
        "            val_loss, val_mae, val_mape = self.model.evaluate(X_val, y_val, verbose=0)\n",
        "            print(f\"‚úÖ Model trained - Val MAE: {val_mae:.4f}, Val MAPE: {val_mape:.2f}%\")\n",
        "\n",
        "            self.is_trained = True\n",
        "\n",
        "            # Save model and scaler\n",
        "            try:\n",
        "                self.model.save(self.model_path)\n",
        "                with open(self.scaler_path, 'wb') as f:\n",
        "                    pickle.dump(self.scaler, f)\n",
        "                print(\"üíæ Model and scaler saved successfully\")\n",
        "            except Exception as save_error:\n",
        "                print(f\"‚ö†Ô∏è Warning: Could not save model: {save_error}\")\n",
        "\n",
        "            # Plot training history\n",
        "            self.plot_training_history(history)\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error during training: {e}\")\n",
        "            return False\n",
        "\n",
        "    def plot_training_history(self, history):\n",
        "        \"\"\"Plot model training history\"\"\"\n",
        "        try:\n",
        "            plt.figure(figsize=(15, 5))\n",
        "\n",
        "            # Loss plot\n",
        "            plt.subplot(1, 3, 1)\n",
        "            plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "            plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "            plt.title('Model Loss')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('MSE Loss')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # MAE plot\n",
        "            plt.subplot(1, 3, 2)\n",
        "            plt.plot(history.history['mae'], label='Training MAE', linewidth=2)\n",
        "            plt.plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
        "            plt.title('Model MAE')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Mean Absolute Error')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # MAPE plot\n",
        "            plt.subplot(1, 3, 3)\n",
        "            plt.plot(history.history['mape'], label='Training MAPE', linewidth=2)\n",
        "            plt.plot(history.history['val_mape'], label='Validation MAPE', linewidth=2)\n",
        "            plt.title('Model MAPE')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Mean Absolute Percentage Error')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save plot\n",
        "            plot_path = 'research/bandwidth_model_training.png'\n",
        "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "            print(f\"üìä Training plots saved to {plot_path}\")\n",
        "            plt.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Warning: Could not plot training history: {e}\")\n",
        "\n",
        "    def load_trained_model(self):\n",
        "        \"\"\"Load previously trained model\"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.model_path) and os.path.exists(self.scaler_path):\n",
        "                self.model = tf.keras.models.load_model(self.model_path)\n",
        "                with open(self.scaler_path, 'rb') as f:\n",
        "                    self.scaler = pickle.load(f)\n",
        "                self.is_trained = True\n",
        "                print(\"‚úÖ Trained model loaded successfully\")\n",
        "                return True\n",
        "            else:\n",
        "                print(\"‚ùå No trained model found\")\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading model: {e}\")\n",
        "            return False\n",
        "\n",
        "    def predict_bandwidth(self, current_data):\n",
        "        \"\"\"Predict future bandwidth based on recent history\"\"\"\n",
        "        try:\n",
        "            if not self.is_trained or self.model is None:\n",
        "                return self.fallback_prediction(current_data)\n",
        "\n",
        "            # Add current data to history\n",
        "            self.history.append(current_data)\n",
        "\n",
        "            if len(self.history) < self.sequence_length:\n",
        "                return self.fallback_prediction(current_data)\n",
        "\n",
        "            # Prepare sequence for prediction\n",
        "            sequence = []\n",
        "            for sample in list(self.history):\n",
        "                sequence.append([\n",
        "                    sample['bandwidth'] / 1000000,\n",
        "                    sample['rtt'] / 100,\n",
        "                    sample['buffer_level'] / 30,\n",
        "                    (sample['timestamp'] % 86400) / 86400\n",
        "                ])\n",
        "\n",
        "            sequence = np.array([sequence], dtype=np.float32)\n",
        "\n",
        "            # Scale features\n",
        "            sequence_reshaped = sequence.reshape(-1, sequence.shape[-1])\n",
        "            sequence_scaled = self.scaler.transform(sequence_reshaped)\n",
        "            sequence_scaled = sequence_scaled.reshape(sequence.shape)\n",
        "\n",
        "            # Make prediction\n",
        "            prediction_mbps = self.model.predict(sequence_scaled, verbose=0)[0][0]\n",
        "            prediction_bps = prediction_mbps * 1000000\n",
        "\n",
        "            # Calculate confidence\n",
        "            confidence = self.calculate_confidence()\n",
        "\n",
        "            return {\n",
        "                'predicted_bandwidth': max(100000, prediction_bps),\n",
        "                'confidence': confidence,\n",
        "                'model_type': 'lstm',\n",
        "                'fallback_bandwidth': self.fallback_prediction(current_data)['bandwidth']\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Prediction error, using fallback: {e}\")\n",
        "            return self.fallback_prediction(current_data)\n",
        "\n",
        "    def fallback_prediction(self, current_data):\n",
        "        \"\"\"Simple fallback prediction when ML model isn't available\"\"\"\n",
        "        try:\n",
        "            if len(self.history) < 3:\n",
        "                return {\n",
        "                    'bandwidth': current_data['bandwidth'],\n",
        "                    'confidence': 0.3,\n",
        "                    'model_type': 'fallback'\n",
        "                }\n",
        "\n",
        "            # Simple trend-based prediction\n",
        "            recent_bandwidths = [h['bandwidth'] for h in list(self.history)[-3:]]\n",
        "            trend = (recent_bandwidths[-1] - recent_bandwidths[0]) / len(recent_bandwidths)\n",
        "            predicted = recent_bandwidths[-1] + trend\n",
        "\n",
        "            return {\n",
        "                'bandwidth': max(100000, predicted),\n",
        "                'confidence': 0.5,\n",
        "                'model_type': 'trend'\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'bandwidth': current_data['bandwidth'],\n",
        "                'confidence': 0.2,\n",
        "                'model_type': 'error_fallback'\n",
        "            }\n",
        "\n",
        "    def calculate_confidence(self):\n",
        "        \"\"\"Calculate prediction confidence based on recent history stability\"\"\"\n",
        "        try:\n",
        "            if len(self.history) < 5:\n",
        "                return 0.5\n",
        "\n",
        "            recent_values = [h['bandwidth'] for h in list(self.history)[-5:]]\n",
        "            variance = np.var(recent_values)\n",
        "            mean_value = np.mean(recent_values)\n",
        "\n",
        "            if mean_value == 0:\n",
        "                return 0.3\n",
        "\n",
        "            # Coefficient of variation as confidence measure\n",
        "            cv = np.sqrt(variance) / mean_value\n",
        "            confidence = max(0.1, 1.0 - cv)\n",
        "\n",
        "            return min(0.95, confidence)\n",
        "        except:\n",
        "            return 0.5\n",
        "\n",
        "    def update_prediction_accuracy(self, predicted, actual):\n",
        "        \"\"\"Update prediction accuracy tracking\"\"\"\n",
        "        try:\n",
        "            if predicted > 0:\n",
        "                accuracy = 1.0 - abs(predicted - actual) / max(predicted, actual)\n",
        "                self.prediction_accuracy.append(max(0, accuracy))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def get_model_stats(self):\n",
        "        \"\"\"Get model performance statistics\"\"\"\n",
        "        try:\n",
        "            if not self.prediction_accuracy:\n",
        "                return {'avg_accuracy': 0.5, 'prediction_count': 0}\n",
        "\n",
        "            return {\n",
        "                'avg_accuracy': np.mean(list(self.prediction_accuracy)),\n",
        "                'prediction_count': len(self.prediction_accuracy),\n",
        "                'is_trained': self.is_trained,\n",
        "                'model_type': 'lstm' if self.is_trained else 'fallback',\n",
        "                'history_length': len(self.history)\n",
        "            }\n",
        "        except:\n",
        "            return {'avg_accuracy': 0.5, 'prediction_count': 0}\n",
        "\n",
        "    def test_prediction_pipeline(self):\n",
        "        \"\"\"Test the complete prediction pipeline\"\"\"\n",
        "        print(\"üß™ Testing prediction pipeline...\")\n",
        "\n",
        "        try:\n",
        "            # Generate test data\n",
        "            test_data = self.generate_synthetic_training_data(100)\n",
        "\n",
        "            # Simulate predictions\n",
        "            correct_predictions = 0\n",
        "            total_predictions = 0\n",
        "\n",
        "            for i, data_point in enumerate(test_data[10:20]):  # Test 10 predictions\n",
        "                prediction = self.predict_bandwidth(data_point)\n",
        "\n",
        "                if prediction['model_type'] == 'lstm':\n",
        "                    # Compare with actual (for synthetic data we know the next value)\n",
        "                    if i + 1 < len(test_data):\n",
        "                        actual = test_data[i + 1]['bandwidth']\n",
        "                        predicted = prediction['predicted_bandwidth']\n",
        "\n",
        "                        # Update accuracy\n",
        "                        self.update_prediction_accuracy(predicted, actual)\n",
        "\n",
        "                        error_percentage = abs(predicted - actual) / actual * 100\n",
        "                        if error_percentage < 30:  # Less than 30% error is considered good\n",
        "                            correct_predictions += 1\n",
        "                        total_predictions += 1\n",
        "\n",
        "            accuracy = correct_predictions / total_predictions * 100 if total_predictions > 0 else 0\n",
        "            print(f\"‚úÖ Pipeline test complete: {accuracy:.1f}% accuracy\")\n",
        "\n",
        "            return accuracy > 50  # Return True if accuracy > 50%\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Pipeline test failed: {e}\")\n",
        "            return False\n"
      ],
      "metadata": {
        "id": "w7vLjcYhYjv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# CELL 3: DEMO AND TESTING\n",
        "# ================================\n",
        "\n",
        "def demo_bandwidth_predictor():\n",
        "    \"\"\"Demo the bandwidth predictor\"\"\"\n",
        "    print(\"üé¨ BANDWIDTH PREDICTOR DEMO\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Initialize predictor\n",
        "    predictor = BandwidthPredictor()\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\n1Ô∏è‚É£ Training ML model...\")\n",
        "    success = predictor.train_model(epochs=30)  # Reduced epochs for demo\n",
        "\n",
        "    if not success:\n",
        "        print(\"‚ùå Training failed, but continuing with fallback...\")\n",
        "\n",
        "    # Test pipeline\n",
        "    print(\"\\n2Ô∏è‚É£ Testing prediction pipeline...\")\n",
        "    pipeline_works = predictor.test_prediction_pipeline()\n",
        "\n",
        "    # Simulate real-time predictions\n",
        "    print(\"\\n3Ô∏è‚É£ Simulating real-time predictions...\")\n",
        "\n",
        "    for i in range(10):\n",
        "        # Simulate network data\n",
        "        network_data = {\n",
        "            'bandwidth': np.random.uniform(1000000, 10000000),\n",
        "            'rtt': np.random.uniform(20, 100),\n",
        "            'buffer_level': np.random.uniform(5, 25),\n",
        "            'timestamp': time.time() + i\n",
        "        }\n",
        "\n",
        "        # Get prediction\n",
        "        prediction = predictor.predict_bandwidth(network_data)\n",
        "\n",
        "        print(f\"Step {i+1:2d}: BW={network_data['bandwidth']/1000000:.1f}Mbps ‚Üí \"\n",
        "              f\"Predicted={prediction['predicted_bandwidth']/1000000:.1f}Mbps \"\n",
        "              f\"(conf={prediction['confidence']:.2f}, {prediction['model_type']})\")\n",
        "\n",
        "        time.sleep(0.5)  # Simulate real-time\n",
        "\n",
        "    # Show final stats\n",
        "    print(\"\\n4Ô∏è‚É£ Final Statistics:\")\n",
        "    stats = predictor.get_model_stats()\n",
        "    for key, value in stats.items():\n",
        "        print(f\"   {key}: {value}\")\n",
        "\n",
        "    return predictor\n"
      ],
      "metadata": {
        "id": "BnazfrEUZlPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #================================\n",
        "# CELL 4: RESEARCH INTEGRATION\n",
        "# ================================\n",
        "\n",
        "def save_predictor_state(predictor, filename='bandwidth_predictor_state.pkl'):\n",
        "    \"\"\"Save predictor state for research continuity\"\"\"\n",
        "    try:\n",
        "        state = {\n",
        "            'predictor': predictor,\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'stats': predictor.get_model_stats()\n",
        "        }\n",
        "\n",
        "        filepath = f'/content/drive/MyDrive/Research/OurCode/{filename}'\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump(state, f)\n",
        "\n",
        "        print(f\"üíæ Predictor state saved to {filepath}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error saving predictor state: {e}\")\n",
        "        return False\n",
        "\n",
        "def load_predictor_state(filename='bandwidth_predictor_state.pkl'):\n",
        "    \"\"\"Load predictor state for research continuity\"\"\"\n",
        "    try:\n",
        "        filepath = f'/content/drive/MyDrive/Research/OurCode/{filename}'\n",
        "        with open(filepath, 'rb') as f:\n",
        "            state = pickle.load(f)\n",
        "\n",
        "        print(f\"üìÇ Predictor state loaded from {state['timestamp']}\")\n",
        "        print(f\"üìä Previous stats: {state['stats']}\")\n",
        "        return state['predictor']\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading predictor state: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "U8Frwb94Yjai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NGQ9Vy_cYi_K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A95fP4wmX1AN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LchGnnmlXMRt"
      },
      "outputs": [],
      "source": [
        "!mkdir -p src/encoding/h265_advanced_encorder.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ4of2LBZdum"
      },
      "outputs": [],
      "source": [
        "encoder_code = '''\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class AdvancedH265Encoder:\n",
        "    def __init__(self, input_video, output_dir):\n",
        "        self.input_video = input_video\n",
        "        self.output_dir = output_dir\n",
        "        self.analysis_data = {}\n",
        "        self.roi_data = []\n",
        "\n",
        "    def analyze_content_advanced(self):\n",
        "        \"\"Advanced content analysis with scene detection and ROI mapping\"\"\n",
        "        cap = cv2.VideoCapture(self.input_video)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        scenes = []\n",
        "        roi_frames = []\n",
        "        complexity_data = []\n",
        "\n",
        "        prev_frame = None\n",
        "        scene_start = 0\n",
        "\n",
        "        print(\"Performing advanced content analysis...\")\n",
        "\n",
        "        for i in range(0, frame_count, max(1, frame_count // 200)):\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Scene detection\n",
        "            if prev_frame is not None:\n",
        "                hist_diff = cv2.compareHist(\n",
        "                    cv2.calcHist([prev_frame], [0], None, [256], [0, 256]),\n",
        "                    cv2.calcHist([gray], [0], None, [256], [0, 256]),\n",
        "                    cv2.HISTCMP_CORREL\n",
        "                )\n",
        "\n",
        "                if hist_diff < 0.7:  # Scene change threshold\n",
        "                    scenes.append({\n",
        "                        'start': scene_start,\n",
        "                        'end': i,\n",
        "                        'duration': (i - scene_start) / fps\n",
        "                    })\n",
        "                    scene_start = i\n",
        "\n",
        "            # ROI Detection (focus areas)\n",
        "            roi_map = self.detect_roi(frame)\n",
        "            roi_frames.append({\n",
        "                'frame': i,\n",
        "                'timestamp': i / fps,\n",
        "                'roi_areas': roi_map\n",
        "            })\n",
        "\n",
        "            # Complexity analysis\n",
        "            complexity = self.calculate_frame_complexity(gray, prev_frame)\n",
        "            complexity_data.append(complexity)\n",
        "\n",
        "            prev_frame = gray\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        self.analysis_data = {\n",
        "            'scenes': scenes,\n",
        "            'roi_frames': roi_frames,\n",
        "            'complexity_data': complexity_data,\n",
        "            'avg_complexity': np.mean([c['combined'] for c in complexity_data])\n",
        "        }\n",
        "\n",
        "        return self.analysis_data\n",
        "\n",
        "    def detect_roi(self, frame):\n",
        "        \"\"Detect regions of interest using multiple techniques\"\"\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Edge-based ROI\n",
        "        edges = cv2.Canny(gray, 50, 150)\n",
        "\n",
        "        # Face detection ROI\n",
        "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "        # Motion-based ROI (if previous frame available)\n",
        "        # Saliency-based ROI\n",
        "        saliency = cv2.saliency.StaticSaliencyFineGrained_create()\n",
        "        success, saliency_map = saliency.computeSaliency(frame)\n",
        "\n",
        "        roi_areas = []\n",
        "\n",
        "        # Face ROI (highest priority)\n",
        "        for (x, y, w, h) in faces:\n",
        "            roi_areas.append({\n",
        "                'type': 'face',\n",
        "                'bbox': [x, y, w, h],\n",
        "                'priority': 1.0,\n",
        "                'weight': 2.0\n",
        "            })\n",
        "\n",
        "        # Saliency ROI\n",
        "        if success:\n",
        "            saliency_thresh = cv2.threshold(saliency_map, 0.7, 1.0, cv2.THRESH_BINARY)[1]\n",
        "            contours, _ = cv2.findContours((saliency_thresh * 255).astype(np.uint8),\n",
        "                                         cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "            for contour in contours:\n",
        "                x, y, w, h = cv2.boundingRect(contour)\n",
        "                if w * h > 1000:  # Minimum area threshold\n",
        "                    roi_areas.append({\n",
        "                        'type': 'saliency',\n",
        "                        'bbox': [x, y, w, h],\n",
        "                        'priority': 0.8,\n",
        "                        'weight': 1.5\n",
        "                    })\n",
        "\n",
        "        return roi_areas\n",
        "\n",
        "    def calculate_frame_complexity(self, gray, prev_frame=None):\n",
        "        \"\"Calculate comprehensive frame complexity metrics\"\"\n",
        "        # Spatial complexity\n",
        "        edges = cv2.Canny(gray, 50, 150)\n",
        "        spatial_complexity = np.sum(edges) / (edges.shape[0] * edges.shape[1])\n",
        "\n",
        "        # Texture complexity\n",
        "        gray_f32 = np.float32(gray)\n",
        "        dst = cv2.cornerHarris(gray_f32, 2, 3, 0.04)\n",
        "        texture_complexity = np.sum(dst > 0.01 * dst.max()) / (gray.shape[0] * gray.shape[1])\n",
        "\n",
        "        # Temporal complexity\n",
        "        temporal_complexity = 0\n",
        "        if prev_frame is not None:\n",
        "            diff = cv2.absdiff(gray, prev_frame)\n",
        "            temporal_complexity = np.mean(diff) / 255.0\n",
        "\n",
        "        # Frequency domain complexity\n",
        "        f_transform = np.fft.fft2(gray)\n",
        "        f_shift = np.fft.fftshift(f_transform)\n",
        "        magnitude_spectrum = np.log(np.abs(f_shift) + 1)\n",
        "        freq_complexity = np.std(magnitude_spectrum) / np.mean(magnitude_spectrum)\n",
        "\n",
        "        combined = (spatial_complexity * 0.3 + texture_complexity * 0.3 +\n",
        "                   temporal_complexity * 0.2 + freq_complexity * 0.2)\n",
        "\n",
        "        return {\n",
        "            'spatial': spatial_complexity,\n",
        "            'texture': texture_complexity,\n",
        "            'temporal': temporal_complexity,\n",
        "            'frequency': freq_complexity,\n",
        "            'combined': combined\n",
        "        }\n",
        "\n",
        "    def generate_roi_encoding_params(self, roi_data):\n",
        "        \"\"Generate ROI-based encoding parameters\"\"\n",
        "        roi_params = []\n",
        "\n",
        "        for frame_data in roi_data:\n",
        "            frame_params = {\n",
        "                'timestamp': frame_data['timestamp'],\n",
        "                'roi_zones': []\n",
        "            }\n",
        "\n",
        "            for roi in frame_data['roi_areas']:\n",
        "                x, y, w, h = roi['bbox']\n",
        "\n",
        "                # Convert to relative coordinates for x265\n",
        "                rel_x = x / 1920  # Assuming 1920x1080\n",
        "                rel_y = y / 1080\n",
        "                rel_w = w / 1920\n",
        "                rel_h = h / 1080\n",
        "\n",
        "                zone_param = f\"({rel_x:.3f},{rel_y:.3f},{rel_w:.3f},{rel_h:.3f},{roi['weight']:.1f})\"\n",
        "                frame_params['roi_zones'].append(zone_param)\n",
        "\n",
        "            roi_params.append(frame_params)\n",
        "\n",
        "        return roi_params\n",
        "\n",
        "    def encode_with_advanced_params(self):\n",
        "        \"\"Encode with advanced H.265 parameters including ROI\"\"\n",
        "        analysis = self.analyze_content_advanced()\n",
        "\n",
        "        # Define encoding profiles with advanced parameters\n",
        "        profiles = {\n",
        "            \"ultra_high\": {\n",
        "                \"bitrate\": \"8000k\",\n",
        "                \"crf\": 18,\n",
        "                \"preset\": \"slow\",\n",
        "                \"framerate\": 60,\n",
        "                \"x265_params\": \"rd=6:psy-rd=2.5:aq-mode=3:aq-strength=0.8:deblock=1,1\"\n",
        "            },\n",
        "            \"high\": {\n",
        "                \"bitrate\": \"5000k\",\n",
        "                \"crf\": 20,\n",
        "                \"preset\": \"medium\",\n",
        "                \"framerate\": 30,\n",
        "                \"x265_params\": \"rd=4:psy-rd=2.0:aq-mode=3:aq-strength=1.0:deblock=0,0\"\n",
        "            },\n",
        "            \"medium\": {\n",
        "                \"bitrate\": \"3000k\",\n",
        "                \"crf\": 23,\n",
        "                \"preset\": \"medium\",\n",
        "                \"framerate\": 30,\n",
        "                \"x265_params\": \"rd=3:psy-rd=1.5:aq-mode=2:aq-strength=1.2\"\n",
        "            },\n",
        "            \"low\": {\n",
        "                \"bitrate\": \"1500k\",\n",
        "                \"crf\": 26,\n",
        "                \"preset\": \"fast\",\n",
        "                \"framerate\": 24,\n",
        "                \"x265_params\": \"rd=2:psy-rd=1.0:aq-mode=2:aq-strength=1.4\"\n",
        "            },\n",
        "            \"ultra_low\": {\n",
        "                \"bitrate\": \"800k\",\n",
        "                \"crf\": 30,\n",
        "                \"preset\": \"veryfast\",\n",
        "                \"framerate\": 15,\n",
        "                \"x265_params\": \"rd=1:psy-rd=0.5:aq-mode=1:aq-strength=1.6\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Adjust parameters based on content analysis\n",
        "        if analysis['avg_complexity'] > 0.4:\n",
        "            # High complexity content - boost quality\n",
        "            for profile in profiles.values():\n",
        "                profile['crf'] = max(15, profile['crf'] - 2)\n",
        "                profile['bitrate'] = str(int(profile['bitrate'][:-1]) + 500) + 'k'\n",
        "\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "        # Encode each profile\n",
        "        for profile_name, params in profiles.items():\n",
        "            print(f\"Encoding {profile_name} profile...\")\n",
        "\n",
        "            # Build x265 parameters string\n",
        "            x265_params = params['x265_params']\n",
        "\n",
        "            # Add ROI parameters if available\n",
        "            if self.roi_data:\n",
        "                roi_zones = self.generate_roi_zones_string()\n",
        "                if roi_zones:\n",
        "                    x265_params += f\":zones={roi_zones}\"\n",
        "\n",
        "            cmd = [\n",
        "                \"ffmpeg\", \"-i\", self.input_video,\n",
        "                \"-c:v\", \"libx265\",\n",
        "                \"-preset\", params['preset'],\n",
        "                \"-crf\", str(params['crf']),\n",
        "                \"-b:v\", params['bitrate'],\n",
        "                \"-maxrate\", str(int(params['bitrate'][:-1]) * 1.2) + 'k',\n",
        "                \"-bufsize\", str(int(params['bitrate'][:-1]) * 2) + 'k',\n",
        "                \"-s\", \"1920x1080\",\n",
        "                \"-r\", str(params['framerate']),\n",
        "                \"-g\", \"60\",\n",
        "                \"-keyint_min\", \"60\",\n",
        "                \"-sc_threshold\", \"0\",\n",
        "                \"-x265-params\", x265_params,\n",
        "                \"-c:a\", \"aac\",\n",
        "                \"-b:a\", \"128k\",\n",
        "                \"-ar\", \"44100\",\n",
        "                \"-ac\", \"2\",\n",
        "                \"-movflags\", \"+faststart\",\n",
        "                f\"{self.output_dir}/video_{profile_name}.mp4\",\n",
        "                \"-y\"\n",
        "            ]\n",
        "\n",
        "            result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                print(f\"‚úì Successfully encoded {profile_name}\")\n",
        "\n",
        "                # Generate quality metrics\n",
        "                self.measure_quality(f\"{self.output_dir}/video_{profile_name}.mp4\", profile_name)\n",
        "            else:\n",
        "                print(f\"‚úó Failed to encode {profile_name}: {result.stderr}\")\n",
        "\n",
        "    def generate_roi_zones_string(self):\n",
        "        \"\"Generate x265 zones parameter for ROI encoding\"\"\n",
        "        if not self.roi_data:\n",
        "            return \"\"\n",
        "\n",
        "        zones = []\n",
        "        for frame_data in self.roi_data[:10]:  # Limit to avoid parameter overflow\n",
        "            for roi in frame_data['roi_areas']:\n",
        "                if roi['type'] == 'face':  # Prioritize faces\n",
        "                    x, y, w, h = roi['bbox']\n",
        "                    # Convert to x265 zone format: start_frame,end_frame,crf_offset\n",
        "                    zones.append(f\"{int(frame_data['timestamp']*30)},{int(frame_data['timestamp']*30)+30},b={roi['weight']}\")\n",
        "\n",
        "        return \"/\".join(zones[:5])  # Limit zones\n",
        "\n",
        "    def measure_quality(self, encoded_file, profile_name):\n",
        "        \"\"Measure encoding quality metrics\"\"\n",
        "        # Use VMAF for quality measurement\n",
        "        try:\n",
        "            cmd = [\n",
        "                \"ffmpeg\", \"-i\", self.input_video, \"-i\", encoded_file,\n",
        "                \"-lavfi\", \"libvmaf=log_path=quality_metrics.json:log_fmt=json\",\n",
        "                \"-f\", \"null\", \"-\"\n",
        "            ]\n",
        "\n",
        "            subprocess.run(cmd, capture_output=True)\n",
        "\n",
        "            # Parse VMAF results\n",
        "            if os.path.exists(\"quality_metrics.json\"):\n",
        "                with open(\"quality_metrics.json\", \"r\") as f:\n",
        "                    vmaf_data = json.load(f)\n",
        "\n",
        "                avg_vmaf = np.mean([frame[\"metrics\"][\"vmaf\"] for frame in vmaf_data[\"frames\"]])\n",
        "\n",
        "                print(f\"VMAF Score for {profile_name}: {avg_vmaf:.2f}\")\n",
        "\n",
        "                # Save metrics\n",
        "                with open(f\"benchmarks/quality-metrics/{profile_name}_metrics.json\", \"w\") as f:\n",
        "                    json.dump({\n",
        "                        \"profile\": profile_name,\n",
        "                        \"vmaf_score\": avg_vmaf,\n",
        "                        \"file_size\": os.path.getsize(encoded_file),\n",
        "                        \"compression_ratio\": os.path.getsize(self.input_video) / os.path.getsize(encoded_file)\n",
        "                    }, f, indent=2)\n",
        "\n",
        "                os.remove(\"quality_metrics.json\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Quality measurement failed for {profile_name}: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    if len(sys.argv) != 3:\n",
        "        print(\"Usage: python3 advanced_h265_encoder.py <input_video> <output_dir>\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    encoder = AdvancedH265Encoder(sys.argv[1], sys.argv[2])\n",
        "    encoder.encode_with_advanced_params()\n",
        "```\n",
        "'''\n",
        "with open(\"src/encoding/advanced_h265_encoder.py\", \"w\") as f:\n",
        "    f.write(encoder_code)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpVDPCV3a46s",
        "outputId": "84ef393f-5d67-4555-d701-ea3d39a401d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "advanced_h265_encoder.py  h265_advanced_encorder.py\n"
          ]
        }
      ],
      "source": [
        "!ls src/encoding/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Sr7r9QqbTNJ",
        "outputId": "94c1300d-ec5c-4024-ed99-ad32b899f423"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  File \"/content/src/encoding/advanced_h265_encoder.py\", line 2\n",
            "    \"\"\"\n",
            "    ^\n",
            "SyntaxError: unterminated triple-quoted string literal (detected at line 337)\n"
          ]
        }
      ],
      "source": [
        "!python3 src/encoding/advanced_h265_encoder.py content/sample.mp4 encoded/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WBE5jll56On"
      },
      "outputs": [],
      "source": [
        "# Define the path\n",
        "file_path = 'src/encoding/advanced_h265_encoder.py'\n",
        "import os\n",
        "# Create necessary directories\n",
        "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blYn8ijk5jlk"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnmA67BA2HQJ"
      },
      "outputs": [],
      "source": [
        "encoder_code = '''\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "class AdvancedH265Encoder:\n",
        "    def __init__(self, input_video, output_dir):\n",
        "        self.input_video = input_video\n",
        "        self.output_dir = output_dir\n",
        "        self.analysis_data = {}\n",
        "        self.roi_data = []\n",
        "\n",
        "    def analyze_content_advanced(self):\n",
        "        #Advanced content analysis with scene detection and ROI mapping\n",
        "        cap = cv2.VideoCapture(self.input_video)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        scenes = []\n",
        "        roi_frames = []\n",
        "        complexity_data = []\n",
        "\n",
        "        prev_frame = None\n",
        "        scene_start = 0\n",
        "\n",
        "        print(\"Performing advanced content analysis...\")\n",
        "\n",
        "        for i in range(0, frame_count, max(1, frame_count // 200)):\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Scene detection\n",
        "            if prev_frame is not None:\n",
        "                hist_diff = cv2.compareHist(\n",
        "                    cv2.calcHist([prev_frame], [0], None, [256], [0, 256]),\n",
        "                    cv2.calcHist([gray], [0], None, [256], [0, 256]),\n",
        "                    cv2.HISTCMP_CORREL\n",
        "                )\n",
        "\n",
        "                if hist_diff < 0.7:  # Scene change threshold\n",
        "                    scenes.append({\n",
        "                        'start': scene_start,\n",
        "                        'end': i,\n",
        "                        'duration': (i - scene_start) / fps\n",
        "                    })\n",
        "                    scene_start = i\n",
        "\n",
        "            # ROI Detection (focus areas)\n",
        "            roi_map = self.detect_roi(frame)\n",
        "            roi_frames.append({\n",
        "                'frame': i,\n",
        "                'timestamp': i / fps,\n",
        "                'roi_areas': roi_map\n",
        "            })\n",
        "\n",
        "            # Complexity analysis\n",
        "            complexity = self.calculate_frame_complexity(gray, prev_frame)\n",
        "            complexity_data.append(complexity)\n",
        "\n",
        "            prev_frame = gray\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        self.analysis_data = {\n",
        "            'scenes': scenes,\n",
        "            'roi_frames': roi_frames,\n",
        "            'complexity_data': complexity_data,\n",
        "            'avg_complexity': np.mean([c['combined'] for c in complexity_data])\n",
        "        }\n",
        "\n",
        "        return self.analysis_data\n",
        "\n",
        "    def detect_roi(self, frame):\n",
        "        #Detect regions of interest using multiple techniques\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Edge-based ROI\n",
        "        edges = cv2.Canny(gray, 50, 150)\n",
        "\n",
        "        # Face detection ROI\n",
        "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "        # Motion-based ROI (if previous frame available)\n",
        "        # Saliency-based ROI\n",
        "        saliency = cv2.saliency.StaticSaliencyFineGrained_create()\n",
        "        success, saliency_map = saliency.computeSaliency(frame)\n",
        "\n",
        "        roi_areas = []\n",
        "\n",
        "        # Face ROI (highest priority)\n",
        "        for (x, y, w, h) in faces:\n",
        "            roi_areas.append({\n",
        "                'type': 'face',\n",
        "                'bbox': [x, y, w, h],\n",
        "                'priority': 1.0,\n",
        "                'weight': 2.0\n",
        "            })\n",
        "\n",
        "        # Saliency ROI\n",
        "        if success:\n",
        "            saliency_thresh = cv2.threshold(saliency_map, 0.7, 1.0, cv2.THRESH_BINARY)[1]\n",
        "            contours, _ = cv2.findContours((saliency_thresh * 255).astype(np.uint8),\n",
        "                                         cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "            for contour in contours:\n",
        "                x, y, w, h = cv2.boundingRect(contour)\n",
        "                if w * h > 1000:  # Minimum area threshold\n",
        "                    roi_areas.append({\n",
        "                        'type': 'saliency',\n",
        "                        'bbox': [x, y, w, h],\n",
        "                        'priority': 0.8,\n",
        "                        'weight': 1.5\n",
        "                    })\n",
        "\n",
        "        return roi_areas\n",
        "\n",
        "    def calculate_frame_complexity(self, gray, prev_frame=None):\n",
        "        #Calculate comprehensive frame complexity metrics\n",
        "        # Spatial complexity\n",
        "        edges = cv2.Canny(gray, 50, 150)\n",
        "        spatial_complexity = np.sum(edges) / (edges.shape[0] * edges.shape[1])\n",
        "\n",
        "        # Texture complexity\n",
        "        gray_f32 = np.float32(gray)\n",
        "        dst = cv2.cornerHarris(gray_f32, 2, 3, 0.04)\n",
        "        texture_complexity = np.sum(dst > 0.01 * dst.max()) / (gray.shape[0] * gray.shape[1])\n",
        "\n",
        "        # Temporal complexity\n",
        "        temporal_complexity = 0\n",
        "        if prev_frame is not None:\n",
        "            diff = cv2.absdiff(gray, prev_frame)\n",
        "            temporal_complexity = np.mean(diff) / 255.0\n",
        "\n",
        "        # Frequency domain complexity\n",
        "        f_transform = np.fft.fft2(gray)\n",
        "        f_shift = np.fft.fftshift(f_transform)\n",
        "        magnitude_spectrum = np.log(np.abs(f_shift) + 1)\n",
        "        freq_complexity = np.std(magnitude_spectrum) / np.mean(magnitude_spectrum)\n",
        "\n",
        "        combined = (spatial_complexity * 0.3 + texture_complexity * 0.3 +\n",
        "                   temporal_complexity * 0.2 + freq_complexity * 0.2)\n",
        "\n",
        "        return {\n",
        "            'spatial': spatial_complexity,\n",
        "            'texture': texture_complexity,\n",
        "            'temporal': temporal_complexity,\n",
        "            'frequency': freq_complexity,\n",
        "            'combined': combined\n",
        "        }\n",
        "\n",
        "    def generate_roi_encoding_params(self, roi_data):\n",
        "        #Generate ROI-based encoding parameters\n",
        "        roi_params = []\n",
        "\n",
        "        for frame_data in roi_data:\n",
        "            frame_params = {\n",
        "                'timestamp': frame_data['timestamp'],\n",
        "                'roi_zones': []\n",
        "            }\n",
        "\n",
        "            for roi in frame_data['roi_areas']:\n",
        "                x, y, w, h = roi['bbox']\n",
        "\n",
        "                # Convert to relative coordinates for x265\n",
        "                rel_x = x / 1920  # Assuming 1920x1080\n",
        "                rel_y = y / 1080\n",
        "                rel_w = w / 1920\n",
        "                rel_h = h / 1080\n",
        "\n",
        "                zone_param = f\"({rel_x:.3f},{rel_y:.3f},{rel_w:.3f},{rel_h:.3f},{roi['weight']:.1f})\"\n",
        "                frame_params['roi_zones'].append(zone_param)\n",
        "\n",
        "            roi_params.append(frame_params)\n",
        "\n",
        "        return roi_params\n",
        "\n",
        "    def encode_with_advanced_params(self):\n",
        "        #Encode with advanced H.265 parameters including ROI\n",
        "        analysis = self.analyze_content_advanced()\n",
        "\n",
        "        # Define encoding profiles with advanced parameters\n",
        "        profiles = {\n",
        "            \"ultra_high\": {\n",
        "                \"bitrate\": \"8000k\",\n",
        "                \"crf\": 18,\n",
        "                \"preset\": \"slow\",\n",
        "                \"framerate\": 60,\n",
        "                \"x265_params\": \"rd=6:psy-rd=2.5:aq-mode=3:aq-strength=0.8:deblock=1,1\"\n",
        "            },\n",
        "            \"high\": {\n",
        "                \"bitrate\": \"5000k\",\n",
        "                \"crf\": 20,\n",
        "                \"preset\": \"medium\",\n",
        "                \"framerate\": 30,\n",
        "                \"x265_params\": \"rd=4:psy-rd=2.0:aq-mode=3:aq-strength=1.0:deblock=0,0\"\n",
        "            },\n",
        "            \"medium\": {\n",
        "                \"bitrate\": \"3000k\",\n",
        "                \"crf\": 23,\n",
        "                \"preset\": \"medium\",\n",
        "                \"framerate\": 30,\n",
        "                \"x265_params\": \"rd=3:psy-rd=1.5:aq-mode=2:aq-strength=1.2\"\n",
        "            },\n",
        "            \"low\": {\n",
        "                \"bitrate\": \"1500k\",\n",
        "                \"crf\": 26,\n",
        "                \"preset\": \"fast\",\n",
        "                \"framerate\": 24,\n",
        "                \"x265_params\": \"rd=2:psy-rd=1.0:aq-mode=2:aq-strength=1.4\"\n",
        "            },\n",
        "            \"ultra_low\": {\n",
        "                \"bitrate\": \"800k\",\n",
        "                \"crf\": 30,\n",
        "                \"preset\": \"veryfast\",\n",
        "                \"framerate\": 15,\n",
        "                \"x265_params\": \"rd=1:psy-rd=0.5:aq-mode=1:aq-strength=1.6\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Adjust parameters based on content analysis\n",
        "        if analysis['avg_complexity'] > 0.4:\n",
        "            # High complexity content - boost quality\n",
        "            for profile in profiles.values():\n",
        "                profile['crf'] = max(15, profile['crf'] - 2)\n",
        "                profile['bitrate'] = str(int(profile['bitrate'][:-1]) + 500) + 'k'\n",
        "\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "        # Encode each profile\n",
        "        for profile_name, params in profiles.items():\n",
        "            print(f\"Encoding {profile_name} profile...\")\n",
        "\n",
        "            # Build x265 parameters string\n",
        "            x265_params = params['x265_params']\n",
        "\n",
        "            # Add ROI parameters if available\n",
        "            if self.roi_data:\n",
        "                roi_zones = self.generate_roi_zones_string()\n",
        "                if roi_zones:\n",
        "                    x265_params += f\":zones={roi_zones}\"\n",
        "\n",
        "            cmd = [\n",
        "                \"ffmpeg\", \"-i\", self.input_video,\n",
        "                \"-c:v\", \"libx265\",\n",
        "                \"-preset\", params['preset'],\n",
        "                \"-crf\", str(params['crf']),\n",
        "                \"-b:v\", params['bitrate'],\n",
        "                \"-maxrate\", str(int(params['bitrate'][:-1]) * 1.2) + 'k',\n",
        "                \"-bufsize\", str(int(params['bitrate'][:-1]) * 2) + 'k',\n",
        "                \"-s\", \"1920x1080\",\n",
        "                \"-r\", str(params['framerate']),\n",
        "                \"-g\", \"60\",\n",
        "                \"-keyint_min\", \"60\",\n",
        "                \"-sc_threshold\", \"0\",\n",
        "                \"-x265-params\", x265_params,\n",
        "                \"-c:a\", \"aac\",\n",
        "                \"-b:a\", \"128k\",\n",
        "                \"-ar\", \"44100\",\n",
        "                \"-ac\", \"2\",\n",
        "                \"-movflags\", \"+faststart\",\n",
        "                f\"{self.output_dir}/video_{profile_name}.mp4\",\n",
        "                \"-y\"\n",
        "            ]\n",
        "\n",
        "            result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                print(f\"‚úì Successfully encoded {profile_name}\")\n",
        "\n",
        "                # Generate quality metrics\n",
        "                self.measure_quality(f\"{self.output_dir}/video_{profile_name}.mp4\", profile_name)\n",
        "            else:\n",
        "                print(f\"‚úó Failed to encode {profile_name}: {result.stderr}\")\n",
        "\n",
        "    def generate_roi_zones_string(self):\n",
        "        #Generate x265 zones parameter for ROI encoding\n",
        "        if not self.roi_data:\n",
        "            return \"\"\n",
        "\n",
        "        zones = []\n",
        "        for frame_data in self.roi_data[:10]:  # Limit to avoid parameter overflow\n",
        "            for roi in frame_data['roi_areas']:\n",
        "                if roi['type'] == 'face':  # Prioritize faces\n",
        "                    x, y, w, h = roi['bbox']\n",
        "                    # Convert to x265 zone format: start_frame,end_frame,crf_offset\n",
        "                    zones.append(f\"{int(frame_data['timestamp']*30)},{int(frame_data['timestamp']*30)+30},b={roi['weight']}\")\n",
        "\n",
        "        return \"/\".join(zones[:5])  # Limit zones\n",
        "\n",
        "    def measure_quality(self, encoded_file, profile_name):\n",
        "        #Measure encoding quality metrics\n",
        "        # Use VMAF for quality measurement\n",
        "        try:\n",
        "            cmd = [\n",
        "                \"ffmpeg\", \"-i\", self.input_video, \"-i\", encoded_file,\n",
        "                \"-lavfi\", \"libvmaf=log_path=quality_metrics.json:log_fmt=json\",\n",
        "                \"-f\", \"null\", \"-\"\n",
        "            ]\n",
        "\n",
        "            subprocess.run(cmd, capture_output=True)\n",
        "\n",
        "            # Parse VMAF results\n",
        "            if os.path.exists(\"quality_metrics.json\"):\n",
        "                with open(\"quality_metrics.json\", \"r\") as f:\n",
        "                    vmaf_data = json.load(f)\n",
        "\n",
        "                avg_vmaf = np.mean([frame[\"metrics\"][\"vmaf\"] for frame in vmaf_data[\"frames\"]])\n",
        "\n",
        "                print(f\"VMAF Score for {profile_name}: {avg_vmaf:.2f}\")\n",
        "\n",
        "                # Save metrics\n",
        "                with open(f\"benchmarks/quality-metrics/{profile_name}_metrics.json\", \"w\") as f:\n",
        "                    json.dump({\n",
        "                        \"profile\": profile_name,\n",
        "                        \"vmaf_score\": avg_vmaf,\n",
        "                        \"file_size\": os.path.getsize(encoded_file),\n",
        "                        \"compression_ratio\": os.path.getsize(self.input_video) / os.path.getsize(encoded_file)\n",
        "                    }, f, indent=2)\n",
        "\n",
        "                os.remove(\"quality_metrics.json\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Quality measurement failed for {profile_name}: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    if len(sys.argv) != 3:\n",
        "        print(\"Usage: python3 advanced_h265_encoder.py <input_video> <output_dir>\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    encoder = AdvancedH265Encoder(sys.argv[1], sys.argv[2])\n",
        "    encoder.encode_with_advanced_params()\n",
        "\n",
        "print(\"‚úÖ All profiles encoded successfully. Outputs saved to:\", self.output_dir)\n",
        "\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iO1K8xIOGta5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEfchtknGuIL"
      },
      "outputs": [],
      "source": [
        "encoder_code = '''\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "class AdvancedH265Encoder:\n",
        "    def __init__(self, input_video, output_dir):\n",
        "        self.input_video = input_video\n",
        "        self.output_dir = output_dir\n",
        "        self.analysis_data = {}\n",
        "        self.roi_data = []\n",
        "\n",
        "    def analyze_content_advanced(self):\n",
        "        #Advanced content analysis with scene detection and ROI mapping\n",
        "        cap = cv2.VideoCapture(self.input_video)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        scenes = []\n",
        "        roi_frames = []\n",
        "        complexity_data = []\n",
        "\n",
        "        prev_frame = None\n",
        "        scene_start = 0\n",
        "\n",
        "        print(\"Performing advanced content analysis...\")\n",
        "\n",
        "        for i in range(0, frame_count, max(1, frame_count // 200)):\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Scene detection\n",
        "            if prev_frame is not None:\n",
        "                hist_diff = cv2.compareHist(\n",
        "                    cv2.calcHist([prev_frame], [0], None, [256], [0, 256]),\n",
        "                    cv2.calcHist([gray], [0], None, [256], [0, 256]),\n",
        "                    cv2.HISTCMP_CORREL\n",
        "                )\n",
        "\n",
        "                if hist_diff < 0.7:  # Scene change threshold\n",
        "                    scenes.append({\n",
        "                        'start': scene_start,\n",
        "                        'end': i,\n",
        "                        'duration': (i - scene_start) / fps\n",
        "                    })\n",
        "                    scene_start = i\n",
        "\n",
        "            # ROI Detection (focus areas)\n",
        "            roi_map = self.detect_roi(frame)\n",
        "            roi_frames.append({\n",
        "                'frame': i,\n",
        "                'timestamp': i / fps,\n",
        "                'roi_areas': roi_map\n",
        "            })\n",
        "\n",
        "            # Complexity analysis\n",
        "            complexity = self.calculate_frame_complexity(gray, prev_frame)\n",
        "            complexity_data.append(complexity)\n",
        "\n",
        "            prev_frame = gray\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        self.analysis_data = {\n",
        "            'scenes': scenes,\n",
        "            'roi_frames': roi_frames,\n",
        "            'complexity_data': complexity_data,\n",
        "            'avg_complexity': np.mean([c['combined'] for c in complexity_data])\n",
        "        }\n",
        "\n",
        "        return self.analysis_data\n",
        "\n",
        "    def detect_roi(self, frame):\n",
        "        #Detect regions of interest using multiple techniques\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Edge-based ROI\n",
        "        edges = cv2.Canny(gray, 50, 150)\n",
        "\n",
        "        # Face detection ROI\n",
        "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "        # Motion-based ROI (if previous frame available)\n",
        "        # Saliency-based ROI\n",
        "        saliency = cv2.saliency.StaticSaliencyFineGrained_create()\n",
        "        success, saliency_map = saliency.computeSaliency(frame)\n",
        "\n",
        "        roi_areas = []\n",
        "\n",
        "        # Face ROI (highest priority)\n",
        "        for (x, y, w, h) in faces:\n",
        "            roi_areas.append({\n",
        "                'type': 'face',\n",
        "                'bbox': [x, y, w, h],\n",
        "                'priority': 1.0,\n",
        "                'weight': 2.0\n",
        "            })\n",
        "\n",
        "        # Saliency ROI\n",
        "        if success:\n",
        "            saliency_thresh = cv2.threshold(saliency_map, 0.7, 1.0, cv2.THRESH_BINARY)[1]\n",
        "            contours, _ = cv2.findContours((saliency_thresh * 255).astype(np.uint8),\n",
        "                                         cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "            for contour in contours:\n",
        "                x, y, w, h = cv2.boundingRect(contour)\n",
        "                if w * h > 1000:  # Minimum area threshold\n",
        "                    roi_areas.append({\n",
        "                        'type': 'saliency',\n",
        "                        'bbox': [x, y, w, h],\n",
        "                        'priority': 0.8,\n",
        "                        'weight': 1.5\n",
        "                    })\n",
        "\n",
        "        return roi_areas\n",
        "\n",
        "    def calculate_frame_complexity(self, gray, prev_frame=None):\n",
        "        #Calculate comprehensive frame complexity metrics\n",
        "        # Spatial complexity\n",
        "        edges = cv2.Canny(gray, 50, 150)\n",
        "        spatial_complexity = np.sum(edges) / (edges.shape[0] * edges.shape[1])\n",
        "\n",
        "        # Texture complexity\n",
        "        gray_f32 = np.float32(gray)\n",
        "        dst = cv2.cornerHarris(gray_f32, 2, 3, 0.04)\n",
        "        texture_complexity = np.sum(dst > 0.01 * dst.max()) / (gray.shape[0] * gray.shape[1])\n",
        "\n",
        "        # Temporal complexity\n",
        "        temporal_complexity = 0\n",
        "        if prev_frame is not None:\n",
        "            diff = cv2.absdiff(gray, prev_frame)\n",
        "            temporal_complexity = np.mean(diff) / 255.0\n",
        "\n",
        "        # Frequency domain complexity\n",
        "        f_transform = np.fft.fft2(gray)\n",
        "        f_shift = np.fft.fftshift(f_transform)\n",
        "        magnitude_spectrum = np.log(np.abs(f_shift) + 1)\n",
        "        freq_complexity = np.std(magnitude_spectrum) / np.mean(magnitude_spectrum)\n",
        "\n",
        "        combined = (spatial_complexity * 0.3 + texture_complexity * 0.3 +\n",
        "                   temporal_complexity * 0.2 + freq_complexity * 0.2)\n",
        "\n",
        "        return {\n",
        "            'spatial': spatial_complexity,\n",
        "            'texture': texture_complexity,\n",
        "            'temporal': temporal_complexity,\n",
        "            'frequency': freq_complexity,\n",
        "            'combined': combined\n",
        "        }\n",
        "\n",
        "    def generate_roi_encoding_params(self, roi_data):\n",
        "        #Generate ROI-based encoding parameters\n",
        "        roi_params = []\n",
        "\n",
        "        for frame_data in roi_data:\n",
        "            frame_params = {\n",
        "                'timestamp': frame_data['timestamp'],\n",
        "                'roi_zones': []\n",
        "            }\n",
        "\n",
        "            for roi in frame_data['roi_areas']:\n",
        "                x, y, w, h = roi['bbox']\n",
        "\n",
        "                # Convert to relative coordinates for x265\n",
        "                rel_x = x / 1920  # Assuming 1920x1080\n",
        "                rel_y = y / 1080\n",
        "                rel_w = w / 1920\n",
        "                rel_h = h / 1080\n",
        "\n",
        "                zone_param = f\"({rel_x:.3f},{rel_y:.3f},{rel_w:.3f},{rel_h:.3f},{roi['weight']:.1f})\"\n",
        "                frame_params['roi_zones'].append(zone_param)\n",
        "\n",
        "            roi_params.append(frame_params)\n",
        "\n",
        "        return roi_params\n",
        "\n",
        "    def encode_with_advanced_params(self):\n",
        "        #Encode with advanced H.265 parameters including ROI\n",
        "        analysis = self.analyze_content_advanced()\n",
        "\n",
        "        # Define encoding profiles with advanced parameters\n",
        "        profiles = {\n",
        "            \"ultra_high\": {\n",
        "                \"bitrate\": \"8000k\",\n",
        "                \"crf\": 18,\n",
        "                \"preset\": \"slow\",\n",
        "                \"framerate\": 60,\n",
        "                \"x265_params\": \"rd=6:psy-rd=2.5:aq-mode=3:aq-strength=0.8:deblock=1,1\"\n",
        "            },\n",
        "            \"high\": {\n",
        "                \"bitrate\": \"5000k\",\n",
        "                \"crf\": 20,\n",
        "                \"preset\": \"medium\",\n",
        "                \"framerate\": 30,\n",
        "                \"x265_params\": \"rd=4:psy-rd=2.0:aq-mode=3:aq-strength=1.0:deblock=0,0\"\n",
        "            },\n",
        "            \"medium\": {\n",
        "                \"bitrate\": \"3000k\",\n",
        "                \"crf\": 23,\n",
        "                \"preset\": \"medium\",\n",
        "                \"framerate\": 30,\n",
        "                \"x265_params\": \"rd=3:psy-rd=1.5:aq-mode=2:aq-strength=1.2\"\n",
        "            },\n",
        "            \"low\": {\n",
        "                \"bitrate\": \"1500k\",\n",
        "                \"crf\": 26,\n",
        "                \"preset\": \"fast\",\n",
        "                \"framerate\": 24,\n",
        "                \"x265_params\": \"rd=2:psy-rd=1.0:aq-mode=2:aq-strength=1.4\"\n",
        "            },\n",
        "            \"ultra_low\": {\n",
        "                \"bitrate\": \"800k\",\n",
        "                \"crf\": 30,\n",
        "                \"preset\": \"veryfast\",\n",
        "                \"framerate\": 15,\n",
        "                \"x265_params\": \"rd=1:psy-rd=0.5:aq-mode=1:aq-strength=1.6\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Adjust parameters based on content analysis\n",
        "        if analysis['avg_complexity'] > 0.4:\n",
        "            # High complexity content - boost quality\n",
        "            for profile in profiles.values():\n",
        "                profile['crf'] = max(15, profile['crf'] - 2)\n",
        "                profile['bitrate'] = str(int(profile['bitrate'][:-1]) + 500) + 'k'\n",
        "\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "        # Encode each profile\n",
        "        for profile_name, params in profiles.items():\n",
        "            print(f\"Encoding {profile_name} profile...\")\n",
        "\n",
        "            # Build x265 parameters string\n",
        "            x265_params = params['x265_params']\n",
        "\n",
        "            # Add ROI parameters if available\n",
        "            if self.roi_data:\n",
        "                roi_zones = self.generate_roi_zones_string()\n",
        "                if roi_zones:\n",
        "                    x265_params += f\":zones={roi_zones}\"\n",
        "\n",
        "            cmd = [\n",
        "                \"ffmpeg\", \"-i\", self.input_video,\n",
        "                \"-c:v\", \"libx265\",\n",
        "                \"-preset\", params['preset'],\n",
        "                \"-crf\", str(params['crf']),\n",
        "                \"-b:v\", params['bitrate'],\n",
        "                \"-maxrate\", str(int(params['bitrate'][:-1]) * 1.2) + 'k',\n",
        "                \"-bufsize\", str(int(params['bitrate'][:-1]) * 2) + 'k',\n",
        "                \"-s\", \"1920x1080\",\n",
        "                \"-r\", str(params['framerate']),\n",
        "                \"-g\", \"60\",\n",
        "                \"-keyint_min\", \"60\",\n",
        "                \"-sc_threshold\", \"0\",\n",
        "                \"-x265-params\", x265_params,\n",
        "                \"-c:a\", \"aac\",\n",
        "                \"-b:a\", \"128k\",\n",
        "                \"-ar\", \"44100\",\n",
        "                \"-ac\", \"2\",\n",
        "                \"-movflags\", \"+faststart\",\n",
        "                f\"{self.output_dir}/video_{profile_name}.mp4\",\n",
        "                \"-y\"\n",
        "            ]\n",
        "\n",
        "            result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                print(f\"‚úì Successfully encoded {profile_name}\")\n",
        "\n",
        "                # Generate quality metrics\n",
        "                self.measure_quality(f\"{self.output_dir}/video_{profile_name}.mp4\", profile_name)\n",
        "            else:\n",
        "                print(f\"‚úó Failed to encode {profile_name}: {result.stderr}\")\n",
        "\n",
        "    def generate_roi_zones_string(self):\n",
        "        #Generate x265 zones parameter for ROI encoding\n",
        "        if not self.roi_data:\n",
        "            return \"\"\n",
        "\n",
        "        zones = []\n",
        "        for frame_data in self.roi_data[:10]:  # Limit to avoid parameter overflow\n",
        "            for roi in frame_data['roi_areas']:\n",
        "                if roi['type'] == 'face':  # Prioritize faces\n",
        "                    x, y, w, h = roi['bbox']\n",
        "                    # Convert to x265 zone format: start_frame,end_frame,crf_offset\n",
        "                    zones.append(f\"{int(frame_data['timestamp']*30)},{int(frame_data['timestamp']*30)+30},b={roi['weight']}\")\n",
        "\n",
        "        return \"/\".join(zones[:5])  # Limit zones\n",
        "\n",
        "    def measure_quality(self, encoded_file, profile_name):\n",
        "        #Measure encoding quality metrics\n",
        "        # Use VMAF for quality measurement\n",
        "        try:\n",
        "            cmd = [\n",
        "                \"ffmpeg\", \"-i\", self.input_video, \"-i\", encoded_file,\n",
        "                \"-lavfi\", \"libvmaf=log_path=quality_metrics.json:log_fmt=json\",\n",
        "                \"-f\", \"null\", \"-\"\n",
        "            ]\n",
        "\n",
        "            subprocess.run(cmd, capture_output=True)\n",
        "\n",
        "            # Parse VMAF results\n",
        "            if os.path.exists(\"quality_metrics.json\"):\n",
        "                with open(\"quality_metrics.json\", \"r\") as f:\n",
        "                    vmaf_data = json.load(f)\n",
        "\n",
        "                avg_vmaf = np.mean([frame[\"metrics\"][\"vmaf\"] for frame in vmaf_data[\"frames\"]])\n",
        "\n",
        "                print(f\"VMAF Score for {profile_name}: {avg_vmaf:.2f}\")\n",
        "\n",
        "                # Save metrics\n",
        "                with open(f\"benchmarks/quality-metrics/{profile_name}_metrics.json\", \"w\") as f:\n",
        "                    json.dump({\n",
        "                        \"profile\": profile_name,\n",
        "                        \"vmaf_score\": avg_vmaf,\n",
        "                        \"file_size\": os.path.getsize(encoded_file),\n",
        "                        \"compression_ratio\": os.path.getsize(self.input_video) / os.path.getsize(encoded_file)\n",
        "                    }, f, indent=2)\n",
        "\n",
        "                os.remove(\"quality_metrics.json\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Quality measurement failed for {profile_name}: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    if len(sys.argv) != 3:\n",
        "        print(\"Usage: python3 advanced_h265_encoder.py <input_video> <output_dir>\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    encoder = AdvancedH265Encoder(sys.argv[1], sys.argv[2])\n",
        "    encoder.encode_with_advanced_params()\n",
        "\n",
        "\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsqJszXa2xKe",
        "outputId": "bb043697-d7b3-4671-f48b-cded44459d60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "encoding\n"
          ]
        }
      ],
      "source": [
        "!ls src/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4lvi5Cg64Y4",
        "outputId": "1e23f0ab-3d76-488a-80c9-dce962f34135"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Created: src/encoding/advanced_h265_encoder.py\n"
          ]
        }
      ],
      "source": [
        "file_path = 'src/encoding/advanced_h265_encoder.py'\n",
        "\n",
        "with open(file_path, 'w') as f:\n",
        "    f.write(encoder_code)\n",
        "\n",
        "print(f'‚úÖ Created: {file_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5Rw6xioY7G2I",
        "outputId": "ac17cddf-91c6-4810-aef3-0525cd08ed2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "import cv2\n",
            "import numpy as np\n",
            "import json\n",
            "import subprocess\n",
            "import os\n",
            "\n",
            "class AdvancedH265Encoder:\n",
            "    def __init__(self, input_video, output_dir):\n",
            "        self.input_video = input_video\n",
            "        self.output_dir = output_dir\n",
            "        self.analysis_data = {}\n",
            "        self.roi_data = []\n",
            "\n",
            "    def analyze_content_advanced(self):\n",
            "        #Advanced content analysis with scene detection and ROI mapping\n",
            "        cap = cv2.VideoCapture(self.input_video)\n",
            "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
            "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
            "\n",
            "        scenes = []\n",
            "        roi_frames = []\n",
            "        complexity_data = []\n",
            "\n",
            "        prev_frame = None\n",
            "        scene_start = 0\n",
            "\n",
            "        print(\"Performing advanced content analysis...\")\n",
            "\n",
            "        for i in range(0, frame_count, max(1, frame_count // 200)):\n",
            "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
            "            ret, frame = cap.read()\n",
            "            if not ret:\n",
            "                break\n",
            "\n",
            "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
            "\n",
            "            # Scene detection\n",
            "            if prev_frame is not None:\n",
            "                hist_diff = cv2.compareHist(\n",
            "                    cv2.calcHist([prev_frame], [0], None, [256], [0, 256]),\n",
            "                    cv2.calcHist([gray], [0], None, [256], [0, 256]),\n",
            "                    cv2.HISTCMP_CORREL\n",
            "                )\n",
            "\n",
            "                if hist_diff < 0.7:  # Scene change threshold\n",
            "                    scenes.append({\n",
            "                        'start': scene_start,\n",
            "                        'end': i,\n",
            "                        'duration': (i - scene_start) / fps\n",
            "                    })\n",
            "                    scene_start = i\n",
            "\n",
            "            # ROI Detection (focus areas)\n",
            "            roi_map = self.detect_roi(frame)\n",
            "            roi_frames.append({\n",
            "                'frame': i,\n",
            "                'timestamp': i / fps,\n",
            "                'roi_areas': roi_map\n",
            "            })\n",
            "\n",
            "            # Complexity analysis\n",
            "            complexity = self.calculate_frame_complexity(gray, prev_frame)\n",
            "            complexity_data.append(complexity)\n",
            "\n",
            "            prev_frame = gray\n",
            "\n",
            "        cap.release()\n",
            "\n",
            "        self.analysis_data = {\n",
            "            'scenes': scenes,\n",
            "            'roi_frames': roi_frames,\n",
            "            'complexity_data': complexity_data,\n",
            "            'avg_complexity': np.mean([c['combined'] for c in complexity_data])\n",
            "        }\n",
            "\n",
            "        return self.analysis_data\n",
            "\n",
            "    def detect_roi(self, frame):\n",
            "        #Detect regions of interest using multiple techniques\n",
            "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
            "\n",
            "        # Edge-based ROI\n",
            "        edges = cv2.Canny(gray, 50, 150)\n",
            "\n",
            "        # Face detection ROI\n",
            "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
            "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
            "\n",
            "        # Motion-based ROI (if previous frame available)\n",
            "        # Saliency-based ROI\n",
            "        saliency = cv2.saliency.StaticSaliencyFineGrained_create()\n",
            "        success, saliency_map = saliency.computeSaliency(frame)\n",
            "\n",
            "        roi_areas = []\n",
            "\n",
            "        # Face ROI (highest priority)\n",
            "        for (x, y, w, h) in faces:\n",
            "            roi_areas.append({\n",
            "                'type': 'face',\n",
            "                'bbox': [x, y, w, h],\n",
            "                'priority': 1.0,\n",
            "                'weight': 2.0\n",
            "            })\n",
            "\n",
            "        # Saliency ROI\n",
            "        if success:\n",
            "            saliency_thresh = cv2.threshold(saliency_map, 0.7, 1.0, cv2.THRESH_BINARY)[1]\n",
            "            contours, _ = cv2.findContours((saliency_thresh * 255).astype(np.uint8),\n",
            "                                         cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
            "\n",
            "            for contour in contours:\n",
            "                x, y, w, h = cv2.boundingRect(contour)\n",
            "                if w * h > 1000:  # Minimum area threshold\n",
            "                    roi_areas.append({\n",
            "                        'type': 'saliency',\n",
            "                        'bbox': [x, y, w, h],\n",
            "                        'priority': 0.8,\n",
            "                        'weight': 1.5\n",
            "                    })\n",
            "\n",
            "        return roi_areas\n",
            "\n",
            "    def calculate_frame_complexity(self, gray, prev_frame=None):\n",
            "        #Calculate comprehensive frame complexity metrics\n",
            "        # Spatial complexity\n",
            "        edges = cv2.Canny(gray, 50, 150)\n",
            "        spatial_complexity = np.sum(edges) / (edges.shape[0] * edges.shape[1])\n",
            "\n",
            "        # Texture complexity\n",
            "        gray_f32 = np.float32(gray)\n",
            "        dst = cv2.cornerHarris(gray_f32, 2, 3, 0.04)\n",
            "        texture_complexity = np.sum(dst > 0.01 * dst.max()) / (gray.shape[0] * gray.shape[1])\n",
            "\n",
            "        # Temporal complexity\n",
            "        temporal_complexity = 0\n",
            "        if prev_frame is not None:\n",
            "            diff = cv2.absdiff(gray, prev_frame)\n",
            "            temporal_complexity = np.mean(diff) / 255.0\n",
            "\n",
            "        # Frequency domain complexity\n",
            "        f_transform = np.fft.fft2(gray)\n",
            "        f_shift = np.fft.fftshift(f_transform)\n",
            "        magnitude_spectrum = np.log(np.abs(f_shift) + 1)\n",
            "        freq_complexity = np.std(magnitude_spectrum) / np.mean(magnitude_spectrum)\n",
            "\n",
            "        combined = (spatial_complexity * 0.3 + texture_complexity * 0.3 +\n",
            "                   temporal_complexity * 0.2 + freq_complexity * 0.2)\n",
            "\n",
            "        return {\n",
            "            'spatial': spatial_complexity,\n",
            "            'texture': texture_complexity,\n",
            "            'temporal': temporal_complexity,\n",
            "            'frequency': freq_complexity,\n",
            "            'combined': combined\n",
            "        }\n",
            "\n",
            "    def generate_roi_encoding_params(self, roi_data):\n",
            "        #Generate ROI-based encoding parameters\n",
            "        roi_params = []\n",
            "\n",
            "        for frame_data in roi_data:\n",
            "            frame_params = {\n",
            "                'timestamp': frame_data['timestamp'],\n",
            "                'roi_zones': []\n",
            "            }\n",
            "\n",
            "            for roi in frame_data['roi_areas']:\n",
            "                x, y, w, h = roi['bbox']\n",
            "\n",
            "                # Convert to relative coordinates for x265\n",
            "                rel_x = x / 1920  # Assuming 1920x1080\n",
            "                rel_y = y / 1080\n",
            "                rel_w = w / 1920\n",
            "                rel_h = h / 1080\n",
            "\n",
            "                zone_param = f\"({rel_x:.3f},{rel_y:.3f},{rel_w:.3f},{rel_h:.3f},{roi['weight']:.1f})\"\n",
            "                frame_params['roi_zones'].append(zone_param)\n",
            "\n",
            "            roi_params.append(frame_params)\n",
            "\n",
            "        return roi_params\n",
            "\n",
            "    def encode_with_advanced_params(self):\n",
            "        #Encode with advanced H.265 parameters including ROI\n",
            "        analysis = self.analyze_content_advanced()\n",
            "\n",
            "        # Define encoding profiles with advanced parameters\n",
            "        profiles = {\n",
            "            \"ultra_high\": {\n",
            "                \"bitrate\": \"8000k\",\n",
            "                \"crf\": 18,\n",
            "                \"preset\": \"slow\",\n",
            "                \"framerate\": 60,\n",
            "                \"x265_params\": \"rd=6:psy-rd=2.5:aq-mode=3:aq-strength=0.8:deblock=1,1\"\n",
            "            },\n",
            "            \"high\": {\n",
            "                \"bitrate\": \"5000k\",\n",
            "                \"crf\": 20,\n",
            "                \"preset\": \"medium\",\n",
            "                \"framerate\": 30,\n",
            "                \"x265_params\": \"rd=4:psy-rd=2.0:aq-mode=3:aq-strength=1.0:deblock=0,0\"\n",
            "            },\n",
            "            \"medium\": {\n",
            "                \"bitrate\": \"3000k\",\n",
            "                \"crf\": 23,\n",
            "                \"preset\": \"medium\",\n",
            "                \"framerate\": 30,\n",
            "                \"x265_params\": \"rd=3:psy-rd=1.5:aq-mode=2:aq-strength=1.2\"\n",
            "            },\n",
            "            \"low\": {\n",
            "                \"bitrate\": \"1500k\",\n",
            "                \"crf\": 26,\n",
            "                \"preset\": \"fast\",\n",
            "                \"framerate\": 24,\n",
            "                \"x265_params\": \"rd=2:psy-rd=1.0:aq-mode=2:aq-strength=1.4\"\n",
            "            },\n",
            "            \"ultra_low\": {\n",
            "                \"bitrate\": \"800k\",\n",
            "                \"crf\": 30,\n",
            "                \"preset\": \"veryfast\",\n",
            "                \"framerate\": 15,\n",
            "                \"x265_params\": \"rd=1:psy-rd=0.5:aq-mode=1:aq-strength=1.6\"\n",
            "            }\n",
            "        }\n",
            "\n",
            "        # Adjust parameters based on content analysis\n",
            "        if analysis['avg_complexity'] > 0.4:\n",
            "            # High complexity content - boost quality\n",
            "            for profile in profiles.values():\n",
            "                profile['crf'] = max(15, profile['crf'] - 2)\n",
            "                profile['bitrate'] = str(int(profile['bitrate'][:-1]) + 500) + 'k'\n",
            "\n",
            "        os.makedirs(self.output_dir, exist_ok=True)\n",
            "\n",
            "        # Encode each profile\n",
            "        for profile_name, params in profiles.items():\n",
            "            print(f\"Encoding {profile_name} profile...\")\n",
            "\n",
            "            # Build x265 parameters string\n",
            "            x265_params = params['x265_params']\n",
            "\n",
            "            # Add ROI parameters if available\n",
            "            if self.roi_data:\n",
            "                roi_zones = self.generate_roi_zones_string()\n",
            "                if roi_zones:\n",
            "                    x265_params += f\":zones={roi_zones}\"\n",
            "\n",
            "            cmd = [\n",
            "                \"ffmpeg\", \"-i\", self.input_video,\n",
            "                \"-c:v\", \"libx265\",\n",
            "                \"-preset\", params['preset'],\n",
            "                \"-crf\", str(params['crf']),\n",
            "                \"-b:v\", params['bitrate'],\n",
            "                \"-maxrate\", str(int(params['bitrate'][:-1]) * 1.2) + 'k',\n",
            "                \"-bufsize\", str(int(params['bitrate'][:-1]) * 2) + 'k',\n",
            "                \"-s\", \"1920x1080\",\n",
            "                \"-r\", str(params['framerate']),\n",
            "                \"-g\", \"60\",\n",
            "                \"-keyint_min\", \"60\",\n",
            "                \"-sc_threshold\", \"0\",\n",
            "                \"-x265-params\", x265_params,\n",
            "                \"-c:a\", \"aac\",\n",
            "                \"-b:a\", \"128k\",\n",
            "                \"-ar\", \"44100\",\n",
            "                \"-ac\", \"2\",\n",
            "                \"-movflags\", \"+faststart\",\n",
            "                f\"{self.output_dir}/video_{profile_name}.mp4\",\n",
            "                \"-y\"\n",
            "            ]\n",
            "\n",
            "            result = subprocess.run(cmd, capture_output=True, text=True)\n",
            "\n",
            "            if result.returncode == 0:\n",
            "                print(f\"‚úì Successfully encoded {profile_name}\")\n",
            "\n",
            "                # Generate quality metrics\n",
            "                self.measure_quality(f\"{self.output_dir}/video_{profile_name}.mp4\", profile_name)\n",
            "            else:\n",
            "                print(f\"‚úó Failed to encode {profile_name}: {result.stderr}\")\n",
            "\n",
            "    def generate_roi_zones_string(self):\n",
            "        #Generate x265 zones parameter for ROI encoding\n",
            "        if not self.roi_data:\n",
            "            return \"\"\n",
            "\n",
            "        zones = []\n",
            "        for frame_data in self.roi_data[:10]:  # Limit to avoid parameter overflow\n",
            "            for roi in frame_data['roi_areas']:\n",
            "                if roi['type'] == 'face':  # Prioritize faces\n",
            "                    x, y, w, h = roi['bbox']\n",
            "                    # Convert to x265 zone format: start_frame,end_frame,crf_offset\n",
            "                    zones.append(f\"{int(frame_data['timestamp']*30)},{int(frame_data['timestamp']*30)+30},b={roi['weight']}\")\n",
            "\n",
            "        return \"/\".join(zones[:5])  # Limit zones\n",
            "\n",
            "    def measure_quality(self, encoded_file, profile_name):\n",
            "        #Measure encoding quality metrics\n",
            "        # Use VMAF for quality measurement\n",
            "        try:\n",
            "            cmd = [\n",
            "                \"ffmpeg\", \"-i\", self.input_video, \"-i\", encoded_file,\n",
            "                \"-lavfi\", \"libvmaf=log_path=quality_metrics.json:log_fmt=json\",\n",
            "                \"-f\", \"null\", \"-\"\n",
            "            ]\n",
            "\n",
            "            subprocess.run(cmd, capture_output=True)\n",
            "\n",
            "            # Parse VMAF results\n",
            "            if os.path.exists(\"quality_metrics.json\"):\n",
            "                with open(\"quality_metrics.json\", \"r\") as f:\n",
            "                    vmaf_data = json.load(f)\n",
            "\n",
            "                avg_vmaf = np.mean([frame[\"metrics\"][\"vmaf\"] for frame in vmaf_data[\"frames\"]])\n",
            "\n",
            "                print(f\"VMAF Score for {profile_name}: {avg_vmaf:.2f}\")\n",
            "\n",
            "                # Save metrics\n",
            "                with open(f\"benchmarks/quality-metrics/{profile_name}_metrics.json\", \"w\") as f:\n",
            "                    json.dump({\n",
            "                        \"profile\": profile_name,\n",
            "                        \"vmaf_score\": avg_vmaf,\n",
            "                        \"file_size\": os.path.getsize(encoded_file),\n",
            "                        \"compression_ratio\": os.path.getsize(self.input_video) / os.path.getsize(encoded_file)\n",
            "                    }, f, indent=2)\n",
            "\n",
            "                os.remove(\"quality_metrics.json\")\n",
            "\n",
            "        except Exception as e:\n",
            "            print(f\"Quality measurement failed for {profile_name}: {e}\")\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    import sys\n",
            "    if len(sys.argv) != 3:\n",
            "        print(\"Usage: python3 advanced_h265_encoder.py <input_video> <output_dir>\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    encoder = AdvancedH265Encoder(sys.argv[1], sys.argv[2])\n",
            "    encoder.encode_with_advanced_params()\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!cat src/encoding/advanced_h265_encoder.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9yXoEUb97Sw4"
      },
      "outputs": [],
      "source": [
        "## Day 2: Enhanced Packaging with Multi-Protocol Support\n",
        "\n",
        "### Complete DASH Packager with CMAF\n",
        "\n",
        "# Create src/packaging/enhanced_dash_packager.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irg2XB2w9YiP"
      },
      "outputs": [],
      "source": [
        "dash_packager_script = '''#!/bin/bash\n",
        "\n",
        "ENCODED_DIR=\"$1\"\n",
        "OUTPUT_DIR=\"$2/packaged/dash\"\n",
        "SEGMENT_DURATION=${3:-4}\n",
        "\n",
        "if [ -z \"$ENCODED_DIR\" ] || [ -z \"$OUTPUT_DIR\" ]; then\n",
        "    echo \"Usage: $0 <encoded_directory> <output_directory> [segment_duration]\"\n",
        "    exit 1\n",
        "fi\n",
        "\n",
        "mkdir -p \"$OUTPUT_DIR\"\n",
        "\n",
        "echo \"Creating enhanced DASH manifest with CMAF segments...\"\n",
        "\n",
        "# Download Shaka Packager (Colab safe version)\n",
        "if ! [ -f \"/usr/local/bin/packager\" ]; then\n",
        "    echo \"Installing Shaka Packager...\"\n",
        "    wget -q https://github.com/shaka-project/shaka-packager/releases/download/v2.6.1/packager-linux-x64 -O packager\n",
        "    chmod +x packager\n",
        "    mv packager /usr/local/bin/packager\n",
        "fi\n",
        "\n",
        "# Package all H.265 streams with enhanced settings\n",
        "packager \\\n",
        "    'in='\"$ENCODED_DIR\"'/video_ultra_high.mp4,stream=video,output='\"$OUTPUT_DIR\"'/video_ultra_high_$Number$.m4s,init_segment='\"$OUTPUT_DIR\"'/video_ultra_high_init.mp4,playlist_name=ultra_high.m3u8' \\\n",
        "    'in='\"$ENCODED_DIR\"'/video_high.mp4,stream=video,output='\"$OUTPUT_DIR\"'/video_high_$Number$.m4s,init_segment='\"$OUTPUT_DIR\"'/video_high_init.mp4,playlist_name=high.m3u8' \\\n",
        "    'in='\"$ENCODED_DIR\"'/video_medium.mp4,stream=video,output='\"$OUTPUT_DIR\"'/video_medium_$Number$.m4s,init_segment='\"$OUTPUT_DIR\"'/video_medium_init.mp4,playlist_name=medium.m3u8' \\\n",
        "    'in='\"$ENCODED_DIR\"'/video_low.mp4,stream=video,output='\"$OUTPUT_DIR\"'/video_low_$Number$.m4s,init_segment='\"$OUTPUT_DIR\"'/video_low_init.mp4,playlist_name=low.m3u8' \\\n",
        "    'in='\"$ENCODED_DIR\"'/video_ultra_low.mp4,stream=video,output='\"$OUTPUT_DIR\"'/video_ultra_low_$Number$.m4s,init_segment='\"$OUTPUT_DIR\"'/video_ultra_low_init.mp4,playlist_name=ultra_low.m3u8' \\\n",
        "    --mpd_output \"$OUTPUT_DIR/manifest.mpd\" \\\n",
        "    --hls_master_playlist_output \"$OUTPUT_DIR/master.m3u8\" \\\n",
        "    --segment_duration \"$SEGMENT_DURATION\" \\\n",
        "    --fragment_duration \"$SEGMENT_DURATION\" \\\n",
        "    --time_shift_buffer_depth 3600 \\\n",
        "    --preserved_segments_outside_live_window 5 \\\n",
        "    --default_language en \\\n",
        "    --hls_playlist_type VOD \\\n",
        "    --generate_static_live_mpd\n",
        "\n",
        "# Enhance MPD\n",
        "python3 << PYTHON\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "tree = ET.parse(\"$OUTPUT_DIR/manifest.mpd\")\n",
        "root = tree.getroot()\n",
        "ns = {'mpd': 'urn:mpeg:dash:schema:mpd:2011'}\n",
        "\n",
        "for adaptation_set in root.findall('.//mpd:AdaptationSet', ns):\n",
        "    adaptation_set.set('par', '16:9')\n",
        "    adaptation_set.set('frameRate', '30/1')\n",
        "    adaptation_set.set('segmentAlignment', 'true')\n",
        "    adaptation_set.set('subsegmentAlignment', 'true')\n",
        "    adaptation_set.set('subsegmentStartsWithSAP', '1')\n",
        "\n",
        "    role_elem = ET.SubElement(adaptation_set, 'Role')\n",
        "    role_elem.set('schemeIdUri', 'urn:mpeg:dash:role:2011')\n",
        "    role_elem.set('value', 'main')\n",
        "\n",
        "    for rep in adaptation_set.findall('.//mpd:Representation', ns):\n",
        "        rep.set('width', '1920')\n",
        "        rep.set('height', '1080')\n",
        "        rep.set('sar', '1:1')\n",
        "\n",
        "tree.write(\"$OUTPUT_DIR/manifest.mpd\", encoding=\"utf-8\", xml_declaration=True)\n",
        "PYTHON\n",
        "\n",
        "echo \"‚úÖ DASH packaging complete: $OUTPUT_DIR/manifest.mpd\"\n",
        "'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25Jpqndl9q5Q",
        "outputId": "40917a05-963e-4907-8a7c-aed357d2174d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "encoding\n"
          ]
        }
      ],
      "source": [
        "!ls src/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dudlIk_u-Mud"
      },
      "outputs": [],
      "source": [
        "# Define the path\n",
        "file_path = 'src/packaging/enhanced_dash_packager.sh'\n",
        "import os\n",
        "# Create necessary directories\n",
        "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f3gSvLD9rWO"
      },
      "outputs": [],
      "source": [
        "# Write to file\n",
        "with open(\"src/packaging/enhanced_dash_packager.sh\", \"w\") as f:\n",
        "    f.write(dash_packager_script)\n",
        "\n",
        "# Make it executable\n",
        "!chmod +x src/packaging/enhanced_dash_packager.sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il86emn8AS8I",
        "outputId": "3c78a3e0-9b99-4d18-d682-73482e5142e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "encoded/video_ultra_high.mp4: ‚ùå Missing\n",
            "encoded/video_high.mp4: ‚ùå Missing\n",
            "encoded/video_medium.mp4: ‚ùå Missing\n",
            "encoded/video_low.mp4: ‚ùå Missing\n",
            "encoded/video_ultra_low.mp4: ‚ùå Missing\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "import subprocess\n",
        "import os\n",
        "for name in [\"ultra_high\", \"high\", \"medium\", \"low\", \"ultra_low\"]:\n",
        "    path = f\"encoded/video_{name}.mp4\"\n",
        "    print(f\"{path}: {'‚úÖ Found' if os.path.exists(path) else '‚ùå Missing'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWI8-wPMHy1l",
        "outputId": "8865f168-36e3-440f-bece-0efb7b3ff62b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enhanced_dash_packager.sh\n"
          ]
        }
      ],
      "source": [
        "!ls src/packaging/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DOXMrRbIi43",
        "outputId": "099395db-8203-47f0-9d50-fe34b11e43b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing advanced content analysis...\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "Encoding ultra_high profile...\n",
            "‚úó Failed to encode ultra_high: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "content/sample.mp4: No such file or directory\n",
            "\n",
            "Encoding high profile...\n",
            "‚úó Failed to encode high: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "content/sample.mp4: No such file or directory\n",
            "\n",
            "Encoding medium profile...\n",
            "‚úó Failed to encode medium: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "content/sample.mp4: No such file or directory\n",
            "\n",
            "Encoding low profile...\n",
            "‚úó Failed to encode low: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "content/sample.mp4: No such file or directory\n",
            "\n",
            "Encoding ultra_low profile...\n",
            "‚úó Failed to encode ultra_low: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "content/sample.mp4: No such file or directory\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python3 src/encoding/advanced_h265_encoder.py content/sample.mp4 encoded/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "dzSdcl2zJnHc",
        "outputId": "a4e58e61-46ad-4897-df66-a0b5be52ef07"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6d231a2a-aa46-42f3-8454-e526b4d13a35\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6d231a2a-aa46-42f3-8454-e526b4d13a35\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving sample.mp4 to sample.mp4\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMzWLNIiRIkQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"content\", exist_ok=True)\n",
        "os.rename(\"sample.mp4\", \"content/sample.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "4g1-crJZRUzF",
        "outputId": "4e149247-9fd0-463f-9b43-b7a86057bdbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing advanced content analysis...\n",
            "Encoding ultra_high profile...\n",
            "‚úì Successfully encoded ultra_high\n",
            "Encoding high profile...\n",
            "‚úì Successfully encoded high\n",
            "Encoding medium profile...\n",
            "‚úì Successfully encoded medium\n",
            "Encoding low profile...\n",
            "‚úì Successfully encoded low\n",
            "Encoding ultra_low profile...\n",
            "‚úì Successfully encoded ultra_low\n"
          ]
        }
      ],
      "source": [
        "!python3 src/encoding/advanced_h265_encoder.py content/sample.mp4 encoded/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TOdZhSJI_qsS"
      },
      "outputs": [],
      "source": [
        "!src/packaging/enhanced_dash_packager.sh encoded .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KS8Xb9VOY0TB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"src/packaging\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vwOawRqXY8EV"
      },
      "outputs": [],
      "source": [
        "### Advanced HLS Packager with Low-Latency Support\n",
        "\n",
        "# Create src/packaging/advanced_hls_packager.sh\n",
        "\n",
        "hls_packager_script = '''#!/bin/bash\n",
        "\n",
        "ENCODED_DIR=\"$1\"\n",
        "OUTPUT_DIR=\"$2/packaged/hls\"\n",
        "LOW_LATENCY=${3:-false}\n",
        "\n",
        "if [ -z \"$ENCODED_DIR\" ] || [ -z \"$OUTPUT_DIR\" ]; then\n",
        "    echo \"Usage: $0 <encoded_directory> <output_directory> [low_latency]\"\n",
        "    exit 1\n",
        "fi\n",
        "\n",
        "mkdir -p \"$OUTPUT_DIR\"\n",
        "\n",
        "echo \"Creating advanced HLS playlists with enhanced features...\"\n",
        "\n",
        "# Define stream configurations\n",
        "declare -A STREAMS=(\n",
        "    [\"ultra_high\"]=\"8000000:60\"\n",
        "    [\"high\"]=\"5000000:30\"\n",
        "    [\"medium\"]=\"3000000:30\"\n",
        "    [\"low\"]=\"1500000:24\"\n",
        "    [\"ultra_low\"]=\"800000:15\"\n",
        ")\n",
        "\n",
        "# Generate HLS segments for each stream\n",
        "for stream in \"${!STREAMS[@]}\"; do\n",
        "    IFS=':' read -r bitrate framerate <<< \"${STREAMS[$stream]}\"\n",
        "\n",
        "    echo \"Processing $stream stream...\"\n",
        "\n",
        "    # Configure HLS parameters based on mode\n",
        "    if [ \"$LOW_LATENCY\" = \"true\" ]; then\n",
        "        HLS_TIME=2\n",
        "        HLS_LIST_SIZE=6\n",
        "        HLS_FLAGS=\"-hls_flags independent_segments+program_date_time\"\n",
        "        SEGMENT_TYPE=\"fmp4\"\n",
        "    else\n",
        "        HLS_TIME=4\n",
        "        HLS_LIST_SIZE=5\n",
        "        HLS_FLAGS=\"-hls_flags independent_segments\"\n",
        "        SEGMENT_TYPE=\"fmp4\"\n",
        "    fi\n",
        "\n",
        "    ffmpeg -i \"$ENCODED_DIR/video_${stream}.mp4\" \\\n",
        "        -c copy \\\n",
        "        -f hls \\\n",
        "        -hls_time $HLS_TIME \\\n",
        "        -hls_list_size $HLS_LIST_SIZE \\\n",
        "        -hls_playlist_type vod \\\n",
        "        -hls_segment_type $SEGMENT_TYPE \\\n",
        "        $HLS_FLAGS \\\n",
        "        -hls_fmp4_init_filename \"${stream}_init.mp4\" \\\n",
        "        -hls_segment_filename \"$OUTPUT_DIR/${stream}_%06d.m4s\" \\\n",
        "        -master_pl_name \"master.m3u8\" \\\n",
        "        \"$OUTPUT_DIR/playlist_${stream}.m3u8\"\n",
        "\n",
        "    # Add stream info to temporary file for master playlist\n",
        "    echo \"${bitrate}:1920x1080:${framerate}:${stream}\" >> /tmp/hls_streams.txt\n",
        "done\n",
        "\n",
        "# Create enhanced master playlist\n",
        "cat > \"$OUTPUT_DIR/master.m3u8\" << EOL\n",
        "#EXTM3U\n",
        "#EXT-X-VERSION:7\n",
        "\n",
        "EOL\n",
        "\n",
        "# Add stream entries\n",
        "while IFS=':' read -r bandwidth resolution framerate stream; do\n",
        "    cat >> \"$OUTPUT_DIR/master.m3u8\" << EOL\n",
        "#EXT-X-STREAM-INF:BANDWIDTH=${bandwidth},RESOLUTION=${resolution},CODECS=\"hvc1.1.6.L150.90\",FRAME-RATE=${framerate}\n",
        "playlist_${stream}.m3u8\n",
        "\n",
        "EOL\n",
        "done < /tmp/hls_streams.txt\n",
        "\n",
        "# Add low-latency specific tags if enabled\n",
        "if [ \"$LOW_LATENCY\" = \"true\" ]; then\n",
        "    sed -i '2a#EXT-X-SERVER-CONTROL:CAN-BLOCK-RELOAD=YES,PART-HOLD-BACK=1.0,CAN-SKIP-UNTIL=12.0' \"$OUTPUT_DIR/master.m3u8\"\n",
        "fi\n",
        "\n",
        "# Cleanup\n",
        "rm -f /tmp/hls_streams.txt\n",
        "\n",
        "echo \"Advanced HLS packaging completed. Master playlist: $OUTPUT_DIR/master.m3u8\"\n",
        "'''\n",
        "\n",
        "with open(\"src/packaging/advanced_hls_packager.sh\", \"w\") as f:\n",
        "    f.write(hls_packager_script)\n",
        "\n",
        "!chmod +x src/packaging/advanced_hls_packager.sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sON7v77GZFso"
      },
      "outputs": [],
      "source": [
        "!src/packaging/advanced_hls_packager.sh encoded . false\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rlj2bb23av3x"
      },
      "outputs": [],
      "source": [
        "### Neural Network Bandwidth Predictor\n",
        "# Create src/ml-models/bandwidth_predictor.py\n",
        "# Complete src/ml-models/bandwidth_predictor.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "g3fuu4mCcjnu"
      },
      "outputs": [],
      "source": [
        "!mkdir -p src/ml-models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l8msCjnEcyL-"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BwmZYc_Uc2ZE"
      },
      "outputs": [],
      "source": [
        "!mkdir -p src/ml-models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cO1ayFv0dFFi"
      },
      "outputs": [],
      "source": [
        "## Day 3: ML-Enhanced Client Player\n",
        "\n",
        "### Neural Network Bandwidth Predictor\n",
        "# Create src/ml-models/bandwidth_predictor.py\n",
        "# Complete src/ml-models/bandwidth_predictor.py\n",
        "\n",
        "bandwidth_predictor_code = '''\n",
        "# your full code here (copy-paste exactly as you posted)\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import pickle\n",
        "import json\n",
        "from collections import deque\n",
        "import time\n",
        "\n",
        "class BandwidthPredictor:\n",
        "    def __init__(self, sequence_length=10):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.history = deque(maxlen=sequence_length)\n",
        "        self.is_trained = False\n",
        "        self.prediction_accuracy = deque(maxlen=20)\n",
        "\n",
        "    def build_model(self):\n",
        "        #Build LSTM model for bandwidth prediction\n",
        "        model = models.Sequential([\n",
        "            layers.LSTM(64, return_sequences=True, input_shape=(self.sequence_length, 4)),\n",
        "            layers.Dropout(0.2),\n",
        "            layers.LSTM(32, return_sequences=False),\n",
        "            layers.Dropout(0.2),\n",
        "            layers.Dense(16, activation='relu'),\n",
        "            layers.Dense(1, activation='linear')\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "        return model\n",
        "\n",
        "    def preprocess_data(self, bandwidth_history):\n",
        "        #Preprocess bandwidth data for training\n",
        "        X, y = [], []\n",
        "\n",
        "        for i in range(len(bandwidth_history) - self.sequence_length):\n",
        "            sequence = bandwidth_history[i:i + self.sequence_length]\n",
        "            target = bandwidth_history[i + self.sequence_length]['bandwidth']\n",
        "\n",
        "            features = []\n",
        "            for sample in sequence:\n",
        "                features.append([\n",
        "                    sample['bandwidth'],\n",
        "                    sample['rtt'],\n",
        "                    sample['buffer_level'],\n",
        "                    sample['timestamp'] % 86400\n",
        "                ])\n",
        "\n",
        "            X.append(features)\n",
        "            y.append(target)\n",
        "\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    def train(self, training_data):\n",
        "        #Train the bandwidth prediction model\n",
        "        if len(training_data) < self.sequence_length + 10:\n",
        "            print(\"Insufficient training data\")\n",
        "            return False\n",
        "\n",
        "        X, y = self.preprocess_data(training_data)\n",
        "\n",
        "        from sklearn.preprocessing import StandardScaler\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "        X_reshaped = X.reshape(-1, X.shape[-1])\n",
        "        X_scaled = self.scaler.fit_transform(X_reshaped)\n",
        "        X_scaled = X_scaled.reshape(X.shape)\n",
        "\n",
        "        self.model = self.build_model()\n",
        "\n",
        "        split_idx = int(len(X_scaled) * 0.8)\n",
        "        X_train, X_val = X_scaled[:split_idx], X_scaled[split_idx:]\n",
        "        y_train, y_val = y[:split_idx], y[split_idx:]\n",
        "\n",
        "        history = self.model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=50,\n",
        "            batch_size=32,\n",
        "            validation_data=(X_val, y_val),\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        self.is_trained = True\n",
        "\n",
        "        self.model.save('src/ml-models/bandwidth_model.h5')\n",
        "        with open('src/ml-models/scaler.pkl', 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)\n",
        "\n",
        "        return True\n",
        "\n",
        "    def predict(self, current_data):\n",
        "        #Predict future bandwidth\n",
        "        if not self.is_trained or self.model is None:\n",
        "            return self.fallback_prediction(current_data)\n",
        "\n",
        "        self.history.append(current_data)\n",
        "\n",
        "        if len(self.history) < self.sequence_length:\n",
        "            return self.fallback_prediction(current_data)\n",
        "\n",
        "        sequence = []\n",
        "        for sample in self.history:\n",
        "            sequence.append([\n",
        "                sample['bandwidth'],\n",
        "                sample['rtt'],\n",
        "                sample['buffer_level'],\n",
        "                sample['timestamp'] % 86400\n",
        "            ])\n",
        "\n",
        "        sequence = np.array([sequence])\n",
        "        sequence_reshaped = sequence.reshape(-1, sequence.shape[-1])\n",
        "        sequence_scaled = self.scaler.transform(sequence_reshaped)\n",
        "        sequence_scaled = sequence_scaled.reshape(sequence.shape)\n",
        "\n",
        "        prediction = self.model.predict(sequence_scaled, verbose=0)[0][0]\n",
        "        confidence = self.calculate_confidence()\n",
        "\n",
        "        return {\n",
        "            'predicted_bandwidth': max(0, prediction),\n",
        "            'confidence': confidence,\n",
        "            'fallback_bandwidth': self.fallback_prediction(current_data)['bandwidth']\n",
        "        }\n",
        "\n",
        "    def fallback_prediction(self, current_data):\n",
        "        #Simple fallback prediction when ML model isn't available\n",
        "        if len(self.history) < 3:\n",
        "            return {'bandwidth': current_data['bandwidth'], 'confidence': 0.3}\n",
        "\n",
        "        recent_bandwidths = [h['bandwidth'] for h in list(self.history)[-3:]]\n",
        "        trend = (recent_bandwidths[-1] - recent_bandwidths[0]) / len(recent_bandwidths)\n",
        "        predicted = recent_bandwidths[-1] + trend\n",
        "\n",
        "        return {'bandwidth': max(0, predicted), 'confidence': 0.5}\n",
        "\n",
        "    def calculate_confidence(self):\n",
        "        #Calculate prediction confidence based on recent accuracy\n",
        "        if len(self.history) < 5:\n",
        "            return 0.5\n",
        "\n",
        "        recent_values = [h['bandwidth'] for h in list(self.history)[-5:]]\n",
        "        variance = np.var(recent_values)\n",
        "        mean_value = np.mean(recent_values)\n",
        "\n",
        "        if mean_value == 0:\n",
        "            return 0.3\n",
        "\n",
        "        coefficient_of_variation = np.sqrt(variance) / mean_value\n",
        "        confidence = max(0.1, 1.0 - coefficient_of_variation)\n",
        "\n",
        "        return min(0.95, confidence)\n",
        "\n",
        "    def update_accuracy(self, predicted, actual):\n",
        "        #Update prediction accuracy tracking\n",
        "        if predicted > 0:\n",
        "            accuracy = 1.0 - abs(predicted - actual) / max(predicted, actual)\n",
        "            self.prediction_accuracy.append(max(0, accuracy))\n",
        "\n",
        "    def get_average_accuracy(self):\n",
        "        #Get average prediction accuracy\n",
        "        if not self.prediction_accuracy:\n",
        "            return 0.5\n",
        "        return np.mean(list(self.prediction_accuracy))\n",
        "\n",
        "class QualityAdaptationEngine:\n",
        "    def __init__(self):\n",
        "        self.bandwidth_predictor = BandwidthPredictor()\n",
        "        self.quality_levels = {\n",
        "            'ultra_high': {'bitrate': 8000000, 'framerate': 60, 'priority': 5},\n",
        "            'high': {'bitrate': 5000000, 'framerate': 30, 'priority': 4},\n",
        "            'medium': {'bitrate': 3000000, 'framerate': 30, 'priority': 3},\n",
        "            'low': {'bitrate': 1500000, 'framerate': 24, 'priority': 2},\n",
        "            'ultra_low': {'bitrate': 800000, 'framerate': 15, 'priority': 1}\n",
        "        }\n",
        "        self.current_quality = 'medium'\n",
        "        self.buffer_target = 10.0  # seconds\n",
        "        self.buffer_panic = 3.0    # seconds\n",
        "        self.switching_cooldown = 5.0  # seconds\n",
        "        self.last_switch_time = 0\n",
        "        self.adaptation_history = deque(maxlen=50)\n",
        "\n",
        "    def select_quality(self, network_state, buffer_level):\n",
        "        #Select optimal quality level based on network and buffer state\n",
        "        current_time = time.time()\n",
        "\n",
        "        # Get bandwidth prediction\n",
        "        prediction = self.bandwidth_predictor.predict(network_state)\n",
        "        predicted_bw = prediction['predicted_bandwidth']\n",
        "        confidence = prediction['confidence']\n",
        "\n",
        "        # Apply safety margin based on confidence\n",
        "        safety_margin = 0.8 + (confidence * 0.2)  # 0.8 to 1.0\n",
        "        effective_bandwidth = predicted_bw * safety_margin\n",
        "\n",
        "        # Buffer-based adjustment\n",
        "        buffer_factor = self.calculate_buffer_factor(buffer_level)\n",
        "        adjusted_bandwidth = effective_bandwidth * buffer_factor\n",
        "\n",
        "        # Find best quality level\n",
        "        best_quality = self.find_best_quality(adjusted_bandwidth)\n",
        "\n",
        "        # Apply switching logic\n",
        "        should_switch = self.should_switch_quality(best_quality, current_time, buffer_level)\n",
        "\n",
        "        if should_switch:\n",
        "            self.current_quality = best_quality\n",
        "            self.last_switch_time = current_time\n",
        "\n",
        "        # Log adaptation decision\n",
        "        self.adaptation_history.append({\n",
        "            'timestamp': current_time,\n",
        "            'predicted_bw': predicted_bw,\n",
        "            'effective_bw': effective_bandwidth,\n",
        "            'buffer_level': buffer_level,\n",
        "            'selected_quality': self.current_quality,\n",
        "            'switched': should_switch\n",
        "        })\n",
        "\n",
        "        return {\n",
        "            'quality_level': self.current_quality,\n",
        "            'bitrate': self.quality_levels[self.current_quality]['bitrate'],\n",
        "            'framerate': self.quality_levels[self.current_quality]['framerate'],\n",
        "            'predicted_bandwidth': predicted_bw,\n",
        "            'confidence': confidence,\n",
        "            'switched': should_switch\n",
        "        }\n",
        "\n",
        "    def calculate_buffer_factor(self, buffer_level):\n",
        "        #Calculate buffer-based adjustment factor\n",
        "        if buffer_level < self.buffer_panic:\n",
        "            return 0.6  # Aggressive downscaling\n",
        "        elif buffer_level < self.buffer_target:\n",
        "            return 0.8  # Conservative scaling\n",
        "        elif buffer_level > self.buffer_target * 2:\n",
        "            return 1.2  # Allow higher quality\n",
        "        else:\n",
        "            return 1.0  # Normal scaling\n",
        "\n",
        "    def find_best_quality(self, available_bandwidth):\n",
        "        #Find the best quality level for given bandwidth\n",
        "        for quality in ['ultra_high', 'high', 'medium', 'low', 'ultra_low']:\n",
        "            if self.quality_levels[quality]['bitrate'] <= available_bandwidth:\n",
        "                return quality\n",
        "        return 'ultra_low'\n",
        "\n",
        "    def should_switch_quality(self, target_quality, current_time, buffer_level):\n",
        "        #Determine if quality switch should occur\n",
        "        if target_quality == self.current_quality:\n",
        "            return False\n",
        "\n",
        "        # Cooldown period\n",
        "        if current_time - self.last_switch_time < self.switching_cooldown:\n",
        "            return False\n",
        "\n",
        "        current_priority = self.quality_levels[self.current_quality]['priority']\n",
        "        target_priority = self.quality_levels[target_quality]['priority']\n",
        "\n",
        "        # Emergency downgrade\n",
        "        if buffer_level < self.buffer_panic and target_priority < current_priority:\n",
        "            return True\n",
        "\n",
        "        # Quality upgrade with hysteresis\n",
        "        if target_priority > current_priority:\n",
        "            # Require significant bandwidth advantage for upgrade\n",
        "            current_bitrate = self.quality_levels[self.current_quality]['bitrate']\n",
        "            target_bitrate = self.quality_levels[target_quality]['bitrate']\n",
        "            return target_bitrate < current_bitrate * 0.8\n",
        "\n",
        "        # Quality downgrade\n",
        "        if target_priority < current_priority:\n",
        "            return buffer_level < self.buffer_target\n",
        "\n",
        "        return False\n",
        "\n",
        "    def get_adaptation_stats(self):\n",
        "        #Get adaptation statistics\n",
        "        if not self.adaptation_history:\n",
        "            return {}\n",
        "\n",
        "        history = list(self.adaptation_history)\n",
        "        switches = sum(1 for h in history if h['switched'])\n",
        "        avg_quality = np.mean([self.quality_levels[h['selected_quality']]['priority'] for h in history])\n",
        "\n",
        "        return {\n",
        "            'total_adaptations': len(history),\n",
        "            'quality_switches': switches,\n",
        "            'switch_rate': switches / len(history) if history else 0,\n",
        "            'average_quality_score': avg_quality,\n",
        "            'prediction_accuracy': self.bandwidth_predictor.get_average_accuracy()\n",
        "        }\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4BNRK3dJlCtY"
      },
      "outputs": [],
      "source": [
        "# Write the code to the file\n",
        "with open('src/ml-models/bandwidth_predictor.py', 'w') as f:\n",
        "    f.write(bandwidth_predictor_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "Ue4mJJU3lSO3"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b0XiylTslYVA"
      },
      "outputs": [],
      "source": [
        "!ls src/packaging/\n",
        "!ls src/\n",
        "!ls src/ml-models/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "381YJ2non4rE"
      },
      "outputs": [],
      "source": [
        "#rename\n",
        "!mv src/ml-models src/ml_models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2udfxrVAmxy8"
      },
      "outputs": [],
      "source": [
        "!ls src/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O-tElk2Cn_oS"
      },
      "outputs": [],
      "source": [
        "from src.ml_models.bandwidth_predictor import BandwidthPredictor, QualityAdaptationEngine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kmV15qBFoZ2t"
      },
      "outputs": [],
      "source": [
        "#simulate a prediction\n",
        "predictor = BandwidthPredictor()\n",
        "sample = {\n",
        "    'bandwidth': 2_000_000,\n",
        "    'rtt': 50,\n",
        "    'buffer_level': 5.0,\n",
        "    'timestamp': time.time()\n",
        "}\n",
        "\n",
        "# Append sample 10 times to simulate history\n",
        "for _ in range(10):\n",
        "    predictor.history.append(sample)\n",
        "\n",
        "# Try predicting\n",
        "result = predictor.predict(sample)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ogqHtiA0oNJ6"
      },
      "outputs": [],
      "source": [
        "# Create src/client/enhanced_player.py\n",
        "import json\n",
        "import time\n",
        "import threading\n",
        "from http.server import HTTPServer, SimpleHTTPRequestHandler\n",
        "import socketserver\n",
        "import os\n",
        "\n",
        "class EnhancedStreamingClient:\n",
        "    def __init__(self, manifest_url):\n",
        "        self.manifest_url = manifest_url\n",
        "        self.adaptation_engine = QualityAdaptationEngine()\n",
        "        self.playback_stats = {\n",
        "            'buffer_level': 10.0,\n",
        "            'current_bandwidth': 5000000,\n",
        "            'rtt': 50,\n",
        "            'frames_dropped': 0,\n",
        "            'rebuffer_events': 0,\n",
        "            'total_playtime': 0\n",
        "        }\n",
        "        self.is_playing = False\n",
        "        self.monitoring_thread = None\n",
        "\n",
        "    def start_playback(self):\n",
        "        \"\"\"Start enhanced video playback with ML adaptation\"\"\"\n",
        "        self.is_playing = True\n",
        "        self.monitoring_thread = threading.Thread(target=self.monitor_playback)\n",
        "        self.monitoring_thread.daemon = True\n",
        "        self.monitoring_thread.start()\n",
        "\n",
        "        print(\"Enhanced H.265 streaming started with ML adaptation\")\n",
        "\n",
        "    def monitor_playback(self):\n",
        "        \"\"\"Monitor playback and adapt quality\"\"\"\n",
        "        while self.is_playing:\n",
        "            # Simulate network measurements (replace with real measurements)\n",
        "            network_state = {\n",
        "                'bandwidth': self.measure_bandwidth(),\n",
        "                'rtt': self.measure_rtt(),\n",
        "                'buffer_level': self.playback_stats['buffer_level'],\n",
        "                'timestamp': time.time()\n",
        "            }\n",
        "\n",
        "            # Get quality adaptation decision\n",
        "            adaptation = self.adaptation_engine.select_quality(\n",
        "                network_state,\n",
        "                self.playback_stats['buffer_level']\n",
        "            )\n",
        "\n",
        "            if adaptation['switched']:\n",
        "                print(f\"Quality switched to {adaptation['quality_level']} \"\n",
        "                      f\"({adaptation['bitrate']/1000000:.1f} Mbps, \"\n",
        "                      f\"{adaptation['framerate']} fps)\")\n",
        "\n",
        "            # Update playback stats\n",
        "            self.update_playback_stats(adaptation)\n",
        "\n",
        "            time.sleep(1)  # Monitor every second\n",
        "\n",
        "    def measure_bandwidth(self):\n",
        "        \"\"\"Measure current bandwidth (simplified simulation)\"\"\"\n",
        "        # In real implementation, measure actual download speed\n",
        "        import random\n",
        "        base_bw = 3000000  # 3 Mbps base\n",
        "        variation = random.uniform(0.5, 1.5)\n",
        "        return int(base_bw * variation)\n",
        "\n",
        "    def measure_rtt(self):\n",
        "        \"\"\"Measure current RTT (simplified simulation)\"\"\"\n",
        "        import random\n",
        "        return random.uniform(20, 100)  # 20-100ms RTT\n",
        "\n",
        "    def update_playback_stats(self, adaptation):\n",
        "        \"\"\"Update playback statistics\"\"\"\n",
        "        # Simulate buffer changes based on quality selection\n",
        "        bitrate_demand = adaptation['bitrate']\n",
        "        available_bw = adaptation['predicted_bandwidth']\n",
        "\n",
        "        if bitrate_demand <= available_bw:\n",
        "            self.playback_stats['buffer_level'] = min(30.0,\n",
        "                self.playback_stats['buffer_level'] + 0.5)\n",
        "        else:\n",
        "            self.playback_stats['buffer_level'] = max(0.0,\n",
        "                self.playback_stats['buffer_level'] - 1.0)\n",
        "\n",
        "        if self.playback_stats['buffer_level'] <= 0:\n",
        "            self.playback_stats['rebuffer_events'] += 1\n",
        "\n",
        "        self.playback_stats['current_bandwidth'] = available_bw\n",
        "        self.playback_stats['total_playtime'] += 1\n",
        "\n",
        "    def get_qoe_metrics(self):\n",
        "        \"\"\"Calculate Quality of Experience metrics\"\"\"\n",
        "        stats = self.adaptation_engine.get_adaptation_stats()\n",
        "\n",
        "        # Calculate rebuffering ratio\n",
        "        rebuffer_ratio = (self.playback_stats['rebuffer_events'] /\n",
        "                         max(1, self.playback_stats['total_playtime']))\n",
        "\n",
        "        # Calculate quality stability (lower is better)\n",
        "        quality_stability = stats.get('switch_rate', 0)\n",
        "\n",
        "        # Calculate average quality score\n",
        "        avg_quality = stats.get('average_quality_score', 2.5)\n",
        "\n",
        "        qoe_score = (\n",
        "            (avg_quality / 5.0) * 0.4 +           # 40% quality weight\n",
        "            (1.0 - rebuffer_ratio) * 0.4 +        # 40% rebuffering weight\n",
        "            (1.0 - quality_stability) * 0.2       # 20% stability weight\n",
        "        ) * 100\n",
        "\n",
        "        return {\n",
        "            'qoe_score': max(0, min(100, qoe_score)),\n",
        "            'average_quality': avg_quality,\n",
        "            'rebuffer_ratio': rebuffer_ratio,\n",
        "            'quality_switches': stats.get('quality_switches', 0),\n",
        "            'prediction_accuracy': stats.get('prediction_accuracy', 0.5)\n",
        "        }\n",
        "\n",
        "    def stop_playback(self):\n",
        "        \"\"\"Stop playback and show final metrics\"\"\"\n",
        "        self.is_playing = False\n",
        "        if self.monitoring_thread:\n",
        "            self.monitoring_thread.join()\n",
        "\n",
        "        metrics = self.get_qoe_metrics()\n",
        "        print(\"\\n=== Playback Session Complete ===\")\n",
        "        print(f\"QoE Score: {metrics['qoe_score']:.1f}/100\")\n",
        "        print(f\"Average Quality: {metrics['average_quality']:.1f}/5\")\n",
        "        print(f\"Rebuffer Ratio: {metrics['rebuffer_ratio']:.2%}\")\n",
        "        print(f\"Quality Switches: {metrics['quality_switches']}\")\n",
        "        print(f\"Prediction Accuracy: {metrics['prediction_accuracy']:.2%}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    client = EnhancedStreamingClient(\"http://localhost:8080/packaged/dash/manifest.mpd\")\n",
        "\n",
        "    try:\n",
        "        client.start_playback()\n",
        "\n",
        "        # Simulate playback for 60 seconds\n",
        "        time.sleep(60)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nPlayback interrupted by user\")\n",
        "    finally:\n",
        "        client.stop_playback()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNM/MzqzCzgssqSEz3t+iKE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}